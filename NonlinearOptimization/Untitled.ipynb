{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    var add_command_shortcuts = {\n",
       "            'Alt-w' : {\n",
       "                help    : 'Add Text Test',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                        var cell = IPython.notebook.get_selected_cell();\n",
       "        \t\tcell.code_mirror.replaceSelection('Testing');\n",
       "                }\n",
       "            }\n",
       "        };\n",
       "\n",
       "    var add_edit_shortcuts = {\n",
       "            'Alt-a' : {\n",
       "                help    : 'Insert alpha',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\alpha');\n",
       "                }\n",
       "            },\n",
       "        'Alt-b' : {\n",
       "                help    : 'Insert beta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\beta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-g' : {\n",
       "                help    : 'Insert gamma',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\gamma');\n",
       "                }\n",
       "            },\n",
       "        'Alt-shift-g' : {\n",
       "                help    : 'Insert Gamma',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\Gamma');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Ctrl-d' : { //Change\n",
       "                help    : 'Insert delta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\delta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Ctrl-e' : {\n",
       "                help    : 'Insert epsilon',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\epsilon');\n",
       "                }\n",
       "            },\n",
       "        'Alt-z' : {\n",
       "                help    : 'Insert zeta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\zeta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-t' : {\n",
       "                help    : 'Insert theta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\theta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-k' : {\n",
       "                help    : 'Insert kappa',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\kappa');\n",
       "                }\n",
       "            },\n",
       "        'Alt-l' : {\n",
       "                help    : 'Insert lambda',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\lambda');\n",
       "                }\n",
       "            },\n",
       "        'Alt-p' : {\n",
       "                help    : 'Insert phi',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\phi');\n",
       "                }\n",
       "            },\n",
       "        'Alt-r' : {\n",
       "                help    : 'Insert rho',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\rho');\n",
       "                }\n",
       "            },\n",
       "        'Alt-s' : {\n",
       "                help    : 'Insert sigma',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\sigma');\n",
       "                }\n",
       "            },\n",
       "        'Alt-w' : {\n",
       "                help    : 'Insert omega',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\omega');\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-1' : {\n",
       "                help    : 'Insert dfrac',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\dfrac{}{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-7' : {\n",
       "                help    : 'Insert integral',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\int_{}^{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-4});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-3' : {\n",
       "                help    : 'Insert cos',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\text{cos}');\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-4' : {\n",
       "                help    : 'Insert angle',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\langle  \\\\rangle');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-7});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-5' : {\n",
       "                help    : 'Insert Brackets',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\left[  \\\\right]');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-6' : {\n",
       "                help    : 'Insert parenthesis',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\left(  \\\\right)');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-7' : {\n",
       "                help    : 'Insert parenthesis',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\left[\\\\begin{matrix}  \\\\end{matrix}\\\\right]');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-20});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-2' : {\n",
       "                help    : 'Insert equation',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('$$  $$');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
       "                }\n",
       "            },\n",
       "        'Alt-0' : {\n",
       "                help    : 'Insert partial',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\partial ');\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-n' : {\n",
       "                help    : 'Insert nabla',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\nabla');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Shift-t' : {\n",
       "                help    : 'Insert text',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\text{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "/*        'Shift-6' : {\n",
       "                help    : 'Insert superscript',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('^{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "        'Shift-_' : {\n",
       "                help    : 'Insert subscript',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('_{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "*/\n",
       "        };\n",
       "\n",
       "   // var load_ipython_extension = function() {\n",
       "        IPython.keyboard_manager.edit_shortcuts.add_shortcuts(add_edit_shortcuts);\n",
       "        IPython.keyboard_manager.command_shortcuts.add_shortcuts(add_command_shortcuts);\n",
       "    //};"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "    var add_command_shortcuts = {\n",
    "            'Alt-w' : {\n",
    "                help    : 'Add Text Test',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                        var cell = IPython.notebook.get_selected_cell();\n",
    "        \t\tcell.code_mirror.replaceSelection('Testing');\n",
    "                }\n",
    "            }\n",
    "        };\n",
    "\n",
    "    var add_edit_shortcuts = {\n",
    "            'Alt-a' : {\n",
    "                help    : 'Insert alpha',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\alpha');\n",
    "                }\n",
    "            },\n",
    "        'Alt-b' : {\n",
    "                help    : 'Insert beta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\beta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-g' : {\n",
    "                help    : 'Insert gamma',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\gamma');\n",
    "                }\n",
    "            },\n",
    "        'Alt-shift-g' : {\n",
    "                help    : 'Insert Gamma',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\Gamma');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Ctrl-d' : { //Change\n",
    "                help    : 'Insert delta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\delta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Ctrl-e' : {\n",
    "                help    : 'Insert epsilon',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\epsilon');\n",
    "                }\n",
    "            },\n",
    "        'Alt-z' : {\n",
    "                help    : 'Insert zeta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\zeta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-t' : {\n",
    "                help    : 'Insert theta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\theta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-k' : {\n",
    "                help    : 'Insert kappa',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\kappa');\n",
    "                }\n",
    "            },\n",
    "        'Alt-l' : {\n",
    "                help    : 'Insert lambda',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\lambda');\n",
    "                }\n",
    "            },\n",
    "        'Alt-p' : {\n",
    "                help    : 'Insert phi',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\phi');\n",
    "                }\n",
    "            },\n",
    "        'Alt-r' : {\n",
    "                help    : 'Insert rho',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\rho');\n",
    "                }\n",
    "            },\n",
    "        'Alt-s' : {\n",
    "                help    : 'Insert sigma',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\sigma');\n",
    "                }\n",
    "            },\n",
    "        'Alt-w' : {\n",
    "                help    : 'Insert omega',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\omega');\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-1' : {\n",
    "                help    : 'Insert dfrac',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\dfrac{}{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-7' : {\n",
    "                help    : 'Insert integral',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\int_{}^{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-4});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-3' : {\n",
    "                help    : 'Insert cos',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\text{cos}');\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-4' : {\n",
    "                help    : 'Insert angle',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\langle  \\\\rangle');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-7});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-5' : {\n",
    "                help    : 'Insert Brackets',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\left[  \\\\right]');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-6' : {\n",
    "                help    : 'Insert parenthesis',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\left(  \\\\right)');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-7' : {\n",
    "                help    : 'Insert parenthesis',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\left[\\\\begin{matrix}  \\\\end{matrix}\\\\right]');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-20});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-2' : {\n",
    "                help    : 'Insert equation',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('$$  $$');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
    "                }\n",
    "            },\n",
    "        'Alt-0' : {\n",
    "                help    : 'Insert partial',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\partial ');\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-n' : {\n",
    "                help    : 'Insert nabla',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\nabla');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Shift-t' : {\n",
    "                help    : 'Insert text',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\text{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "/*        'Shift-6' : {\n",
    "                help    : 'Insert superscript',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('^{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "        'Shift-_' : {\n",
    "                help    : 'Insert subscript',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('_{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "*/\n",
    "        };\n",
    "\n",
    "   // var load_ipython_extension = function() {\n",
    "        IPython.keyboard_manager.edit_shortcuts.add_shortcuts(add_edit_shortcuts);\n",
    "        IPython.keyboard_manager.command_shortcuts.add_shortcuts(add_command_shortcuts);\n",
    "    //};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "if the alpha is small enough, the ratio is always bounded by a half $\\epsilon$. If this is true, we can simply plug it in.\n",
    "\n",
    "$$ \\leq f(x) + \\alpha d^T \\nabla f(x) + \\dfrac{1}{2}\\epsilon\\alpha  $$\n",
    "$$ = f(x) - \\alpha \\epsilon + \\dfrac{1}{2}\\epsilon \\alpha = f(x)  - \\dfrac{1}{2} \\epsilon \\alpha $$\n",
    "\n",
    "The limit\n",
    "\n",
    "$$\\lim_{\\alpha \\to 0} g(\\alpha) < \\epsilon)$$\n",
    "\n",
    "gradient direction. For minimizing quadratic function, if step size is too large, it diverges. If too small, no progress is being made. We want to pick the stop size such that progress is made without making the algorithm unstable. \n",
    "\n",
    "Algorithm:\n",
    "\n",
    "$$ x^{r+1} = x^r + \\alpha_rd^r, r=0,1,...$$\n",
    "\n",
    "where $\\nabla f(x^r) \\neq 0$, the direction of d satisfies $\\nabla f(x^r)d^r < 0$, and $\\alpha^r$ is a positive step size. Use steepest descent.\n",
    "\n",
    "General case: Gradient descent method\n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha_rD^r\\nabla f(x^r), r=0,1,...$$\n",
    "\n",
    "Where D is a positive definite matrix called a scaling matrix. This helps to scale things down. (units can be miles). Units aren't the same.\n",
    "\n",
    "Special Case I: D is an Identity matrix. This is steepest descent.\n",
    "Special case II: Newtons method. Specialization where D is the inversion of the Hashan matrix. \n",
    "\n",
    "In practise, this is generally the behavior we see (zig-zag behavior). The local gradient direction does not correspond to global direction. Newton's method, on the other hand, is very good at solving quadratic problems. Think about what it is doing. Say we have a quadratic problem. How does Newton's method solve it. \n",
    "\n",
    "$$ f(x) = x^T Qx + bx $$\n",
    "\n",
    "Want to appl yNewton method to it. \n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha Q^{-1} \\left( Qx + b \\right)  $$\n",
    "\n",
    "Q is the Hashan.\n",
    "\n",
    "$$ = x^T - \\alpha \\left( x + Q^{-1}b \\right) $$\n",
    "\n",
    "Then we choose a step size 1 ($\\alpha=1). \n",
    "\n",
    "$$ = Q^{-1} b $$\n",
    "\n",
    "Newtons method solves a quadratic method in 1 step. It goes to the optimal solution. The gradient information. Hashan tells curvature in all dimensions. Treats objective locally as a quadrativ problem around $x^r$. This means that if we can pproximate the problem quadrativ locally. The difficulty is that it's hard to make numerically stable because there is a matrix inversion. This can cause problems. If the Q is not invertible, cannot solve problems.\n",
    "\n",
    "$$ f(x) \\approx f(x^r) + <\\nabla f(x^r), x- x^r) + \\dfrac{1}{2}x - x^r)^t  $$\n",
    "\n",
    "We focus on the general case. As long as the direction d solves the necessary condition. Ibce we decide direction, we can decide step size.\n",
    "\n",
    "$$ \\alpha_r = \\alpha $$\n",
    "\n",
    "Usually use a constant step size. No matter which iteration we are at. What to choose? Minimization rule: Pick $\\alpha_r$ such that $\\alpha_r = arg \\min_{a \\leq 0}f(x^r + \\alpha d^r)$\n",
    "\n",
    "Now $x^r$ and $d^$ are fixed. We looked in one direction (a slice of the objective function). Basically, the stp size is the smallest known value of the function over which we step. Down side is it's another optimization problem. \n",
    "\n",
    "$$ g(\\alpha) = f(x^r + \\alpha d^r) $$\n",
    "\n",
    "The last optio nis diminishing step size: useful in practice. No knowledge of problem. We choose a large step size, then reduce. \n",
    "\n",
    "$$ \\alpha_r \\rightarrow -,\\text{   } \\sum_{r=1}^{\\infty} a_r = \\infty $$\n",
    "\n",
    "Interesting step size; must sum to infinity (pretty sure???). Last one to mention is the Armijo rule: let $\\sigma \\exist (0,\\frac{1}{2})$.  First walk to the wall, test if we achieve descent. If not, we half it and come back half a step. repeat stepping, testing for descent, and if not, back up half a step. \n",
    "\n",
    "Current objective minus last sample is less than inner product of gradient and direction.\n",
    "\n",
    "$$ f(x^r + \\alpha d^r) - f(x^r) \\leq \\sigma \\alpha <\\nabla f(x^r),d^r> $$\n",
    "\n",
    "We will dwaw a picture. slope is $<d^r, \\nabla f(x^r)>$. Second line: slope $\\sigma<d^r, \\nabla f(x^r)>$. Let's say we are at a point, $g(\\alpha)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img  src=\"IMG_0531.JPG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compare with a constant step size, requires some trial and error. We start very aggressive, the nreduce. \n",
    "\n",
    "No matter what stadegey we choose, there should be sufficient descent in the objective of eac hstep. The objective function f(x) seves as a potental toguide the optimization process. These methods are called descent methods for preciesely this reason. As an overview: 2 quesions.\n",
    "\n",
    "When does algorithm converge?\n",
    "How fast does it converge? In the limit \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Lecture Gradient Methods III: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How to decide optimal solution exists:\n",
    "\n",
    "min f(x). Weierstrass theorem. prop A8 in the book. Conditions:\n",
    "\n",
    "1. Given a constrained set, if it is bounded, it must have a minimum. \n",
    "2. Must consider the set $X = \\{x: f(x) \\leq \\gamma \\}$ where $\\gamma$ is some constant bounding the y-axis of the function. x is nonempty bounded. \n",
    "\n",
    "Check convexity of the problem:\n",
    "\n",
    "$$ f(x,y) = || A - xy^T|| $$\n",
    "\n",
    "What we can do: an easier problem (a scalar problem): $f(x,y) = (a - xy)^2$. No we only check the simple case. ONly a 2D problem. Check the Hessian of the matrix:\n",
    "\n",
    "$$ \\nabla f(x,y) = \\left[\\begin{matrix} y^2 & -a \\\\ -a & x^2 \\end{matrix}\\right] $$\n",
    "\n",
    "Play with this, make positive definite. Make diagonals very small. Or plug in numbers. The rest of the problem will be similar for one in class. Remember, for vectors, it is more complex. Must know how to take derivative with vectors. \n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha_r \\nabla f(x^r) , r=0,1,... $$\n",
    "\n",
    "We want to drive hte objective value to decrease. When does the algorithm converge? Look at convergence analysis. How quickly? Rate analysis. Focus on the steepest gradient descent.\n",
    "\n",
    "The most common rule is to use a constant step size ($\\alpha_r = \\alpha$). But how to pick $\\alpha$? Intuition is to make the step size inversely proportional to the maximum curvature of the function. Think about the 1D problem. Max curvature. Because the function is very curved. A stationary solution doesn't really mean anything. Does not give global efficiency. Unless has one stationary solution: then global min. The convergence rate: local analysis assuming already close to a solution, let number of iterations go to infinity. \n",
    "\n",
    "Lets start with first assumpiton: $\\exists L$:\n",
    "\n",
    "$$ \\nabla ^2 f(x) \\succeq LI $$\n",
    "\n",
    "$$ \\left[\\begin{matrix} L & ... & 0\\\\ 0 & L & ...\\\\0 & ... & L \\end{matrix}\\right] - \\nabla^2 f(x) \\geq 0 $$\n",
    "\n",
    "For problems like |x|, very sharp, bad curbature. \n",
    "\n",
    "This implies the curvature of the function is bounded. Then we have something suc hthat the curvature is bounded by a quadratic function. We can use a quadratic function with fixed curvature to upperbound your function. For any point, we should have a quadratic function which only touches your function at one point. \n",
    "\n",
    "$$ f(x) = f(y) + \\langle \\nabla f(y), x-y \\rangle - \\dfrac{1}{2}(x - y)^T\\nabla^2 f(x)(x-y) \\leq f(y) + \\langle \\nabla f(y), x-y \\rangle + \\dfrac{L}{2}||x-y||^2 := u(x;y), \\forall x,y $$\n",
    "\n",
    "We have constructed a quadratic upperbound for f(x), which eif we evauate the bound on y, the right hand side is precisely f(y). At the point of y, we are reducing the funciton. Thus, we want to minimize the quadrativ funciton. If we do this, we move closer to the local minimum. We use this new point to bound the function with anothe quadratic. \n",
    "\n",
    "To minimize, take gradient w.r.t. x.\n",
    "\n",
    "$$ \\nabla(y) + L(x^*-y) = 0 $$\n",
    "$$ x^* = y - \\dfrac{1}{L}\\nabla f(x^*) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the next step! :) $x^*$. Note that the step size is $\\frac{1}{L}$. Makes sense: more curvature, less your step size will be. \n",
    "\n",
    "This is a useful inequality. Proved by mean value theorem (not shown I think). \n",
    "\n",
    "Another set of conditions, but a little more involved. Use in literature. Called the Lipschitz gradient:\n",
    "\n",
    "if difference of gradient is bounded by the L, then the same inequality will also arise. Look at simplest case: f as a quadratic function. This condition says:\n",
    "\n",
    "$$ f(x) = \\dfrac{1}{2}x^T Ax + b^T x $$\n",
    "$$ \\nabla f(x) = Ax + b $$\n",
    "$$ || Ax + b - (Ay - b) || $$\n",
    "$$ = || A(x-y) || \\leq L ||x - y|| $$\n",
    "\n",
    "Shows that this quadratic function always bounds it. The proof is in the book.  Example:\n",
    "\n",
    "$$ x^3 = y^3 + 3 \\langle x-y, y^2\\rangle + \\dfrac{1}{2}(x-y)^2 6y $$\n",
    "\n",
    "As long as we are considering a finite bound, 6y is always bounded. always less than $\\leq y^3 + 3 \\langle x-y ,y^2 \\rangle + 6\\cdot 100/2(x-y)^2$.\n",
    "\n",
    "Altought hte funciton seems to grow quickly, if it is bounded, the curvature is also bounded.\n",
    "\n",
    "Look at another case:\n",
    "\n",
    "$$ x^{r+1} = x^r - \\dfrac{1}{L}\\nabla f(x^r) $$\n",
    "\n",
    "Previously we were at $f(x^r)$. Now war are at $f(x^{r+1})$. How much of a descent occurs between functions? \n",
    "\n",
    "$$ f(x^{r+1} - f(x^r) \\leq \\dfrac{-1}{L}||\\nabla f(x^r)||^2 + \\dfrac{1}{2L} || \\nabla f(x^r) ||^2 \\leq -\\nabla{1}{2L} ||\\nabla f(x^r) ||^2 $$\n",
    "\n",
    "Then we are done with the convergence analysis. If we simply have quadratic bounds:\n",
    "\n",
    "$$ f(x^{r+1} - f(x^r) \\leq -\\\\dfrac{1}{2L}||\\nabla f(x^r) ||^2 $$\n",
    "\n",
    "Two possibilities: gradient doesn't go to zero. This will then continue to decrease to infinity. If we know that f(x) is lower bounded, then the gradient must become zero. Therefore:\n",
    "\n",
    "If f(x) is lower bounded, $\\implies \\nabla f(x^r) \\rightarrow 0$\n",
    "\n",
    "To make this more intuitive, summing up everything from the zeroth iteration, we will have:\n",
    "\n",
    "$$ \\lim_{r \\to \\infty} f(x^{r+1}) - f(x^0) \\leq -\\dfrac{1}{2L}\\sum_{t=0}^r ||\\nabla f(x^t)||^2 $$\n",
    "\n",
    "We know precisely what f(x0) is. The series above must have a limit, and if it does, the tail goes to zero. The direction is precisely gradient direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, welook at nonconstant step size and the direction is no longer gradient direction. The direction d cannot be orthogonal to the gradient.\n",
    "\n",
    "Gradient related condition: For any sequence {xr} converting to a nonstationary point, the corresponding directon {dr} is strictly less than zero.\n",
    "\n",
    "$$ \\lim_{r \\to \\infty} \\langle \\nabla f(x^r), d^r \\angle < 0 $$\n",
    "\n",
    "The Lipschiz continuty equation. The claim is more involved: the step size $\\alpha_r$ has an upper and lower bound.\n",
    "\n",
    "1. $ \\epsilon < \\alpha_r \\leq - \\dfrac{(2-\\epsilon) \\langle \\nabla f(x^r),d^r \\angle}{L||d^r||^2} $\n",
    "\n",
    "2. $ \\alpha_r \\rightarrow 0 $ and $\\sum_{r=1}^{\\infty} \\alpha_r = \\infty$ (i.e. $\\alpha_r = \\frac{1}{r}$\n",
    "\n",
    "Look at 1. Must pick the direction correctly. General assumptions. Look at the upper bound. Inner product of gradient and direction. If we pick direction correclty, it will be negative. With other parts, the entire r.h.s. turns out to be positive. if d is the gradient of x, the directionswill simply cancel out in the top and bottom: $\\epsilon < \\alpha_r \\leq \\dfrac{(2-\\epsilon)}{L}$. The largest step size we can choose is bounded by these. \n",
    "\n",
    "Again, use the quadratic upper bound. If we plug in the quadratic upper bound:\n",
    "\n",
    "$$ f(x^r + \\alpha d^r) - f(x^r) \\leq \\alpha \\langle \\nabla f(x^r), d^r \\rangle + \\dfrac{L}{2}\\alpha^2 ||d^r||^2 $$\n",
    "\n",
    "$$ \\alpha_r \\leq - \\dfrac{(2 - \\epsilon) \\langle \\nabla f(x), d \\rangle}{L ||d||^2} $$\n",
    "\n",
    "$$ \\dfrac{L}{2}\\alpha^2 ||d||^2 $$\n",
    "$$ = \\dfrac{L}{2}\\dfrac{(2-\\epsilon)^2 \\langle \\nabla f(x^r),d \\rangle^2}{L^2 ||d||^2}||d||^2 $$\n",
    "\n",
    "What comes out of the second term is a positive term. We do this for th firs term too. If we plug in the upper gound:\n",
    "\n",
    "If the objective is always decreasing, then:\n",
    "\n",
    "$$ \\langle \\nabla f(x^r), d^r \\rangle < 0 $$\n",
    "\n",
    "For diminishing stepsizes, sama analysis but much more involve.d Key thing, each iteraion will be descending. One thing for diminishing step size, the objective can be increasing. If we don't know the curvature $L$, should generally use $\\alpha_r = \\dfrac{1}{r}$. \n",
    "\n",
    "Next time, we will talk about convergence rate. HW due tomorrow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Midterm is Octover 23rd\n",
    "\n",
    "Gradient descent. Constructing quadratic upper bound.\n",
    "\n",
    "Lets look at the funciton:\n",
    "$$ f(x) = \\dfrac{2}{3}|x|^3 + \\dfrac{1}{2}x^2 $$\n",
    "\n",
    "Apply the gradient descnet with diminishing step size\n",
    "\n",
    "$$ x^{k+1} = x^k - \\dfrac{1}{k}f^\\prime (x^k) $$\n",
    "\n",
    "The direction of this is minux the gradient. Also note the step size is $\\alpha_k = \\dfrac{1}{k}$. Need to calculate the derivative for the absolute values:\n",
    "\n",
    "$$ f^\\prime (x) = \\begin{cases} 2x^2 + x, &  x>0\\\\2x^2 + x, & x < 0  \\end{cases} $$\n",
    "\n",
    "Let's see how the algorithm works out:\n",
    "\n",
    "* $x^1 = 1$ - initial guess\n",
    "* $x^2 = 1-\\dfrac{1}{1}(3) = -2$\n",
    "* $x^3 = -2 - \\dfrac{1}{2}(-2\\cdot4-2) = 3 $\n",
    "* $x^4 = 3 - \\dfrac{1}{3}(18+3) = -4$\n",
    "\n",
    "Hmmm... seems to be getting larger. We are using the gradinet as th direction. This should satisfy several of our conditions. ** The curvature is not bounded!!!** Let's look at the objective, $f(x)$. The absolute value means the curvature isn't bounded. It is useful.\n",
    "\n",
    "The point of having $\\alpha \\to 0$ is so that we can start somewhere, even far away, and arrive at the minimum. \n",
    "\n",
    "$$ f(x^{r+1} \\leq f(x^r) + \\alpha^k \\langle \\nabla f(x^k), d^k \\rangle + \\dfrac{L}{2}(\\alpha^k) || d^k ||^2 $$\n",
    "$$ = f(x^k) + \\alpha^k \\left( - | \\langle d^k, \\nabla f(x^k) \\rangle | + \\dfrac{L}{2}\\alpha^k ||d^k||^2 \\right) $$\n",
    "$$ \\leq f(x^k) + \\alpha^k \\left( -C_1 || \\nabla f(x^k) ||^2 + \\dfrac{L c_2}{2}\\alpha^k ||\\nabla f(x^k)||^2 \\right) $$\n",
    "Taking out the common gradient squared:\n",
    "$$ =f(x^k) + \\alpha^k \\left( -C_1 + \\dfrac{LC_2}{2}\\alpha^k \\right)||\\nabla f(x^k) ||^2 $$\n",
    "\n",
    "Overall, $-C_1$ is negative and $LC_2/2$ is a positive number. We see that $\\alpha^k \\to 0$. However, $C_1$ remains constant. when $k  > \\bar{k}$ where $\\bar{k}$ is the point at which $\\leq f(x^k) + \\alpha^k \\left( \\dfrac{-C_1}{2} \\right) ||\\nabla f(x^k)||^2$. Basically the point at which the $C_2$ term goes to zero. There is no reason to believe the algorithm will descend. But by reducing the step size, when C2< C1, then we will have our descent. In words, since C1 is negative, we have just shown that the next iteration, $f(x^{k+1})$  will be less than the previous iteration $f(x^k)$ minus some constant value. Snazzy! Gurantees descent.\n",
    "\n",
    "$$ \\lim_{T \\to \\infty} f(x^T) - f(x^\\bar{k}) \\leq \\dfrac{C_1}{2}\\sum_{k = \\bar{k}} ^ T - \\alpha^k ||\\nabla f(x^k) ||^2 $$\n",
    "\n",
    "Is it possible that in the limit, $|| \\nabla f(x^k) || \\to \\epsilon > 0 $. In the limit is it strictly bounded away from zero? If this happens, then\n",
    "\n",
    "$$ -\\sum_{k = \\bar{k}} ^ \\infty \\alpha^k \\epsilon $$\n",
    "\n",
    "Is it possible that in the limit goes to zero? Hopefully! otherwise theres no minimum pretty sure.\n",
    "\n",
    "A simple regression proglem: Predict the price of the house by lining area:\n",
    "\n",
    "$$ \\left[\\begin{matrix} Living ARea & Price \\\\ 5719 & 567 \\\\ 3241 & 345\\\\ ... & ...  \\end{matrix}\\right] $$\n",
    "\n",
    "Basically we want to plot the slope and the intercept. \n",
    "\n",
    "$$ \\sum_{i=1}^N \\left(x_1 A_i + x_2 - P_i\\right)^2 $$\n",
    "\n",
    "This is our model. Construct a data matrix.\n",
    "\n",
    "$$ \\rvert \\rvert \\left[\\begin{matrix} A_1 & 1\\\\ A_2 & 1 \\\\ ... & ...\\\\ A_N & 1 \\end{matrix}\\right] \\left[\\begin{matrix} x_1 \\\\ x_2 \\end{matrix}\\right] - P \\rvert \\rvert^2 $$\n",
    "\n",
    "First try, we will multiple all the areas such that the price and the area are within a similar magnitude. Poor fitting occurs the first time.  If we scale differently again, we see a convergence. The convergence rate for the two were different. If we look at the eigenvalues of the first, the eigenvlaues of $A^\\prime A = $. The second scaling has eigenvalues of $A^\\prime A = 0.4, 18.448$.\n",
    "\n",
    "First, define an optimal solution as $\\epsilon$: $\\{x_\\epsilon := f(x^r) - f^* \\leq \\epsilon\\}$\n",
    "\n",
    "Convergence Rate: Measures the numer of iterations requared to gan an $\\epsilon$ optimal solution. Gives global behavior of the algorithm. A popular and important measure for evaluating algorithms in big data. What determines the convergence rate? If we want to decide how fast we approach the wall, per se.\n",
    "\n",
    "We look at a family of functions. Linear convergence means the error is shrunk by a constant factor at each iteration. \n",
    "\n",
    "$$ e^{r+1} = \\beta e^r $$\n",
    "$$ = \\beta^r e^0 $$\n",
    "$$ \\log(e^{r+1}) = r\\log(\\beta) + \\log(e^0) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take example $f(x) = x^2 - 2x$. Check to see if it satisfies the bound\n",
    "\n",
    "$$ || f^\\prime (x) - f^\\prime (y)|| \\leq |x - y|$ \n",
    "\n",
    "plug values in\n",
    "\n",
    "$$ || x+2 - (y+2) \\leq |x - y| $$\n",
    "\n",
    "This is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we spoke about this problem:\n",
    "    \n",
    "$$f(x) = \\dfrac{2}{3}|x|^3 + \\dfrac{1}{2}x^2 $$\n",
    "    \n",
    "If we apply gradient descent, it will diverge. Let's double check:\n",
    "\n",
    "$$ |f(x) - f(y)| \\leq |x-y| L|x-y| \\forall (x,y) $$\n",
    "\n",
    "We mentioned the gradient is:\n",
    "\n",
    "$$ f^\\prime (x) = 2x^2 \\text{sgn}(x) + x, x\\neq 0 $$\n",
    "\n",
    "$$ 2x^2 \\text{sgn}(x) - 2y^2 \\text{sgn}(y) + x-y $$\n",
    "$$ \\leq |2x^2 \\text{sgn}(x) - 2y^2 \\text{sgn}(y)| + |x-y| $$\n",
    "\n",
    "We want a constant L; in this example, L=1. Can we bound this one? Assume x and y have the same sign.\n",
    "\n",
    "$$ \\leq |2x^2 - 2y^2| + |x-y| $$\n",
    "$$ \\leq 2|x+y||x-y| + |x-y| $$\n",
    "$$ = (2|x+y| + 1)|x-y| $$\n",
    "\n",
    "Because there is no bound on this, it is unbounded.\n",
    "\n",
    "Exercise: show the function $f(x) = |x|$, $x\\neq 0$.\n",
    "\n",
    "$$ f^\\prime (x) = \\text{sgn}(x) $$\n",
    "$$ |f^\\prime (x) - f^\\prime (y) = |\\text{sgn}(x) - \\text{sgn}(y)| $$\n",
    "$$ = 2 \\nleq L|x-y| $$\n",
    "\n",
    "\n",
    "Last time, we talked about the example of regression. They must converge in a different manner. They must behave w.r.t the convergence rate. The error, $e(x^r)= f(x^r) - f(x^*) \\geq 0$. The error will decrease in the following manner:\n",
    "\n",
    "$$ e^{r+1}(x) \\leq \\beta e(x^r) \\text{, } \\beta \\in (0,1) $$\n",
    "\n",
    "Recall $x^*$ is the global min. This type of behavior is linear convergence. It will reduce in a line. This is a strongly convex function. If we call $f(x)$ as convex, it will satisfy the condition iff:\n",
    "\n",
    "$$ f(y) \\geq f(x) + \\langle \\nabla f(x) ,y-x \\rangle $$\n",
    "\n",
    "If we draw a line at any given point x, the function is always larger than that line. Also possible to say that the curvature is greater than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must define a function which is strongly convex. This is strongly convex if we can fit a quadtratic function below f(x) instead of a line.\n",
    "\n",
    "Ex: $f(y) = y^2$. This is a strongly convex function. A convex function will allow a very flat region. Therefore, intuitively, a fucntion is strongly convex if it is convex and has no \"flat\" regions. Another way of lookin at this is that tif the largest eigenvalue of a is greater than 0. Largest eigenvalue gives the largest change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression problem most likely to solve this. We are ready to characterize this as athe gradient descent.\n",
    "\n",
    "* step 1 $f(x^r) - f(x^{r+1}) \\geq \\dfrac{1}{2L}||\\nabla f(x^r) ||^2$\n",
    "* step 2 $f(x^{r+1} - f(x^*) \\leq \\dfrac{1}{2\\sigma}||\\nabla f(x^r)||^2$\n",
    "\n",
    "Let's look at the second one, how we got there. For the second one, we need to use strong convexity assumption. it says the following:\n",
    "\n",
    "$$ f(x^*) \\geq f(x^r) + \\langle \\nabla f(x^r), x^* - x^r \\rangle + \\dfrac{\\sigma}{2}||x^r - x^*||^2 $$\n",
    "\n",
    "What we do is minimize the r.h.s. over $x^*$, we obtain the optimal solution: $x^* = x^r - \\dfrac{1}{\\sigma}\\nabla f(x^r)$. Because we minimize the r.h.s, we can make another inequality:\n",
    "\n",
    "$$ f(x^*) \\geq f(x^*) - \\dfrac{1}{2\\sigma} ||\\nabla f(x^r) ||^2 $$\n",
    "\n",
    "Now what can we do. L is the largest curvature, $\\sigma$ is the smallest. From the first step, we have:\n",
    "\n",
    "$$ f(x^{r+1}) \\leq f(x^r) - \\dfrac{1}{2}L ||\\nabla f(x^r)||^2$$\n",
    "\n",
    "Remember, we want to characterize error. Error is $f(x^{r+1}) - f(x^*)$. By subtracting this from both sides we get:\n",
    "$$ f(x^{r+1}) - f(x^*) \\leq f(x^r) - \\dfrac{1}{2}L ||\\nabla f(x^r)||^2 - f(x^*)$$\n",
    "$$ e^{r+1} \\leq e^r - \\dfrac{1}{2L}||\\nabla f(x^r) ||^2 $$\n",
    "\n",
    "Then we bring in an organized version of step 2 from earlier. (both sides have been multiplied by $-2\\sigma$.: $-2\\sigma (f(x^{r+1} - f(x^*)) \\geq ||\\nabla f(x^r)||^2$. Plut this in:\n",
    "\n",
    "$$ \\leq e^r + \\dfrac{1}{2L} \\left( -2\\sigma(f(x^r) - f(x^*))\\right) $$\n",
    "\n",
    "we now see another error. Let's reorganize:\n",
    "\n",
    "$$ = e^r \\left( 1 - \\dfrac{2\\sigma}{2 L} \\right) $$\n",
    "$$ = e^r \\left( 1 - \\dfrac{\\sigma}{L} \\right) $$\n",
    "\n",
    "We define $\\beta = \\left( 1 - \\dfrac{\\sigma}{L} \\right)$. What we are getting at is that the error is decreasing this way. The $\\beta < 1$, and how small it is depends on the eigenvalue of the hessian matrix. If the condition number, $L/\\sigma$, of f is large, it is ill-condition and will be slow to converge. If it is small, it is well conditioned. \n",
    "\n",
    "\n",
    "We know that the simulation error should now obey this rule:\n",
    "\n",
    "$$ e(x^r) \\leq \\beta^r e(x^0) $$\n",
    "\n",
    "If e want to minimize $||Ax - b||^2$. The error wil lbe bounded by the objective value. We should know $e(x^0)$ because the difference betwen the iniial gues s and the mniimum is 0: $f(x^0) - f(x^*) \\leq f(x^*)$. Question: how many iterations do we need so that $e(x^r) \\leq 10^{-4}$.  In orer for the error to be less than our constraint, it is siffucient to have:\n",
    "\n",
    "$$ \\beta^r e(x^0) \\leq \\epsilon $$\n",
    "$$ r\\log(\\beta) + \\log(e(x^0)) \\leq \\log (\\epsilon) $$\n",
    "\n",
    "$$ r \\leq \\dfrac{-\\log(\\dfrac{e(x^0)}{\\epsilon}}{\\log \\beta} $$\n",
    "\n",
    "So if we want to achieve $\\epsilon = 10^{-4}$, $\\beta$ must be fairly large. It must also be strongly convex!!! :) There is some trade offs. In order to say something is stornger, we need to make more assumptions. As we can see, the condition number plays a role here. As we explain the the last lectures examples with price per room in feet. :\n",
    "\n",
    "$$ ||Ax - b||^2 $$\n",
    "\n",
    "Here, A is the squared feet, b is the price. \n",
    "* $L:= \\sigma \\text{ max} (A^TA)$\n",
    "* $\\sigma = \\sigma \\text{ min} (A^T A)$\n",
    "* Condition Number: $\\dfrac{L}{\\sigma}$\n",
    "\n",
    "Would be agood idea to do some preprocessing before attacking this problem again. We can normalize the A and center the bs.:\n",
    "\n",
    "$$ A^\\prime _k = \\dfrac{A_k - \\bar{A}_k}{\\sigma A_k} $$\n",
    "$$ b^\\prime = b- \\bar{b} $$\n",
    "\n",
    "Once we obtain $A^\\prime _k$ we can transform back using $A_k = \\sigma A_k A^\\prime _k + \\bar{A}_k$\n",
    "\n",
    "for the L and $\\sigma$, how do we know if it is fast or slow? Look at $\\beta$.\n",
    "\n",
    "$$ \\beta = \\left( 1 - \\dfrac{\\sigma}{L} \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Order Methods:\n",
    "\n",
    "How will we stop the iterations? For gradient descent:\n",
    "\n",
    "$$ x^{r+1} - x^r = \\alpha^r \\nabla f(x^r) $$\n",
    "\n",
    "If the step size is constant, it may be fine; but what if it is very small? Not a good method to determine if we are done iterating. The normalized gradient instead should be measured. \n",
    "\n",
    "$$ \\dfrac{|| \\nabla f(x^r) || }{|| \\nabla f(x^0)||} \\leq \\epsilon $$\n",
    "\n",
    "the second thing to mention is this: we can always add in a spacer step between our iterations. Gradinet descent can be combined with many other algorithms. Last thing to mention: gradient error. At this time, all our analysis has assumed a perfect access to the gradient. In practice this rarely happens. Take a case:\n",
    "\n",
    "* case 1 $ ||e^k|| \\leq |x| \\cdot ||\\nabla f(x^r)|| $\n",
    "* case 2 $||e^k|| \\eq \\delta $. Causes oscillations in error (see explanation below)\n",
    "* case 3 $ ||e^k|| $ is a random variable\n",
    "\n",
    "For the first for the gradient, the descent, if there is no error:\n",
    "\n",
    "$$ d^r = \\nabla(x^r) + e^r $$\n",
    "$$ \\leq - \\langle \\nabla f(x^r) , d^r \\rangle $$\n",
    "$$ = ||\\nabla f(x^r) ||^2 - \\langle e^r, \\nabla f(x^r) \\rangle $$\n",
    "$$ \\leq ||\\nabla f(x^r) ||^2 - ||e^r|| ||\\nabla f(x^r))|| $$\n",
    "\n",
    "In this case, we hvae case 1 (i think)??? The error is always smaller than the gradient, therefore it is always descending.\n",
    "\n",
    "For case 2, we have:\n",
    "\n",
    "$$ \\geq ||\\nabla f(x^r)||(||\\nabla f(x^r) || - \\delta) $$\n",
    "\n",
    "In this case, the term in parenthesis is positive when the gradient is large. When the gradient is small, it is zero. This will cause oscillations.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
