{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    var add_command_shortcuts = {\n",
       "            'Alt-w' : {\n",
       "                help    : 'Add Text Test',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                        var cell = IPython.notebook.get_selected_cell();\n",
       "        \t\tcell.code_mirror.replaceSelection('Testing');\n",
       "                }\n",
       "            }\n",
       "        };\n",
       "\n",
       "    var add_edit_shortcuts = {\n",
       "            'Alt-a' : {\n",
       "                help    : 'Insert alpha',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\alpha');\n",
       "                }\n",
       "            },\n",
       "        'Alt-b' : {\n",
       "                help    : 'Insert beta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\beta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-g' : {\n",
       "                help    : 'Insert gamma',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\gamma');\n",
       "                }\n",
       "            },\n",
       "        'Alt-shift-g' : {\n",
       "                help    : 'Insert Gamma',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\Gamma');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Ctrl-d' : { //Change\n",
       "                help    : 'Insert delta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\delta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Ctrl-e' : {\n",
       "                help    : 'Insert epsilon',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\epsilon');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Ctrl-Shift-e' : {\n",
       "                help    : 'Insert varepsilon',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\varepsilon');\n",
       "                }\n",
       "            },\n",
       "        'Alt-z' : {\n",
       "                help    : 'Insert zeta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\zeta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-t' : {\n",
       "                help    : 'Insert theta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\theta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-k' : {\n",
       "                help    : 'Insert kappa',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\kappa');\n",
       "                }\n",
       "            },\n",
       "        'Alt-l' : {\n",
       "                help    : 'Insert lambda',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\lambda');\n",
       "                }\n",
       "            },\n",
       "        'Alt-p' : {\n",
       "                help    : 'Insert prime',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\prime');\n",
       "                }\n",
       "            },\n",
       "        'Alt-r' : {\n",
       "                help    : 'Insert rho',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\rho');\n",
       "                }\n",
       "            },\n",
       "        'Alt-s' : {\n",
       "                help    : 'Insert sigma',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\sigma');\n",
       "                }\n",
       "            },\n",
       "        'Alt-w' : {\n",
       "                help    : 'Insert omega',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\omega');\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-1' : {\n",
       "                help    : 'Insert dfrac',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\dfrac{}{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-2' : {\n",
       "                help    : 'Insert equation',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('$$  $$');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-3' : {\n",
       "                help    : 'Insert bar',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\bar{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-4' : {\n",
       "                help    : 'Insert hat',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\hat{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-5' : {\n",
       "                help    : 'Insert hat',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\tilde{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-6' : {\n",
       "                help    : 'Insert parenthesis',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\left(  \\\\right)');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-7' : {\n",
       "                help    : 'Insert Brackets',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\left[  \\\\right]');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-8' : {\n",
       "                help    : 'Insert angle',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\langle  \\\\rangle');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-9' : {\n",
       "                help    : 'Insert Matrix',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\left[\\\\begin{matrix}  \\\\end{matrix}\\\\right]');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-20});\n",
       "                }\n",
       "            },\n",
       "        'Alt-0' : {\n",
       "                help    : 'Insert partial',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\partial ');\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-n' : {\n",
       "                help    : 'Insert nabla',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\nabla');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Shift-t' : {\n",
       "                help    : 'Insert text',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\text{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "/*        'Shift-6' : {\n",
       "                help    : 'Insert superscript',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('^{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "        'Shift-_' : {\n",
       "                help    : 'Insert subscript',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('_{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "*/\n",
       "        };\n",
       "\n",
       "   // var load_ipython_extension = function() {\n",
       "        IPython.keyboard_manager.edit_shortcuts.add_shortcuts(add_edit_shortcuts);\n",
       "        IPython.keyboard_manager.command_shortcuts.add_shortcuts(add_command_shortcuts);\n",
       "    //};"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "    var add_command_shortcuts = {\n",
    "            'Alt-w' : {\n",
    "                help    : 'Add Text Test',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                        var cell = IPython.notebook.get_selected_cell();\n",
    "        \t\tcell.code_mirror.replaceSelection('Testing');\n",
    "                }\n",
    "            }\n",
    "        };\n",
    "\n",
    "    var add_edit_shortcuts = {\n",
    "            'Alt-a' : {\n",
    "                help    : 'Insert alpha',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\alpha');\n",
    "                }\n",
    "            },\n",
    "        'Alt-b' : {\n",
    "                help    : 'Insert beta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\beta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-g' : {\n",
    "                help    : 'Insert gamma',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\gamma');\n",
    "                }\n",
    "            },\n",
    "        'Alt-shift-g' : {\n",
    "                help    : 'Insert Gamma',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\Gamma');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Ctrl-d' : { //Change\n",
    "                help    : 'Insert delta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\delta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Ctrl-e' : {\n",
    "                help    : 'Insert epsilon',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\epsilon');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Ctrl-Shift-e' : {\n",
    "                help    : 'Insert varepsilon',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\varepsilon');\n",
    "                }\n",
    "            },\n",
    "        'Alt-z' : {\n",
    "                help    : 'Insert zeta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\zeta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-t' : {\n",
    "                help    : 'Insert theta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\theta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-k' : {\n",
    "                help    : 'Insert kappa',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\kappa');\n",
    "                }\n",
    "            },\n",
    "        'Alt-l' : {\n",
    "                help    : 'Insert lambda',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\lambda');\n",
    "                }\n",
    "            },\n",
    "        'Alt-p' : {\n",
    "                help    : 'Insert prime',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\prime');\n",
    "                }\n",
    "            },\n",
    "        'Alt-r' : {\n",
    "                help    : 'Insert rho',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\rho');\n",
    "                }\n",
    "            },\n",
    "        'Alt-s' : {\n",
    "                help    : 'Insert sigma',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\sigma');\n",
    "                }\n",
    "            },\n",
    "        'Alt-w' : {\n",
    "                help    : 'Insert omega',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\omega');\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-1' : {\n",
    "                help    : 'Insert dfrac',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\dfrac{}{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-2' : {\n",
    "                help    : 'Insert equation',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('$$  $$');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-3' : {\n",
    "                help    : 'Insert bar',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\bar{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-4' : {\n",
    "                help    : 'Insert hat',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\hat{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-5' : {\n",
    "                help    : 'Insert hat',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\tilde{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-6' : {\n",
    "                help    : 'Insert parenthesis',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\left(  \\\\right)');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-7' : {\n",
    "                help    : 'Insert Brackets',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\left[  \\\\right]');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-8' : {\n",
    "                help    : 'Insert angle',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\langle  \\\\rangle');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-9' : {\n",
    "                help    : 'Insert Matrix',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\left[\\\\begin{matrix}  \\\\end{matrix}\\\\right]');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-20});\n",
    "                }\n",
    "            },\n",
    "        'Alt-0' : {\n",
    "                help    : 'Insert partial',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\partial ');\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-n' : {\n",
    "                help    : 'Insert nabla',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\nabla');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Shift-t' : {\n",
    "                help    : 'Insert text',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\text{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "/*        'Shift-6' : {\n",
    "                help    : 'Insert superscript',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('^{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "        'Shift-_' : {\n",
    "                help    : 'Insert subscript',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('_{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "*/\n",
    "        };\n",
    "\n",
    "   // var load_ipython_extension = function() {\n",
    "        IPython.keyboard_manager.edit_shortcuts.add_shortcuts(add_edit_shortcuts);\n",
    "        IPython.keyboard_manager.command_shortcuts.add_shortcuts(add_command_shortcuts);\n",
    "    //};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "if the alpha is small enough, the ratio is always bounded by a half $\\epsilon$. If this is true, we can simply plug it in.\n",
    "\n",
    "$$ \\leq f(x) + \\alpha d^T \\nabla f(x) + \\dfrac{1}{2}\\epsilon\\alpha  $$\n",
    "$$ = f(x) - \\alpha \\epsilon + \\dfrac{1}{2}\\epsilon \\alpha = f(x)  - \\dfrac{1}{2} \\epsilon \\alpha $$\n",
    "\n",
    "The limit\n",
    "\n",
    "$$\\lim_{\\alpha \\to 0} g(\\alpha) < \\epsilon)$$\n",
    "\n",
    "gradient direction. For minimizing quadratic function, if step size is too large, it diverges. If too small, no progress is being made. We want to pick the stop size such that progress is made without making the algorithm unstable. \n",
    "\n",
    "Algorithm:\n",
    "\n",
    "$$ x^{r+1} = x^r + \\alpha_rd^r, r=0,1,...$$\n",
    "\n",
    "where $\\nabla f(x^r) \\neq 0$, the direction of d satisfies $\\nabla f(x^r)d^r < 0$, and $\\alpha^r$ is a positive step size. Use steepest descent.\n",
    "\n",
    "General case: Gradient descent method\n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha_rD^r\\nabla f(x^r), r=0,1,...$$\n",
    "\n",
    "Where D is a positive definite matrix called a scaling matrix. This helps to scale things down. (units can be miles). Units aren't the same.\n",
    "\n",
    "Special Case I: D is an Identity matrix. This is steepest descent.\n",
    "Special case II: Newtons method. Specialization where D is the inversion of the Hashan matrix. \n",
    "\n",
    "In practise, this is generally the behavior we see (zig-zag behavior). The local gradient direction does not correspond to global direction. Newton's method, on the other hand, is very good at solving quadratic problems. Think about what it is doing. Say we have a quadratic problem. How does Newton's method solve it. \n",
    "\n",
    "$$ f(x) = x^T Qx + bx $$\n",
    "\n",
    "Want to appl yNewton method to it. \n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha Q^{-1} \\left( Qx + b \\right)  $$\n",
    "\n",
    "Q is the Hashan.\n",
    "\n",
    "$$ = x^T - \\alpha \\left( x + Q^{-1}b \\right) $$\n",
    "\n",
    "Then we choose a step size 1 ($\\alpha=1). \n",
    "\n",
    "$$ = Q^{-1} b $$\n",
    "\n",
    "Newtons method solves a quadratic method in 1 step. It goes to the optimal solution. The gradient information. Hashan tells curvature in all dimensions. Treats objective locally as a quadrativ problem around $x^r$. This means that if we can pproximate the problem quadrativ locally. The difficulty is that it's hard to make numerically stable because there is a matrix inversion. This can cause problems. If the Q is not invertible, cannot solve problems.\n",
    "\n",
    "$$ f(x) \\approx f(x^r) + <\\nabla f(x^r), x- x^r) + \\dfrac{1}{2}x - x^r)^t  $$\n",
    "\n",
    "We focus on the general case. As long as the direction d solves the necessary condition. Ibce we decide direction, we can decide step size.\n",
    "\n",
    "$$ \\alpha_r = \\alpha $$\n",
    "\n",
    "Usually use a constant step size. No matter which iteration we are at. What to choose? Minimization rule: Pick $\\alpha_r$ such that $\\alpha_r = arg \\min_{a \\leq 0}f(x^r + \\alpha d^r)$\n",
    "\n",
    "Now $x^r$ and $d^$ are fixed. We looked in one direction (a slice of the objective function). Basically, the stp size is the smallest known value of the function over which we step. Down side is it's another optimization problem. \n",
    "\n",
    "$$ g(\\alpha) = f(x^r + \\alpha d^r) $$\n",
    "\n",
    "The last optio nis diminishing step size: useful in practice. No knowledge of problem. We choose a large step size, then reduce. \n",
    "\n",
    "$$ \\alpha_r \\rightarrow -,\\text{   } \\sum_{r=1}^{\\infty} a_r = \\infty $$\n",
    "\n",
    "Interesting step size; must sum to infinity (pretty sure???). Last one to mention is the Armijo rule: let $\\sigma \\exist (0,\\frac{1}{2})$.  First walk to the wall, test if we achieve descent. If not, we half it and come back half a step. repeat stepping, testing for descent, and if not, back up half a step. \n",
    "\n",
    "Current objective minus last sample is less than inner product of gradient and direction.\n",
    "\n",
    "$$ f(x^r + \\alpha d^r) - f(x^r) \\leq \\sigma \\alpha <\\nabla f(x^r),d^r> $$\n",
    "\n",
    "We will dwaw a picture. slope is $<d^r, \\nabla f(x^r)>$. Second line: slope $\\sigma<d^r, \\nabla f(x^r)>$. Let's say we are at a point, $g(\\alpha)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img  src=\"IMG_0531.JPG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compare with a constant step size, requires some trial and error. We start very aggressive, the nreduce. \n",
    "\n",
    "No matter what stadegey we choose, there should be sufficient descent in the objective of eac hstep. The objective function f(x) seves as a potental toguide the optimization process. These methods are called descent methods for preciesely this reason. As an overview: 2 quesions.\n",
    "\n",
    "When does algorithm converge?\n",
    "How fast does it converge? In the limit \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Lecture Gradient Methods III: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How to decide optimal solution exists:\n",
    "\n",
    "min f(x). Weierstrass theorem. prop A8 in the book. Conditions:\n",
    "\n",
    "1. Given a constrained set, if it is bounded, it must have a minimum. \n",
    "2. Must consider the set $X = \\{x: f(x) \\leq \\gamma \\}$ where $\\gamma$ is some constant bounding the y-axis of the function. x is nonempty bounded. \n",
    "\n",
    "Check convexity of the problem:\n",
    "\n",
    "$$ f(x,y) = || A - xy^T|| $$\n",
    "\n",
    "What we can do: an easier problem (a scalar problem): $f(x,y) = (a - xy)^2$. No we only check the simple case. ONly a 2D problem. Check the Hessian of the matrix:\n",
    "\n",
    "$$ \\nabla f(x,y) = \\left[\\begin{matrix} y^2 & -a \\\\ -a & x^2 \\end{matrix}\\right] $$\n",
    "\n",
    "Play with this, make positive definite. Make diagonals very small. Or plug in numbers. The rest of the problem will be similar for one in class. Remember, for vectors, it is more complex. Must know how to take derivative with vectors. \n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha_r \\nabla f(x^r) , r=0,1,... $$\n",
    "\n",
    "We want to drive hte objective value to decrease. When does the algorithm converge? Look at convergence analysis. How quickly? Rate analysis. Focus on the steepest gradient descent.\n",
    "\n",
    "The most common rule is to use a constant step size ($\\alpha_r = \\alpha$). But how to pick $\\alpha$? Intuition is to make the step size inversely proportional to the maximum curvature of the function. Think about the 1D problem. Max curvature. Because the function is very curved. A stationary solution doesn't really mean anything. Does not give global efficiency. Unless has one stationary solution: then global min. The convergence rate: local analysis assuming already close to a solution, let number of iterations go to infinity. \n",
    "\n",
    "Lets start with first assumpiton: $\\exists L$:\n",
    "\n",
    "$$ \\nabla ^2 f(x) \\succeq LI $$\n",
    "\n",
    "$$ \\left[\\begin{matrix} L & ... & 0\\\\ 0 & L & ...\\\\0 & ... & L \\end{matrix}\\right] - \\nabla^2 f(x) \\geq 0 $$\n",
    "\n",
    "For problems like |x|, very sharp, bad curbature. \n",
    "\n",
    "This implies the curvature of the function is bounded. Then we have something suc hthat the curvature is bounded by a quadratic function. We can use a quadratic function with fixed curvature to upperbound your function. For any point, we should have a quadratic function which only touches your function at one point. \n",
    "\n",
    "$$ f(x) = f(y) + \\langle \\nabla f(y), x-y \\rangle - \\dfrac{1}{2}(x - y)^T\\nabla^2 f(x)(x-y) \\leq f(y) + \\langle \\nabla f(y), x-y \\rangle + \\dfrac{L}{2}||x-y||^2 := u(x;y), \\forall x,y $$\n",
    "\n",
    "We have constructed a quadratic upperbound for f(x), which eif we evauate the bound on y, the right hand side is precisely f(y). At the point of y, we are reducing the funciton. Thus, we want to minimize the quadrativ funciton. If we do this, we move closer to the local minimum. We use this new point to bound the function with anothe quadratic. \n",
    "\n",
    "To minimize, take gradient w.r.t. x.\n",
    "\n",
    "$$ \\nabla(y) + L(x^*-y) = 0 $$\n",
    "$$ x^* = y - \\dfrac{1}{L}\\nabla f(x^*) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the next step! :) $x^*$. Note that the step size is $\\frac{1}{L}$. Makes sense: more curvature, less your step size will be. \n",
    "\n",
    "This is a useful inequality. Proved by mean value theorem (not shown I think). \n",
    "\n",
    "Another set of conditions, but a little more involved. Use in literature. Called the Lipschitz gradient:\n",
    "\n",
    "if difference of gradient is bounded by the L, then the same inequality will also arise. Look at simplest case: f as a quadratic function. This condition says:\n",
    "\n",
    "$$ f(x) = \\dfrac{1}{2}x^T Ax + b^T x $$\n",
    "$$ \\nabla f(x) = Ax + b $$\n",
    "$$ || Ax + b - (Ay - b) || $$\n",
    "$$ = || A(x-y) || \\leq L ||x - y|| $$\n",
    "\n",
    "Shows that this quadratic function always bounds it. The proof is in the book.  Example:\n",
    "\n",
    "$$ x^3 = y^3 + 3 \\langle x-y, y^2\\rangle + \\dfrac{1}{2}(x-y)^2 6y $$\n",
    "\n",
    "As long as we are considering a finite bound, 6y is always bounded. always less than $\\leq y^3 + 3 \\langle x-y ,y^2 \\rangle + 6\\cdot 100/2(x-y)^2$.\n",
    "\n",
    "Altought hte funciton seems to grow quickly, if it is bounded, the curvature is also bounded.\n",
    "\n",
    "Look at another case:\n",
    "\n",
    "$$ x^{r+1} = x^r - \\dfrac{1}{L}\\nabla f(x^r) $$\n",
    "\n",
    "Previously we were at $f(x^r)$. Now war are at $f(x^{r+1})$. How much of a descent occurs between functions? \n",
    "\n",
    "$$ f(x^{r+1} - f(x^r) \\leq \\dfrac{-1}{L}||\\nabla f(x^r)||^2 + \\dfrac{1}{2L} || \\nabla f(x^r) ||^2 \\leq -\\nabla{1}{2L} ||\\nabla f(x^r) ||^2 $$\n",
    "\n",
    "Then we are done with the convergence analysis. If we simply have quadratic bounds:\n",
    "\n",
    "$$ f(x^{r+1} - f(x^r) \\leq -\\\\dfrac{1}{2L}||\\nabla f(x^r) ||^2 $$\n",
    "\n",
    "Two possibilities: gradient doesn't go to zero. This will then continue to decrease to infinity. If we know that f(x) is lower bounded, then the gradient must become zero. Therefore:\n",
    "\n",
    "If f(x) is lower bounded, $\\implies \\nabla f(x^r) \\rightarrow 0$\n",
    "\n",
    "To make this more intuitive, summing up everything from the zeroth iteration, we will have:\n",
    "\n",
    "$$ \\lim_{r \\to \\infty} f(x^{r+1}) - f(x^0) \\leq -\\dfrac{1}{2L}\\sum_{t=0}^r ||\\nabla f(x^t)||^2 $$\n",
    "\n",
    "We know precisely what f(x0) is. The series above must have a limit, and if it does, the tail goes to zero. The direction is precisely gradient direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, welook at nonconstant step size and the direction is no longer gradient direction. The direction d cannot be orthogonal to the gradient.\n",
    "\n",
    "Gradient related condition: For any sequence {xr} converting to a nonstationary point, the corresponding directon {dr} is strictly less than zero.\n",
    "\n",
    "$$ \\lim_{r \\to \\infty} \\langle \\nabla f(x^r), d^r \\angle < 0 $$\n",
    "\n",
    "The Lipschiz continuty equation. The claim is more involved: the step size $\\alpha_r$ has an upper and lower bound.\n",
    "\n",
    "1. $ \\epsilon < \\alpha_r \\leq - \\dfrac{(2-\\epsilon) \\langle \\nabla f(x^r),d^r \\angle}{L||d^r||^2} $\n",
    "\n",
    "2. $ \\alpha_r \\rightarrow 0 $ and $\\sum_{r=1}^{\\infty} \\alpha_r = \\infty$ (i.e. $\\alpha_r = \\frac{1}{r}$\n",
    "\n",
    "Look at 1. Must pick the direction correctly. General assumptions. Look at the upper bound. Inner product of gradient and direction. If we pick direction correclty, it will be negative. With other parts, the entire r.h.s. turns out to be positive. if d is the gradient of x, the directionswill simply cancel out in the top and bottom: $\\epsilon < \\alpha_r \\leq \\dfrac{(2-\\epsilon)}{L}$. The largest step size we can choose is bounded by these. \n",
    "\n",
    "Again, use the quadratic upper bound. If we plug in the quadratic upper bound:\n",
    "\n",
    "$$ f(x^r + \\alpha d^r) - f(x^r) \\leq \\alpha \\langle \\nabla f(x^r), d^r \\rangle + \\dfrac{L}{2}\\alpha^2 ||d^r||^2 $$\n",
    "\n",
    "$$ \\alpha_r \\leq - \\dfrac{(2 - \\epsilon) \\langle \\nabla f(x), d \\rangle}{L ||d||^2} $$\n",
    "\n",
    "$$ \\dfrac{L}{2}\\alpha^2 ||d||^2 $$\n",
    "$$ = \\dfrac{L}{2}\\dfrac{(2-\\epsilon)^2 \\langle \\nabla f(x^r),d \\rangle^2}{L^2 ||d||^2}||d||^2 $$\n",
    "\n",
    "What comes out of the second term is a positive term. We do this for th firs term too. If we plug in the upper gound:\n",
    "\n",
    "If the objective is always decreasing, then:\n",
    "\n",
    "$$ \\langle \\nabla f(x^r), d^r \\rangle < 0 $$\n",
    "\n",
    "For diminishing stepsizes, sama analysis but much more involve.d Key thing, each iteraion will be descending. One thing for diminishing step size, the objective can be increasing. If we don't know the curvature $L$, should generally use $\\alpha_r = \\dfrac{1}{r}$. \n",
    "\n",
    "Next time, we will talk about convergence rate. HW due tomorrow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Midterm is Octover 23rd\n",
    "\n",
    "Gradient descent. Constructing quadratic upper bound.\n",
    "\n",
    "Lets look at the funciton:\n",
    "$$ f(x) = \\dfrac{2}{3}|x|^3 + \\dfrac{1}{2}x^2 $$\n",
    "\n",
    "Apply the gradient descnet with diminishing step size\n",
    "\n",
    "$$ x^{k+1} = x^k - \\dfrac{1}{k}f^\\prime (x^k) $$\n",
    "\n",
    "The direction of this is minux the gradient. Also note the step size is $\\alpha_k = \\dfrac{1}{k}$. Need to calculate the derivative for the absolute values:\n",
    "\n",
    "$$ f^\\prime (x) = \\begin{cases} 2x^2 + x, &  x>0\\\\2x^2 + x, & x < 0  \\end{cases} $$\n",
    "\n",
    "Let's see how the algorithm works out:\n",
    "\n",
    "* $x^1 = 1$ - initial guess\n",
    "* $x^2 = 1-\\dfrac{1}{1}(3) = -2$\n",
    "* $x^3 = -2 - \\dfrac{1}{2}(-2\\cdot4-2) = 3 $\n",
    "* $x^4 = 3 - \\dfrac{1}{3}(18+3) = -4$\n",
    "\n",
    "Hmmm... seems to be getting larger. We are using the gradinet as th direction. This should satisfy several of our conditions. ** The curvature is not bounded!!!** Let's look at the objective, $f(x)$. The absolute value means the curvature isn't bounded. It is useful.\n",
    "\n",
    "The point of having $\\alpha \\to 0$ is so that we can start somewhere, even far away, and arrive at the minimum. \n",
    "\n",
    "$$ f(x^{r+1} \\leq f(x^r) + \\alpha^k \\langle \\nabla f(x^k), d^k \\rangle + \\dfrac{L}{2}(\\alpha^k) || d^k ||^2 $$\n",
    "$$ = f(x^k) + \\alpha^k \\left( - | \\langle d^k, \\nabla f(x^k) \\rangle | + \\dfrac{L}{2}\\alpha^k ||d^k||^2 \\right) $$\n",
    "$$ \\leq f(x^k) + \\alpha^k \\left( -C_1 || \\nabla f(x^k) ||^2 + \\dfrac{L c_2}{2}\\alpha^k ||\\nabla f(x^k)||^2 \\right) $$\n",
    "Taking out the common gradient squared:\n",
    "$$ =f(x^k) + \\alpha^k \\left( -C_1 + \\dfrac{LC_2}{2}\\alpha^k \\right)||\\nabla f(x^k) ||^2 $$\n",
    "\n",
    "Overall, $-C_1$ is negative and $LC_2/2$ is a positive number. We see that $\\alpha^k \\to 0$. However, $C_1$ remains constant. when $k  > \\bar{k}$ where $\\bar{k}$ is the point at which $\\leq f(x^k) + \\alpha^k \\left( \\dfrac{-C_1}{2} \\right) ||\\nabla f(x^k)||^2$. Basically the point at which the $C_2$ term goes to zero. There is no reason to believe the algorithm will descend. But by reducing the step size, when C2< C1, then we will have our descent. In words, since C1 is negative, we have just shown that the next iteration, $f(x^{k+1})$  will be less than the previous iteration $f(x^k)$ minus some constant value. Snazzy! Gurantees descent.\n",
    "\n",
    "$$ \\lim_{T \\to \\infty} f(x^T) - f(x^\\bar{k}) \\leq \\dfrac{C_1}{2}\\sum_{k = \\bar{k}} ^ T - \\alpha^k ||\\nabla f(x^k) ||^2 $$\n",
    "\n",
    "Is it possible that in the limit, $|| \\nabla f(x^k) || \\to \\epsilon > 0 $. In the limit is it strictly bounded away from zero? If this happens, then\n",
    "\n",
    "$$ -\\sum_{k = \\bar{k}} ^ \\infty \\alpha^k \\epsilon $$\n",
    "\n",
    "Is it possible that in the limit goes to zero? Hopefully! otherwise theres no minimum pretty sure.\n",
    "\n",
    "A simple regression proglem: Predict the price of the house by lining area:\n",
    "\n",
    "$$ \\left[\\begin{matrix} Living ARea & Price \\\\ 5719 & 567 \\\\ 3241 & 345\\\\ ... & ...  \\end{matrix}\\right] $$\n",
    "\n",
    "Basically we want to plot the slope and the intercept. \n",
    "\n",
    "$$ \\sum_{i=1}^N \\left(x_1 A_i + x_2 - P_i\\right)^2 $$\n",
    "\n",
    "This is our model. Construct a data matrix.\n",
    "\n",
    "$$ \\rvert \\rvert \\left[\\begin{matrix} A_1 & 1\\\\ A_2 & 1 \\\\ ... & ...\\\\ A_N & 1 \\end{matrix}\\right] \\left[\\begin{matrix} x_1 \\\\ x_2 \\end{matrix}\\right] - P \\rvert \\rvert^2 $$\n",
    "\n",
    "First try, we will multiple all the areas such that the price and the area are within a similar magnitude. Poor fitting occurs the first time.  If we scale differently again, we see a convergence. The convergence rate for the two were different. If we look at the eigenvalues of the first, the eigenvlaues of $A^\\prime A = $. The second scaling has eigenvalues of $A^\\prime A = 0.4, 18.448$.\n",
    "\n",
    "First, define an optimal solution as $\\epsilon$: $\\{x_\\epsilon := f(x^r) - f^* \\leq \\epsilon\\}$\n",
    "\n",
    "Convergence Rate: Measures the numer of iterations requared to gan an $\\epsilon$ optimal solution. Gives global behavior of the algorithm. A popular and important measure for evaluating algorithms in big data. What determines the convergence rate? If we want to decide how fast we approach the wall, per se.\n",
    "\n",
    "We look at a family of functions. Linear convergence means the error is shrunk by a constant factor at each iteration. \n",
    "\n",
    "$$ e^{r+1} = \\beta e^r $$\n",
    "$$ = \\beta^r e^0 $$\n",
    "$$ \\log(e^{r+1}) = r\\log(\\beta) + \\log(e^0) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Take example $f(x) = x^2 - 2x$. Check to see if it satisfies the bound\n",
    "\n",
    "$$ || f^\\prime (x) - f^\\prime (y)|| \\leq |x - y|$ \n",
    "\n",
    "plug values in\n",
    "\n",
    "$$ || x+2 - (y+2) \\leq |x - y| $$\n",
    "\n",
    "This is true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Last time we spoke about this problem:\n",
    "    \n",
    "$$f(x) = \\dfrac{2}{3}|x|^3 + \\dfrac{1}{2}x^2 $$\n",
    "    \n",
    "If we apply gradient descent, it will diverge. Let's double check:\n",
    "\n",
    "$$ |f(x) - f(y)| \\leq |x-y| L|x-y| \\forall (x,y) $$\n",
    "\n",
    "We mentioned the gradient is:\n",
    "\n",
    "$$ f^\\prime (x) = 2x^2 \\text{sgn}(x) + x, x\\neq 0 $$\n",
    "\n",
    "$$ 2x^2 \\text{sgn}(x) - 2y^2 \\text{sgn}(y) + x-y $$\n",
    "$$ \\leq |2x^2 \\text{sgn}(x) - 2y^2 \\text{sgn}(y)| + |x-y| $$\n",
    "\n",
    "We want a constant L; in this example, L=1. Can we bound this one? Assume x and y have the same sign.\n",
    "\n",
    "$$ \\leq |2x^2 - 2y^2| + |x-y| $$\n",
    "$$ \\leq 2|x+y||x-y| + |x-y| $$\n",
    "$$ = (2|x+y| + 1)|x-y| $$\n",
    "\n",
    "Because there is no bound on this, it is unbounded.\n",
    "\n",
    "Exercise: show the function $f(x) = |x|$, $x\\neq 0$.\n",
    "\n",
    "$$ f^\\prime (x) = \\text{sgn}(x) $$\n",
    "$$ |f^\\prime (x) - f^\\prime (y) = |\\text{sgn}(x) - \\text{sgn}(y)| $$\n",
    "$$ = 2 \\nleq L|x-y| $$\n",
    "\n",
    "\n",
    "Last time, we talked about the example of regression. They must converge in a different manner. They must behave w.r.t the convergence rate. The error, $e(x^r)= f(x^r) - f(x^*) \\geq 0$. The error will decrease in the following manner:\n",
    "\n",
    "$$ e^{r+1}(x) \\leq \\beta e(x^r) \\text{, } \\beta \\in (0,1) $$\n",
    "\n",
    "Recall $x^*$ is the global min. This type of behavior is linear convergence. It will reduce in a line. This is a strongly convex function. If we call $f(x)$ as convex, it will satisfy the condition iff:\n",
    "\n",
    "$$ f(y) \\geq f(x) + \\langle \\nabla f(x) ,y-x \\rangle $$\n",
    "\n",
    "If we draw a line at any given point x, the function is always larger than that line. Also possible to say that the curvature is greater than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We must define a function which is strongly convex. This is strongly convex if we can fit a quadtratic function below f(x) instead of a line.\n",
    "\n",
    "Ex: $f(y) = y^2$. This is a strongly convex function. A convex function will allow a very flat region. Therefore, intuitively, a fucntion is strongly convex if it is convex and has no \"flat\" regions. Another way of lookin at this is that tif the largest eigenvalue of a is greater than 0. Largest eigenvalue gives the largest change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Regression problem most likely to solve this. We are ready to characterize this as athe gradient descent.\n",
    "\n",
    "* step 1 $f(x^r) - f(x^{r+1}) \\geq \\dfrac{1}{2L}||\\nabla f(x^r) ||^2$\n",
    "* step 2 $f(x^{r+1} - f(x^*) \\leq \\dfrac{1}{2\\sigma}||\\nabla f(x^r)||^2$\n",
    "\n",
    "Let's look at the second one, how we got there. For the second one, we need to use strong convexity assumption. it says the following:\n",
    "\n",
    "$$ f(x^*) \\geq f(x^r) + \\langle \\nabla f(x^r), x^* - x^r \\rangle + \\dfrac{\\sigma}{2}||x^r - x^*||^2 $$\n",
    "\n",
    "What we do is minimize the r.h.s. over $x^*$, we obtain the optimal solution: $x^* = x^r - \\dfrac{1}{\\sigma}\\nabla f(x^r)$. Because we minimize the r.h.s, we can make another inequality:\n",
    "\n",
    "$$ f(x^*) \\geq f(x^*) - \\dfrac{1}{2\\sigma} ||\\nabla f(x^r) ||^2 $$\n",
    "\n",
    "Now what can we do. L is the largest curvature, $\\sigma$ is the smallest. From the first step, we have:\n",
    "\n",
    "$$ f(x^{r+1}) \\leq f(x^r) - \\dfrac{1}{2}L ||\\nabla f(x^r)||^2$$\n",
    "\n",
    "Remember, we want to characterize error. Error is $f(x^{r+1}) - f(x^*)$. By subtracting this from both sides we get:\n",
    "$$ f(x^{r+1}) - f(x^*) \\leq f(x^r) - \\dfrac{1}{2}L ||\\nabla f(x^r)||^2 - f(x^*)$$\n",
    "$$ e^{r+1} \\leq e^r - \\dfrac{1}{2L}||\\nabla f(x^r) ||^2 $$\n",
    "\n",
    "Then we bring in an organized version of step 2 from earlier. (both sides have been multiplied by $-2\\sigma$.: $-2\\sigma (f(x^{r+1} - f(x^*)) \\geq ||\\nabla f(x^r)||^2$. Plut this in:\n",
    "\n",
    "$$ \\leq e^r + \\dfrac{1}{2L} \\left( -2\\sigma(f(x^r) - f(x^*))\\right) $$\n",
    "\n",
    "we now see another error. Let's reorganize:\n",
    "\n",
    "$$ = e^r \\left( 1 - \\dfrac{2\\sigma}{2 L} \\right) $$\n",
    "$$ = e^r \\left( 1 - \\dfrac{\\sigma}{L} \\right) $$\n",
    "\n",
    "We define $\\beta = \\left( 1 - \\dfrac{\\sigma}{L} \\right)$. What we are getting at is that the error is decreasing this way. The $\\beta < 1$, and how small it is depends on the eigenvalue of the hessian matrix. If the condition number, $L/\\sigma$, of f is large, it is ill-condition and will be slow to converge. If it is small, it is well conditioned. \n",
    "\n",
    "\n",
    "We know that the simulation error should now obey this rule:\n",
    "\n",
    "$$ e(x^r) \\leq \\beta^r e(x^0) $$\n",
    "\n",
    "If e want to minimize $||Ax - b||^2$. The error wil lbe bounded by the objective value. We should know $e(x^0)$ because the difference betwen the iniial gues s and the mniimum is 0: $f(x^0) - f(x^*) \\leq f(x^*)$. Question: how many iterations do we need so that $e(x^r) \\leq 10^{-4}$.  In orer for the error to be less than our constraint, it is siffucient to have:\n",
    "\n",
    "$$ \\beta^r e(x^0) \\leq \\epsilon $$\n",
    "$$ r\\log(\\beta) + \\log(e(x^0)) \\leq \\log (\\epsilon) $$\n",
    "\n",
    "$$ r \\leq \\dfrac{-\\log(\\dfrac{e(x^0)}{\\epsilon}}{\\log \\beta} $$\n",
    "\n",
    "So if we want to achieve $\\epsilon = 10^{-4}$, $\\beta$ must be fairly large. It must also be strongly convex!!! :) There is some trade offs. In order to say something is stornger, we need to make more assumptions. As we can see, the condition number plays a role here. As we explain the the last lectures examples with price per room in feet. :\n",
    "\n",
    "$$ ||Ax - b||^2 $$\n",
    "\n",
    "Here, A is the squared feet, b is the price. \n",
    "* $L:= \\sigma \\text{ max} (A^TA)$\n",
    "* $\\sigma = \\sigma \\text{ min} (A^T A)$\n",
    "* Condition Number: $\\dfrac{L}{\\sigma}$\n",
    "\n",
    "Would be agood idea to do some preprocessing before attacking this problem again. We can normalize the A and center the bs.:\n",
    "\n",
    "$$ A^\\prime _k = \\dfrac{A_k - \\bar{A}_k}{\\sigma A_k} $$\n",
    "$$ b^\\prime = b- \\bar{b} $$\n",
    "\n",
    "Once we obtain $A^\\prime _k$ we can transform back using $A_k = \\sigma A_k A^\\prime _k + \\bar{A}_k$\n",
    "\n",
    "for the L and $\\sigma$, how do we know if it is fast or slow? Look at $\\beta$.\n",
    "\n",
    "$$ \\beta = \\left( 1 - \\dfrac{\\sigma}{L} \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First Order Methods:\n",
    "\n",
    "How will we stop the iterations? For gradient descent:\n",
    "\n",
    "$$ x^{r+1} - x^r = \\alpha^r \\nabla f(x^r) $$\n",
    "\n",
    "If the step size is constant, it may be fine; but what if it is very small? Not a good method to determine if we are done iterating. The normalized gradient instead should be measured. \n",
    "\n",
    "$$ \\dfrac{|| \\nabla f(x^r) || }{|| \\nabla f(x^0)||} \\leq \\epsilon $$\n",
    "\n",
    "the second thing to mention is this: we can always add in a spacer step between our iterations. Gradinet descent can be combined with many other algorithms. Last thing to mention: gradient error. At this time, all our analysis has assumed a perfect access to the gradient. In practice this rarely happens. Take a case:\n",
    "\n",
    "* case 1 $ ||e^k|| \\leq |x| \\cdot ||\\nabla f(x^r)|| $\n",
    "* case 2 $||e^k|| \\eq \\delta $. Causes oscillations in error (see explanation below)\n",
    "* case 3 $ ||e^k|| $ is a random variable\n",
    "\n",
    "For the first for the gradient, the descent, if there is no error:\n",
    "\n",
    "$$ d^r = \\nabla(x^r) + e^r $$\n",
    "$$ \\leq - \\langle \\nabla f(x^r) , d^r \\rangle $$\n",
    "$$ = ||\\nabla f(x^r) ||^2 - \\langle e^r, \\nabla f(x^r) \\rangle $$\n",
    "$$ \\leq ||\\nabla f(x^r) ||^2 - ||e^r|| ||\\nabla f(x^r))|| $$\n",
    "\n",
    "In this case, we hvae case 1 (i think)??? The error is always smaller than the gradient, therefore it is always descending.\n",
    "\n",
    "For case 2, we have:\n",
    "\n",
    "$$ \\geq ||\\nabla f(x^r)||(||\\nabla f(x^r) || - \\delta) $$\n",
    "\n",
    "In this case, the term in parenthesis is positive when the gradient is large. When the gradient is small, it is zero. This will cause oscillations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The rows of A represent features. The columns in the features. The length of x represents the features. We can solve this in terms of coordinate descent.\n",
    "\n",
    "$$ ||\\sum_{i=1}^k A_i x_i - b|| $$\n",
    "\n",
    "Now we are optimizing only the ith feature (ith column). Minimize the ith coordinate\n",
    "\n",
    "$$ \\min \\dfrac{1}{2}||A_ix_i + \\sum{j\\neq i}^n A_j x_j^r - b ||^2 $$\n",
    "\n",
    "Solution is very simple, no inversion needed\n",
    "\n",
    "$$ x_i^{r+1} = \\dfrac{1}{A_i^T A_i}A_i^T\\left( b - \\sum_{j \\neq 1}^n A_j x_j^r \\right) $$\n",
    "\n",
    "The Ai means the feature across all the data. if the Ai is zero, it means the feature doesn't appear in the data. Because we only look at one coordinate, we can write down a very simple update. Everything here becomes a regular regression for n. There is no step size selection. In many cases this works extremely well. The benefit of doing coordinate descent is we only do one coordinate at a time. Divide and conquer stradegy. Very popular method. Many algorithms, including the most popular method for solving problems. At least for now, the idea is simple. Another thing to mention is the scaled steepest descent.\n",
    "\n",
    "\n",
    "\n",
    "The scaled steepest descent.\n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha_r D^r \\nabla f(x^r) $$\n",
    "\n",
    "We are sort of scaling the gradients. By doing so, we can reduce the condition number (make it better). This will make it converge faster. For analyzing the interation above, we see that multiplying the gradient by D is the equivelant of solving a transform of the brplem. Consider a change of variable:\n",
    "\n",
    "$ x = Sy $ with $S = D^{1/2}$.\n",
    "\n",
    "If we minimize the function $h(y) = f(Sy)$ subject to $y \\in R^n$. Applying the steepest descent:\n",
    "\n",
    "$$ y^{r+1} = y^r - \\alpha_r \\nabla h(y^r) $$\n",
    "$$ Sy^{r+1} = Sy^r - \\alpha_r S\\nabla h(y^r) $$\n",
    "$$ x^{r+1} = x^r - \\alpha_r D\\nabla f(y^r) $$\n",
    "\n",
    "See what we did? By definition of h, we can use the chain rule:\n",
    "\n",
    "$$ \\nabla_y h(y^r) $$\n",
    "$$ = \\nabla_y f(xy)  $$\n",
    "$$ =S \\nabla_{sy} f(xy) $$\n",
    "$$ = S\\nabla _x f(x) $$\n",
    "\n",
    "What's the benefit? Take for example the scaled version:\n",
    "\n",
    "$$ S \\left[\\nabla ^2 f(x)\\right] S $$\n",
    "\n",
    "The hassian for this, for example, including the scaled S:\n",
    "$$ S \\left[\\begin{matrix} 1000 & ... & 0\\\\ 0 & 1 & ... \\\\ ... & 0 & 1 \\end{matrix}\\right] $$\n",
    "\n",
    "This is a poorly conditioned matrix (look at the ratio of the largest and smallest eigenvalues). We should pick S such that they cancel:\n",
    "\n",
    "$$ \\left[\\begin{matrix} 1/100 & ... & 0\\\\ 0 & 1 & ... \\\\ ... & 0 & 1 \\end{matrix}\\right]\\left[\\begin{matrix} 1000 & ... & 0\\\\ 0 & 1 & ... \\\\ ... & 0 & 1 \\end{matrix}\\right] \\left[\\begin{matrix} 1/100 & ... & 0\\\\ 0 & 1 & ... \\\\ ... & 0 & 1 \\end{matrix}\\right] $$\n",
    "\n",
    "This is something which we will try in the homework. If f(x) is a quadratic problem, the Hessian is a fixed matrix. For linear regression problem, the S will simply be the inverse of $A^TA$. IN practice we don't know the optimal thing; must do trial and error. This is sthe scaled steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Incremental Gradient Methods\n",
    "\n",
    "Previously, we have discussed the matrix A. We know a lot of things about the problem. We can decompose across the dimension, optimizing one feature at a time. What is the dimension? It is the data direction. Each time looking at ome data point and optimize. This makes a lot of sense! \n",
    "\n",
    "Another way of thinking about the least squared problem earlier:\n",
    "$$ ||Ax - b||^2 $$\n",
    "$$ = \\sum_{j=1}^N ||a_j x - b_j||^2 $$\n",
    "\n",
    "Where $a_j$ is the jth row of A. Each term, decompose the const function, as the sum of N termps. But the measuring of x. label number. Ignore all the sums. Look at the second term. The stradegy is clear. look at one term at a time (called incremental method). This is the motivation. This is a very useful algorithm. Each optimization is related to the variable x. In this case, $g_i$ is quadratic, but is not always this. \n",
    "\n",
    "Let $\\psi_0 = x^r$. Here, $\\psi$ looks at points, and is exactly like $x$; we just needed a new variable since we are using a gradient descent between two points $x^r$ and $x^{r+1}$.\n",
    "\n",
    "$$ \\psi_i = \\psi_{i-1} - \\alpha_r \\nabla g_i(\\psi_{i-1} $$\n",
    "\n",
    "update variable: $x^{r+1} = \\psi_m$\n",
    "\n",
    "This ammounts to:\n",
    "$$ x^{r+1} = x^r -\\alpha_r \\sum_{i=1}^m \\nabla f_i(\\psi_{i+1})$$\n",
    "\n",
    "Because the objective is a sum of m components, so is the function g. only difference is tha they areevaluated at different points:\n",
    "$$ x^{r+1} = x^r -\\alpha_r \\sum_{i=1}^m \\nabla f_i(x^r)$$\n",
    "\n",
    "\n",
    "The benefit of the first is that we only have to look at the data once. Look at incremental updates.\n",
    "\n",
    "Compute a single gradient. After we cycle through all variables, go back to the first one. Throughout the process, we are always updating the variables.  View the incremental method as the gradient step. from xr to xr+1, what we did was subracting m gradients. Each of them are evaluated at slightly different points:\n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha+r \\sum_{i=1}^m \\nabla g_i(x^r) + \\alpha_r \\sum_{i=1}^m\\left( \\nabla g_i(x^r) - \\nabla g_i(\\psi_{i-1}) \\right) $$\n",
    "\n",
    "The second sum amounts to an error term. Can be seen as a noisy gradient descent. The noise is the difference between all the gradeint descent. Not guranteed a descent every iteration. Need the step sizes to go to 0, since we now have an error term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## More on first order methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example of training a neural network. AS we have talked aobut in the previus lecture.\n",
    "\n",
    "INcremental gradient mehtod is some cost function squared.\n",
    "\n",
    "$$ f(x) = \\dfrac{1}{2}|| f(x)||^2 = \\dfrac{1}{2}\\sum_{i=1}^m ||g_i(x)||^2 $$\n",
    "\n",
    "Example: Neural network. Inputs $x_o$ and outputs $x_N$. Inbetween are N layers of neural network. We should look at the kth layer and the k+1 layer. At each node, we will have an output. The output will be connecting to all the nodes in the next layers. look at the fist node in the k+1 layer. The ouptut is simply a scalar number. The inputs are all the outputs from the previous layers ($x_k^1, x_k^2, .., x_k^m$)/ These outputs will be multiplying some weights to go into the node ($u_k^{11}, u_k^{21}, u_k^{31}, ...$). The output can be written as:\n",
    "\n",
    "$$ x_{k+1} = \\Phi \\left( \\sum_{s+1}^{n_k} x_k^s u_k^{s,1} \\right) $$\n",
    "\n",
    "The $\\Phi$ mimics the activiation function of a neuron: $\\Phi(y) = \\dfrac{1}{1+e^{-y}}$ called the sigmouid function. This will go to the next and the next. Once we define this function, we no longer try to optimize.\n",
    "\n",
    "These weights are the things that we train. suppose everyting is fixed (including the weights). We can put into the network and input at get out an output. We can express the output of the network as a funciton of the input and the optimization parameters (aka the weights):\n",
    "\n",
    "$$ x_n = h(x_k; u) $$\n",
    "\n",
    "What we want to do is have the features. $x_n$ will be the label, or also a vector. \n",
    "\n",
    "Suppose we have m samples of data, written as (y1,z1), (y2,z2) .... Recall that h(y,u) is the output of the network. We can do a least squares problem:\n",
    "\n",
    "$$ \\text{min} \\dfrac{1}{2}\\sum_{i=1}^m || z_1 - h(y_i,u) ||^2  $$\n",
    "\n",
    "If h is a linear function, we are back at the tradtional least squared function. This is obviously rarely the case in neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture - More on First Order Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for $f(x) = e^x$, we can look at the $f^\\prime^\\prime(*x) = e^x \\geq 0$. We cannot use that fact that the funciton is convex to show that the function has a global min. minimize the function:\n",
    "\n",
    "$$ f(x) = \\dfrac{1}{2} ||g(x)||^2 = \\dfrac{1}{2} \\sum_{i=1}^m ||g_i(x)||^2 $$\n",
    "\n",
    "If we use constant step size for an incremental method. Skipping example. We have two terms, c1 and c2. Optimal solution is the average of the two $(x* = c_1+c_2)/2$. Use diminishing step size to solve it.\n",
    "\n",
    "How many iterations to reach a region around the minimum. The simplist quadratic problem: when Q is an identity matrix. If we minimize to the best that we can. These directions will no longer be called orthogonal, but conjugate. \n",
    "\n",
    "Directions $d0,...,d^r$ are Q-conjugate if: $(d^i)^\\prime Q d^j = 0, \\forall i\\neq j$. Suppose we can find these directions. The method of conjugate direction is very simple then:\n",
    "\n",
    "$$ x^{r+1} = x^r + \\alpha_r d^r $$\n",
    "\n",
    "were $d^r$s are conjugate and $\\alpha$ is obtained by line minimization. The step size, note, is obtained by line minimization. \n",
    "\n",
    "Linearly independent. Assume $x^j \\neq 0$. Argument by contradition. Assume they aren't linearly dependent and set to zero through the definition of orthogonality. \n",
    "\n",
    "$$ \\sum_{i=1}^k x^r d^i=0 \\implies x^i =0 \\forall i $$\n",
    "\n",
    "Multiply bot sides by Q:\n",
    "\n",
    "$$ \\sum_{i=1}^k x^i Q d^i = 0 $$\n",
    "\n",
    "Multiply by dj\n",
    "\n",
    "$$ \\sum_{i=1}^k x^i d_jQ d^i = 0 $$\n",
    "\n",
    "By definition, since $d_iQd_j=0$ for all $i\\neq j$, then it is nonzero when\n",
    "\n",
    "$$ x^j (d^j)^T Q d^j = 0 $$\n",
    "\n",
    "Since $x^j\\neq 0$, the $ (d^j)^T Q d^j = 0 $ portion cannot =0. Thus, not true by contradition.\n",
    "\n",
    "\n",
    "\n",
    "We must use htis GramSchmidt procedure to determine directions. Given a set of linearly independent vecotrs $\\varepsilon^0,...,\\varepsilon^r$, we can construct a set of Q-conjugate directions $d0, ..., d^r$. So long as we know the c coefficinet, we can compute the next d. Question is how we compute the next c.\n",
    "\n",
    "$$ c_{i+1}_j $$\n",
    "\n",
    "Why? need to know correct form of c. We need to choose the c such that $d^{i+1}$ is Q-conjugate to $d^0,...,d^i$:\n",
    "\n",
    "$$ (d^{i+1})^\\prime Q d^j = (\\varepsilon^{i+1})^\\prime Qd^j + \\left( \\sum_{m=0}^i c_{i+1,m}d^m \\right) Qd^j =0 $$\n",
    "\n",
    "Te\n",
    "\n",
    "$$ \\varepsilon^{r+1} Q d^j + C_{(i+1),j} d^j Q d^j = 0 $$\n",
    "\n",
    "We want to find the c. Solve for it.\n",
    "\n",
    "$$ C_{(i+1),j} = -\\dfrac{\\varepsilon^{r+1}Qd^j}{d^j Q d^j} $$\n",
    "\n",
    "Compute them this way for $\\forall 0 \\leq j \\leq i$. We do this step by step and we will find all of the conjugate directions. As i increases, we need to do more and more and more. Special case when $c$ is the gradient, we only need to solve once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Conjugate direction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Quesion is, what is line minimization: at r+1 iteration, $x^{r+1} = x^r \\alpha^{r+1}d^r$. We find the r+1, by minimizing the objective function along the direction d:. My next direction will be orthogonal to my past direction.\n",
    "\n",
    "$$ \\alpha^{r+1} = \\text{min }f(x^r + \\alpha d^r)  $$\n",
    "\n",
    "The optimality condition for this problem is taking the gradient with alpha. $\\left( d^r \\right)^T \\nabla f(x^{r+1}) = 0$. This is the first order optimality condition. We wan show that for any i < r, we have:\n",
    "\n",
    "$$ \\nabla f(x^{r+1})^\\prime d^i = (Qx^{r+1} + b)^\\prime d^i $$\n",
    "\n",
    "Says we are writing down the gradient. Recall we are minimizing the problem: $\\dfrac{1}{2} x^T Q x + b^\\prime x$.\n",
    "\n",
    "$$ x^1 = x^0 + \\alpha^1 d^1 $$\n",
    "$$ x^2 = x^1 + \\alpha^2 d^r $$\n",
    "$$ ...  $$\n",
    "$$ x^r = x^{r-1} + \\alpha^r d^{r-1} $$\n",
    "\n",
    "So, by plugign all this into a single sum,\n",
    "\n",
    "$$ x^{r+1} = x^0 + \\sum_{i=1}^r \\alpha^{i+1}d^i $$\n",
    "\n",
    "Back to the proof above:\n",
    "\n",
    "$$ \\nabla f(x^{r+1})^\\prime d^i = (Qx^{r+1} + b)^\\prime d^i $$\n",
    "$$ = \\left( x^{i+1} + \\sum_{j=i+1}^r \\alpha_j d^j \\right)^\\prime Qd^i + b^\\prime d^i $$\n",
    "\n",
    "... (other portions not shown)\n",
    "\n",
    "The second step is to show convergence. We can actually show that. $\\beta$ is a step size.\n",
    "\n",
    "$$ \\dfrac{\\partial  f(x^0 + \\beta_o d^0 + .. + \\beta_rd^r}{\\partial \\beta_i}\\rvert \\beta=\\alpha = \\nabla f(x^{r+1} ^\\prime d^i $$\n",
    "\n",
    "No matter what direction we look at (ll of the different directions $d^r$, the gradient = 0 meaning that we are already the best. Therefore we have:\n",
    "\n",
    "$$ x^{r+1} = \\text{arg min}(x\\in M^r) f(x) $$\n",
    "\n",
    "Where $M^r = \\{ x|x=x^0 + v, v\\in \\text{span}(d^0, d^1, ... d^r) \\}$\n",
    "\n",
    "This argument implies that after r iteraion, I am done. we look at it subspace by subspace. This analysis is only valid for quadratic functions. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For the gram-Schmidt procedure.\n",
    "\n",
    "We start somewhere, compute the gradient. $\\varepsilon^r = -g^r = -\\nabla f(x^r)$. Find the directon $d^r$.\n",
    "\n",
    "$$ d^r = -g^r + \\sum_{j=0}^{r-1} \\dfrac{gr^\\prime Q d^j}{d^j^\\prime Qd^j}d^j $$\n",
    "\n",
    "This can simplify the computation by a lot.  By direciton formula can be simlified, the directions of the conjugate gradient method generatedy by $d^0=-g^0$ and\n",
    "\n",
    "$$ d^r = -g^r + \\beta_r d^{r-1} $$\n",
    "\n",
    "Where $\\beta_r$ is the sum from the precious thing.\n",
    "\n",
    "$$ \\beta_r = \\dfrac{(g^r)^\\prime g^r}{(g^{r-1})^\\prime g^{r-1}} $$\n",
    "\n",
    "Here, $g^i = \\nabla f(x^i)$.  The iteration will look like this:\n",
    "\n",
    "$$ x^0 \\rightarrow \\nabla f(x^0) \\right arrow d^0 \\right arrow x^1 \\rightarrow \\nabla f(x^1) \\rightarrow d^1 \\rightarrow x^2 ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Constraint optimization.\n",
    "\n",
    "Briefly mention that we talk about Newton's method. For Newton's method, we use the Hessian, invert it, and equivelantly, for each iteration, new can be written as a minimization problem:\n",
    "\n",
    "$$ x^{r+1} = \\text{argmin } f(x^r) + \\nabla f(x^r) (x-x^r) + \\sum^{\\alpha_r} (x-x^r)^T \\nabla^2 f(x^r)(x-x^r) $$\n",
    "\n",
    "What we are doing is exactly minimizing the quadratic approximation using a Taylor expansion of the objective function. This is Newton's method. We discuss Newton's method may be unstable, and discuss about quasi Newton's method, which is a smart way of designing the direction matrix $D^r$. We use current at past direction gradients to calculate this. For quadratic problems, after N updates, $D^n = Q^{-1}$. Recall that Q is the Hassain. \n",
    "\n",
    "This is called the BFGS method (Brayden-Fletcher-Goldfort-Shamo), after people who invented version of this. After theorem for this. This is simply the gradient descent, but now more complicated now that we have to update the scalings $D^r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Trust Region method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Briefly mentioned this at the beginning of the course. For all hte moethds we've seen so far, we always select the direction, them compute the step size. Do exact minimization. HOwever, there is one method which is slightly different. Related to Newton. We decide how big of a step to walk. \n",
    "\n",
    "The objective function is precisely the Netwon Method, but now we constrain it to a region around the current iteration, limited to size $s_r$. \n",
    "\n",
    "$$ || x - x^r || \\leq s_r $$\n",
    "\n",
    "We need to assign a number to $s_r$. We call the $q(x)$ function the model function. The constant $s_r$ is the stepsize ofthe rth iteration. The objective function may be nonconvex if $\\nabla^2 f(x^r) \\ssuc 0$. Byt it can be efficinetly solved regardless of convexity! :)\n",
    "\n",
    "Let's see an example of how the algorithm will work. This is the problem:\n",
    "\n",
    "$$ f(x) = 10(x_2 - x_1^2)^2 + (1-x_1)^2 $$\n",
    "\n",
    "We begin with an initial solution $x_0 = (0,1)$. Compute the Hessian next:\n",
    "$$ \\nabla^2 f(x_o) = \\left[\\begin{matrix} -38 & 0 \\\\ 0 & 20 \\end{matrix}\\right] $$\n",
    "\n",
    "Construct a model, which is a quadratic function. We can plot the contour. Remember, we want to minimize q(x):\n",
    "\n",
    "$$ q(x) = \\nabla f(x^r)^\\prime (x-x^r) + \\dfrac{1}{2}(x-x^r)^\\prime \\nabla^2 f(x^r)(x-x^r) $$\n",
    "\n",
    "The trust region is a ball around the current point. If we start at a current point. The direction we are going to go will be a minimizer. If we make it larger, the region defined by radius $s_r$ really drives the direction we move. We need to have a rule to decide how good the solution is. Combining this rule, we will get the algorithm. Update rules:\n",
    "\n",
    "* Stepsize rule:  assume there is an optimal step size $s_r$ (somehow we know this, an assumption). Because x(s_r) is the optimial solution, a trivial solution is x=0. This means that the optimal solution of q(x) is nonpositive. \n",
    "* Three cases:\n",
    "    * $s_{r/2}$ Whether dew iteration achieves sufficient descent: $f(x^r) - f(x(s_r)) < -\\dfrac{1}{2}q(x(s_r))$, the solution is no good, half the step size\n",
    "    * $s_r$ Second condition. If the difference is greater than $-\\dfrac{1}{2}q(x(s_r))$, then we're happy, keep current size.\n",
    "    * min $\\{2s_r, \\bar{s}\\}$Last one, if we are doing super good, increase the region. if $f(x^r) - f(x(s_r)) > 2q(x(s_r))$, and $||x(s_r) - x^r|| = s_r$\n",
    "* Next iterate updated as $x^{r+1} = $\n",
    "Repeating! We start with an unconstrained problem. However, to implement the trust region method, we must solve a subregion with a constraint. Suppose we can solve this problem and use the update rule.\n",
    "\n",
    "Can we solve this Q is Hassian, b is the quadratic. $f(x) = \\dfrac{1}{2}x^\\prime Qx + b^\\prime x$ subject to $||x||^2 \\leq 1$.\n",
    "\n",
    "If b=0, we are now constrained to the problem of minimizing $x^T Qx$ constrained to $||x||^2 \\leq 1$. Make it simpler, $x=1$. The optmimal solution will be an eigenvalue. If we didn't see it, we are seeking a vector which multiplies the Hessian matrix, the optimal solution is the optimal eigenvalue. There is an algorithm called the Rayleigh-Riez Theorem. \n",
    "\n",
    "$$ \\dfrac{x^T Ax}{||x||^2} $$\n",
    "$$ \\lambda_{max} = \\text{max}_x \\left[ \\dfrac{x^T A x}{||x||^2} \\right] $$\n",
    "\n",
    "The sum believe that the value is closely related to this. Problem can solve it in closed form. No proof is given.\n",
    "\n",
    "End of discussion of constrained problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Unconstrained Problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " We are starting with an unconstrained problem. \n",
    " \n",
    " Do we minimize f(x) when we minimize q(x)?\n",
    " \n",
    " Everything we see so far, the optimization variables. If we look at the contour, the optimimum solution. We have constraints like this. We want to make sure we satisfy certain constraints. We want to look for optimal solutions within this region. Build a model and separate these two classes while having the largest margin possible. The problem can be formulated into a constrained opmimal problem where each data point is a constraing.  $b_i \\in \\{-1,1\\}$ states which group they are in (only 2 groups, 2 choices). If a new data point comes in. We want to minimize$x^tx$ constrainged to $b_i(a_i^T x) \\geq 1, \\forall i$. The objective function is straightforward. Without constraints, solution is obviously zero. However, cannot do that with constraints.\n",
    " \n",
    " Specifically, we have a funtion $f(x)$ subject to $x \\in X$. Now we have another qualification: X is a convex set. This means that we allow the following types of constraints:\n",
    " \n",
    "1. $g(x) \\leq 0$  where g(x) is a convex function.\n",
    "2. $h(x) = 0$ where h(x) is an affine function: $Cx + d =0 $ \n",
    "Basically, we can draw a line between two bounds and have everything on the line be within the set. For example, a circle contains as convex set, but the letter \"c\" would not. Let's say $g(x) == ||x-1||^2$ constrained to $g(x) \\leq 1$. Constraints like that are okay and will be considered in this lecture. Another figure to see why it makes sense. $g(x) < -$\n",
    "\n",
    "Let's look at the set $||x-1||^2 = 1$. This is not a convex set, because it does not include anything within the circle.\n",
    "\n",
    "Optmiality conditions. if $x^*$ is a local minimum of $f$ or $X$, then (necessary condition):\n",
    "\n",
    "$$ \\langle \\nabla f(x^*), x-x^* \\rangle \\geq 0, \\forall x\\in X $$\n",
    "\n",
    "The worst we can do on the boundary. Basically the degree is less than 90 degree for all degrees. We must check forall x  in X. Minimization. This is the optimality condition. We will use this to design conditions. ONe additional thing. If f is also a convex function, this condition is also sufficient for $x^*$ to minimize $f$ over all $X$. ONe comment. No constraint, we must satisfy.\n",
    "\n",
    "there are two parts for its proof. ARgue by contradition. Suppose $\\nabla f(x^*)^\\prime (x-x^*) < 0$, we can conldue by contradition that $x^*$ would no longer be a local optimal for x. Second portion. Look at the term: $f(x) \\geq f(x^*) + \\nabla f(x^*)^\\prime(x-x^*)$. For every x, if \n",
    "$\\nabla f(x^*)^\\prime(x-x^*) \\geq 0$ holds for all x, it is a global min.\n",
    "\n",
    "Very important concept coming out of discussion. Projection. What is a projection? Thinking about constrained problem. What is difficulty.We are generating the sequence. We still perform the gradient descent until we see that we are going outside. we MUST have a way to come pack inside. If we go outside, we must project back into $X$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture Optimization over a Convex Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Test office hours, Thurs 11-12, open book open note, calculator. Covers Hw 1-3 and 6a.\n",
    "\n",
    "Projection over a convex set. Let's minimize \n",
    "\n",
    "This intuitively makes sense. Suppose we have a point $z \\notin R^n$. How can we find the projection of $z$ onto the feasible region? Look for a solution where $x^* = \\text{proj}[z]$; the angle between $z-x^*$ and $x - x^*$ is $\\geq 90^o \\forall x \\in X$ or $(z-x^*)(x-x^*) \\leq 0$. \n",
    "\n",
    "$$ \\text{proj}[z] = \\dfrac{z}{||z||_2} $$\n",
    "\n",
    "If we have a bondary, we now know how to come back if our point lies outside the set of acceptable values. Later we will design algorithms which are gradient based. We must ensure feasability. Let's write down formally what the projection is.\n",
    "\n",
    "Let's try to minimize $\\text{min}_x ||z-x||^2$ for $x \\in X$. the optimal solution to P is defined as $x^* = \\text{proj}_x(z)$.\n",
    "\n",
    "$$ \\langle x^* - z, x-x^* \\rangle \\geq 0 \\forall x \\ in X $$\n",
    "\n",
    "This actually lends a perfect amount of intuition. Let's multiply by -1.\n",
    "\n",
    "$$ \\langle z - x^*, x-x^* \\rangle \\geq 0 \\forall x \\ in X $$\n",
    "\n",
    "This says that for any x within this space, this is the inner product. Less than 0, means angle between the two vectors will be greater than $90^o$. If it's a plane, you can imagine how this will be precisely defined. Let's say, for example, we have a curved set of points. This is optimality condition. use this condition to verify. Let's say we have quadrant I, a square which defines our valid region.\n",
    "\n",
    "$$ \\text{min} \\dfrac{1}{2}||z-x||^2, s.t. x \\geq 0 $$\n",
    "$$ \\langle x^* - z, x-x^* \\rangle \\geq 0, \\forall x \\geq 0 $$\n",
    "\n",
    "We need to find the projection $x^*$.\n",
    "\n",
    "$$ \\sum_{i=1}^N \\langle x^* - z_i, x_i - x^*_i \\rangle \\geq 0 \\forall x_i \\geq 0 $$\n",
    "$$ \\langle x_i^* -z_i, x_i - x_i^* \\rangle \\geq 0, \\forall x_i \\geq 0 $$\n",
    "\n",
    "* case 1: $z_i \\geq 0$. Here, $x_i^* = z_i$\n",
    "* case 2: $z_i \\leq 0$. Here, $x_i^* = 0$. This will reduce the problem to $\\langle -z_i, x_i \\rangle \\geq 0 \\forall x_i \\geq 0$\n",
    "* case 3: \n",
    "\n",
    "Therefore, the projection of z onto X can be defined as:\n",
    "\n",
    "$$ \\text{proj}[z]_X = \\begin{cases}z_i,& z_i \\geq 0 \\\\ 0, & z_i < 0 \\end{cases} $$\n",
    "\n",
    "Example 2: A circle area\n",
    "\n",
    "$$ X := \\{x \\rvert ||x||_2 \\leq 1 \\} $$\n",
    "$$ \\text{min} \\dfrac{1}{2}||z-x||^2 $$\n",
    "\n",
    "How can we find $x^*$?\n",
    "\n",
    "* case 1: $z: ||z||_2 \\leq 1$ $x^* = z$\n",
    "* case 2 $a: ||z||_2 > 1$. Here, $x^* = \\dfrac{z}{||z||_2}$\n",
    "\n",
    "We want to now show that the inner product of the following is less than 0:\n",
    "\n",
    "$$ \\langle \\dfrac{z}{||z||_2} - z, x - \\dfrac{z}{||z||} \\rangle $$\n",
    "$$ = \\left( 1 - ||z||_2 \\right)\\langle \\dfrac{z}{||z||_2}, x - \\dfrac{z}{||z||} \\rangle $$\n",
    "\n",
    "We've taken out a common term. Because we have assumed $||z||_2<0$, the common term is positive. The inner product of two vectors is less than the individual signs. We hope we have a better understanding of a projection operator.\n",
    "\n",
    "If we specialize the convex set a little, then we can go into detail further. How to do the projection closed form. For the simple sets, projection can be done in closed form.\n",
    "\n",
    "The first bullet: if we consider a projection ofthe x and y, then after projecting it back to the feasible set, the distance between the projected points will always be smaller than the distance from the original points. Stated mathematically:\n",
    "\n",
    "$$ || \\text{proj}[x] - \\text{proj}[y] || \\leq || x-y||, \\forall x,y \\in R^n $$\n",
    "\n",
    "Exercies: assume $X$ is convex. a cector $x^* \\in X$ is a stationary point of (minimizing $f(x) subject to $x\\in X$ Iff $x^*$ satisfies the following fixed point equation:\n",
    "\n",
    "$$ x^* = \\text{proj}\\left[ x^* - \\alpha \\nabla f(x^*) \\right] $$\n",
    "\n",
    "Look, for instance, at when the gradient is 0; of course, $x^* = x^*$. My gradient descent algorithm tries very hard ot make the condition satisfied: $x^{r+1} = x^r - \\alpha \\nabla f(x^r)$. This is where the gradient descent algorithm comes from intuitively. With constraint, we can no longer directly do this. This is because we don't know if we are still in the set. This is why we add the projeciton: $ x^* = \\text{proj}\\left[ x^* - \\alpha \\nabla f(x^*) \\right] $. What this is saying is the following. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture Optimization over a Convex Set Pt. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To begin: test is next monday. Office hours Thurs 11-12. Regular TA office hour. Also, the video is online.  Solution for HW2 is online. Solution for HW3 online shortly. Some constraints within the optimization problems. The projection will be easy to implement. Today, we focus on the methods, rather than the conditions.\n",
    "\n",
    "Feasible Directions\n",
    "\n",
    "The idea: we have constraints (no longer go wherever). We must determine if your direction is feasible. Must determine how. Suppose we can find the direction. The algorithm will be as follows:\n",
    "\n",
    "$$ x^{r+1} = x^r + \\alpha_r d^r $$\n",
    "\n",
    "We go to the direciton with a certain step size. The direction must be feasible and descent (further optimizes the problem). Previously, we only need descent. thus: $\\nabla f(x^r)^\\prime d^r < 0$ such that $x^{r+1} \\in X$. If we cannot find a direction which satisfies this, \n",
    "\n",
    "$$ \\langle \\nabla f(x^r), y-x^r \\rangle $$\n",
    "\n",
    "Direction is a vector, difference of some point to the min. Basically, the $d^r$ is the portion $y-x^r$. We no longer have a direction when this is satisfied, which means that $x^r$ is a stationary point. Otherwise, the direction is feasible. We will go over a couple ways to find the direction. The property we need is the property relatedness. Later, we wil see that we won't do much proof. \n",
    "\n",
    "As an alternative definition, we replace $d^r$ with $\\bar{x}^r - x^r$:\n",
    "\n",
    "$$ x^{r+1} = x^r + \\alpha_r \\left( \\bar{x}^r - x^r \\right) $$\n",
    "\n",
    "Convergence Analysis\n",
    "\n",
    "The limit point will be a stationary solution. \n",
    "\n",
    "Details: need to still find a direction.\n",
    "\n",
    "In fact, we can find direction through the Gradient Projection Method.\n",
    "\n",
    "$$ x^{r+1} = x^r + \\alpha_r \\left( \\bar{x}^r - x^r \\right) $$\n",
    "$$ \\bar{x}^r  = \\text{proj}_x \\left[ x^r - s_r \\nabla f(x^r) \\right] $$\n",
    "\n",
    "This projection algorithm gurantees it will be a feasible direction. $s_r$ is a positive scalar. \n",
    "\n",
    "Suppose we are at $x^r$. Walk a direction. First we compute $x^r - s_r \\nabla f(x^r)$. Basically, we can use $s_r$ to shorten the vector such that it is within the set. Here, we have two step sizes to choose: $s_r$ and $\\alpha_r$. Two ways to do this. Keep $s_r$ constant. Let's say we have another choice. We can fix $\\alpha_r=1$ and let $s_r$ be adaptive. When this happens, the $x^r$s cancel out in the two equations above, leaving us with:\n",
    "\n",
    "$$ x^{r+1} = \\text{proj}_x \\left[ x^r - s_r \\nabla f(x^r) \\right] $$\n",
    "\n",
    "Either case, we can tie it back to something we already know. Another interpretation for this, to better internalize what is going on. If we still remember, let's compare this with the gradient step.\n",
    "\n",
    "$$ x^{r+1} = x^r - s_r \\nabla f(x^r) $$\n",
    "\n",
    "We interprit this step as minimizing some quadratic approximation of the problem. AKA: $x^{r+1} = \\text{argmin }f(x^r) + \\langle \\nabla f(x^r), x-x^r \\rangle + \\dfrac{1}{2s_r}||x-x^r||^2 $. We approsimated the function as a quadratic function, and trapped the original function below the quadratic funciton. We then minimize the quadratic function, and use the minimum of the quadratic function as the next step. For the gradient projection, it is exactly the same thing, except that we add a constraint, $x \\in X$. Let's derive this:\n",
    "\n",
    "Projection operator. Optimization problem.\n",
    "\n",
    "$$ \\text{min }||x-\\left( x^r - s_r\\nabla f(x^r) \\right)||^2 $$\n",
    "\n",
    "Recall again that $\\left( x^r - s_r\\nabla f(x^r) \\right) = z$, where z is a point outside the set (imagine a point above a plain). \n",
    "\n",
    "$$ = ||\\left( x - x^r \\right) + s_r \\nabla f(x^r)||^2 $$\n",
    "$$ = ||x-x^r||^2 + ||s_r \\nabla f(x^r)||^2 + 2\\langle x-x^r, s_r \\nabla f(x^r) \\rangle $$\n",
    "\n",
    "If we multiply the constant in from the the min term, the problem will not change. Thus we take out a constant. Thus:\n",
    "\n",
    "$$ 2s_r\\left( \\dfrac{1}{2s_r}||x-x^r||^2 + \\dfrac{1}{2s_r}||s_r \\nabla f(x^r)|| + \\langle x-x^r, \\nabla f(x^r) \\rangle \\right) $$\n",
    "\n",
    "The middle term goes to 1. Sweet! do the remaining terms matter? Nope! They don't chaing the optimal solution. If we minimize $f(x)$, we also simultaneously minimize $1/2\\cdot f(x) + 2 + 3 ...$. These don't change the location of the minimum. Therefore, in minimizing, we are allowed to add and multiply constants. The claim is that there is a close relationship between:\n",
    "\n",
    "$$ x^{r+1} = \\text{proj}_x \\left[ x^r - s_r \\nabla f(x^r) \\right] $$\n",
    "$$ x^{r+1} = x^r - s_r \\nabla f(x^r) $$\n",
    "\n",
    "The only differentiation is that there is a constraint! Mention one more thing: Let me remind you that when we talk about gradient descent, we can use a constant step size, the $s_r = \\dfrac{2}{L}$ where $L$ is the curvature. Anything smaller than this will work. Why is this true? If we want to find a bound which is on top of the graph, this is the curvature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example: Nonnegative LS. \n",
    "\n",
    "We want to linearly combine these features. We want to do a linear combination. This says that we will not subtract the features; only add. It doesn't matter to subtract. In those cases, we must have nonnegativity. \n",
    "\n",
    "Gradient projection. Recall we can choose anything up to $\\dfrac{2}{L}$, so we choose $\\dfrac{1}{L}$.\n",
    "\n",
    "$$ x^{r+1} = \\text{proj}_{x\\geq 0} \\left[ x^r - \\dfrac{1}{L}\\left( A^T\\left( Ax^r - b \\right) \\right) \\right] $$\n",
    "\n",
    "If it is greater than 0, great! Smaller than 0, set it to 0. Remember, go to the closes point on zero. Check it 1 by 1, compare with the gradient descent algorithm, we need to go over the coordinates again and check if it is less than 0. Not much more difficult than gradient descent. In order to do gradient projection, add this. Because this set is easy; we know how to do projection onto it. If we have a more difficult constraint set (such as a ball or nonnegativity), we don't know. Knowing how to do projections are important. If we implement these algorithms, must be efficient.  Typically, this is an optimization problem; if it is expensive, not very good to begin with. \n",
    "\n",
    "How to do projection: remember! we did it last time. \n",
    "\n",
    "We need to show that the direction $\\bar{x}^r - x^r$ is gradient related. WE must probe that $\\{||\\bar{x}^r 0 x^r||\\}_{r\\in k}$ bounded and $\\lim_{r \\to \\infty} \\nabla f(x^r)^\\prime (\\bar{x}^r - x^r) < 0$\n",
    "\n",
    " The first relation holds because $\\{||\\bar{x}^r 0 x^r||\\}_{r\\in k}$ converges to $\\text{proj}_x\\left[ x-s\\nabla f(x) \\right] - x$. Using the properties of projection:\n",
    " \n",
    " $$ \\left( x^r - s\\nabla f(x^r) - \\bar{x}^r \\right)^\\prime \\left( x - \\bar{x}^r \\right) \\leq 0,\\forall x \\in X $$\n",
    " \n",
    " So, to begin with, we need to use hte property of projection:\n",
    " \n",
    " $$ \\bar{x}^r = \\text{proj}\\left( x^r - s\\nabla f(x^r) \\right) $$\n",
    " $$ \\bar{x}^r - x^r = \\text{proj} (x^r - s\\nabla f(x^r)) - x6r $$\n",
    " \n",
    " We specialize the projection of z. Projection of z will be \n",
    " \n",
    " $$ \\langle z - \\text{proj}(z), x - \\text{proj}(z) \\rangle \\leq 0$$\n",
    " $$ \\langle x^r - s\\nabla f(x^r) - \\bar{x}^r, x - \\bar{x}^r \\rangle $$\n",
    " \n",
    " The previous inequality becomes \n",
    " \n",
    " $$ \\langle x^r - s\\nabla f(x^r) - \\bar{x}^r, x^r - \\bar{x}^r \\rangle \\leq 0 $$\n",
    " \n",
    "$$ -\\langle s \\nabla f(x^r), x^r - \\bar{x}^r \\rangle + \\langle x^r - \\bar{x}^r, x^r - \\bar{x}^r \\rangle \\leq 0 $$\n",
    "\n",
    "$$ \\implies s \\langle \\nabla f(x^r) , \\bar{x}^r - x^r \\rangle \\leq - ||x^r - \\bar{x}^r||^2 $$\n",
    "\n",
    "The $\\bar{x}$ is the direction I picked. This is a good direction. This gives us the gradient relatedness. if we pass the limit to both sides:\n",
    "\n",
    "$$ \\lim_{r\\to \\infty} \\nabla f(x^r) (\\bar{x}^r - x^r) \\leq - \\dfrac{1}{s} ||\\tilde{x} - \\text{proj}_x \\left[ \\tilde{x} - s\\nabla f(x\\tilde{x}) \\right] ||^2 < 0 $$\n",
    "\n",
    "Assumption: nonstationary solution. This means the latter portion is not zero. A stationary point if and oly if $x^*$ satisfies the following equation:\n",
    "\n",
    "$$ x^* = \\text{proj}\\left[ x^r - \\alpha \\nabla f(x^*) \\right] $$\n",
    "\n",
    "Of limit point is not a stationary solution. Not only feasible, but also gradient related. If we multiply this diretion with the gradient, it must be negative. These are all the pieces we need to show convergence. If we want to choose a cosntant $\\alpha_r,s_r=s$, we must have $0 \\leq s \\leq \\dfrac{2}{L}$\n",
    "\n",
    "Must be careful on how we choose diretions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Convergence Rate Analysis\n",
    "\n",
    "Not going into details. We care about the condition number. The key thing is that, if we look at a quadratic problem, the difference between the next iteration and the optimal solution, it will be  strictly less than: $(1-\\dfrac{2}{k+1}||x^r - x^*||$. Basically, the distance is decreased by a constant fraction each time, determined by a scalar $k$. This difference will be very close to 0. Dependent on the objective function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Conditional Gradient (Frank Wolfe) Method\n",
    "\n",
    "Here, there is another very interesting way to find a feasible point. We start at the current iteration $x^r$. This method says, first compute gradient. Then, go opposite gradient directoin as far as possible within possible set, $\\bar{x}^r$. The new iteration will be something in the middle between these two points. If we do gradient projection, we are now always going to extremes, and the selected point is now always along the save vector as your original direction. \n",
    "\n",
    "$$ \\bar{x}^r = \\text{argmin }_{x \\in X} \\nabla f(x^r)^\\prime (x-x^r) $$\n",
    "\n",
    "To find a feasible solution, in the previous section, to implement the iteration, we have a quadratic problem. Here, we have a linear problem (since it stays along the gradient vector). Basically ,we simply want to minimize:\n",
    "\n",
    "$$ \\text{min } \\langle \\nabla f(x^r), x \\rangle \\text{subject to } x\\in X  $$\n",
    "\n",
    "Very important that we have a constraint. Basically, the solution is the intersection with the boundary. We gurangeed that we go as far as we can.\n",
    "\n",
    "We will also be able to show that this is a good direction.  Because it is linear, we can compute all of the gradients, then we pick the one which gives you the smallest magnitude (number). \n",
    "\n",
    "$$ \\text{min } \\langle \\nabla f(x^r),x \\rangle \\text{s.t. } x_i \\geq 0, \\forall i $$\n",
    "\n",
    "One coordinate will be active, it will activley satisfy this constraint. The above minimization problem gets very easy to solve, which makes it very attractive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture - Constrained Optimization: Lagrangian Multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Test Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let $1 \\in R^M$ be an all one vector. The simplex\n",
    "\n",
    "Proof:\n",
    "\n",
    "$$ || v + \\delta 1 - x||^2 = ||v-x||^2 + \\delta^2 ||1||^2 + 2\\delta(v-x)^T1 $$\n",
    "$$ = ||v-x||^2 + \\delta n + 2\\delta(v^T1-1) $$\n",
    "\n",
    "We look at the problem rearranged. The constant terms which are taken out will not affect the minimization problem. Hence,\n",
    "\n",
    "$$ P_\\Delta(v+\\delta 1) = \\text{argmin }_{x\\in \\Delta} ||v + \\delta 1 - x||^2 $$\n",
    "$$ = \\text{argmin }_{x\\in \\Delta} ||v-x||^2 = P_{\\Delta} (v) $$\n",
    "\n",
    "TThe x is simplex, and satisfies the constraint stated in the problem. Here, we look at the two problems; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have aproblem with fwo functions in the objectve, and run incremental gradient. The first iteration, we only optimize the firs function. The second step, we minimize the second function. Based on the new updates, make new step. \n",
    "\n",
    "First limit:\n",
    "\n",
    "$$ x^\\infty (1) = \\dfrac{(1-\\alpha) c_1 + c_2}{z-\\alpha} $$\n",
    "$$ x^\\infty (2) = \\dfrac{(1-\\alpha) c_2 + c_1}{z-\\alpha} $$\n",
    "\n",
    "Setting these equal to each other, we see that $c_1 = c_2$. This means that this is not really 2 different. If the function is not, they are different and the function will oscillate.\n",
    "\n",
    "Let's plug in the two equations into one.\n",
    "\n",
    "$$ x^{r+1}(1) = (1-\\alpha)^2 x^2(1) + (1-\\alpha)\\alpha c_1 + \\alpha c_2 $$\n",
    "$$ x^r(1) = (1-\\alpha)^2 x^{r-1}(1) + (1-\\alpha) \\alpha c_1 + \\alpha c_2 $$\n",
    "$$ = (1-\\alpha)^4 x^{r-1} + \\left( (1-\\alpha)^2 + 1 \\right)\\left( (1-\\alpha)^2 c_1 + \\alpha c_2 \\right) $$\n",
    "\n",
    "In order to see the limiting behavior, we must look at the pattern. We see that the exponentials increase. Let's look at the first repeating sequence:\n",
    "\n",
    "$$ \\left[ ... + (1-\\alpha)^4 + (1-\\alpha)^2 + 1 \\right] = \\dfrac{1}{1-(1-\\alpha)^2} = \\dfrac{1}{(z-\\alpha)\\alpha} $$\n",
    "\n",
    "In the limit, the original problem now becomes:\n",
    "\n",
    "$$ x^\\infty(1) = 0 + \\dfrac{1}{(z-\\alpha)\\alpha}\\left( (1-\\alpha)\\alpha c_1 + \\alpha c_2 \\right) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "More about constrained optimization. \n",
    "\n",
    "We spoke about problems with constraints and algorithms to deal with them.\n",
    "\n",
    "$$ x^{r+1} = x^r +\\alpha_r d^r $$\n",
    "\n",
    "We need to make sure the new directions are feasible, meaning if we look to that direction, there is a step size small enoughto bring me to a feasible point. Thei nner product between point and direction is less than 0 $(\\nabla f(x^r)^\\prime d^r < 0)$\n",
    "\n",
    "The gradient projection method: perform a projection. Taking the usual gradient step. Two choices of step size; fix $s$ or fix $\\alpha$. \n",
    "\n",
    "$$ \\bar{x}^r = \\text{proj}_x\\left[ x^r - s_r \\nabla f(x^r) \\right] $$\n",
    "\n",
    "Can alsodo the Armijo rule along the projection arc.\n",
    "\n",
    "Example, didn't go through the derivation or application of network flow. To motivate the type of constraint, it is a simplex constraint. For this type of constraint, this problem is very easy. \n",
    "\n",
    "$P_\\omega$: a given set of paths. $x_p4: the traffic assicned to path p.\n",
    "\n",
    "$$ \\sum_{p=1}^P x_p = r_\\omega, x_p \\geq 0, \\forall p $$\n",
    "\n",
    "For each path $p$, there will be some sort of cost $c_p$. Minimize $\\sum c_p(x_p)$, a.k.k our cost function for using path $p$. \n",
    "\n",
    "Can we derive a gradient based approach to minimize using a few lines.  Minimize the optimization variable $x_p$. $r$ indicates the previous iteration (weird notation I know, but we need to solve a sub-minimization problem to know the next step in solving the global minimization problem). \n",
    "\n",
    "$$ \\sum_{p=1}^P C_p(x_p) (x_p - x_p^r) $$\n",
    "\n",
    "We look at the constrained problem. \n",
    "\n",
    "Take the partial derivative. Remember, $\\langle \\nabla f(x), x - x^* \\rangle \\geq 0$.\n",
    "\n",
    "$$ \\sum_{p=p}^P \\langle \\dfrac{\\partial f(x)}{\\partial x_p}, x_p - x_p^* \\rangle \\geq 0 $$\n",
    "$$ \\implies \\sum \\langle C_p^\\prime (x_p),x_p - x_p^* \\rangle \\geq 0 $$\n",
    "\n",
    "Let's fix a coodrinate $l$. Let's suppose $x_p^* > 0$. Let's plug this back into 0. Let the corresponding $x_l = 0$. Pick another $j \\neq l$ such that $x_j = x_j^* + x_l^*$. Another $x_m = x_m^*, m\\neq i,j$\n",
    "\n",
    "We are trying to satisfy the inequality in the original equation. We set everything the same as the optimal solution. We can choose any feasible solution. We can set all the rest of the coordinates the same, except the two coordinats. We do this because the solution we constructed, x, is feasible because everything else is the same as the optimal solution except the l and j comoponents.\n",
    "\n",
    "$$ x = \\left[\\begin{matrix} x_1 \\\\ ...\\\\x_p  \\end{matrix}\\right] = \\left[\\begin{matrix} x_1^* \\\\ ...\\\\ 0  \\\\ ... \\\\ x_j^* + x_l^*\\\\...\\\\x_p^* \\end{matrix}\\right] $$\n",
    "\n",
    "The 0 occurs at the $l$th position and the $j$th position is $x_j^* + x_l^*$. Two terms remain! \n",
    "\n",
    "$$ C_l^\\prime (x_l)\\cdot (0-x_l^*) + c_j^\\prime (x_j)\\cdot (x_l^*) \\geq 0 $$\n",
    "\n",
    "Let's merge the two stars.\n",
    "\n",
    "$$ \\left[ -c_l^\\prime + c_j^\\prime(x_j) \\right] x_l^* \\geq 0 $$\n",
    "\n",
    "$$ x_l^* > 0 [\\implies c_j^\\prime(x_j) \\geq c_l^\\prime(x_l) $$\n",
    "\n",
    "Optimal solution for this problem when compoment is greater than 0, then the correxponding gradient must be smaller than all the rest of the $j$s. In word, this says that the lth component is sricktly greater than 0, then the corresponding derivating must be small compared to the rest of the $j$s. What does this say?\n",
    "\n",
    "Find $i^*$ such that the derivative w.r.t. my current iteration is less:\n",
    "\n",
    "$$ c_j^\\prime(x_l) \\leq C_j^\\prime(x_j),. \\forall j\\neq i $$\n",
    "\n",
    "The new $i^*$ must also solve the constraint. Thus:\n",
    "\n",
    "$$ x^* = \\left[\\begin{matrix} 0 \\\\ 0\\\\...\\\\ r\\\\ 0 \\\\ ...\\\\0 \\end{matrix}\\right] $$\n",
    "\n",
    "Where $r$ is at the $i^*$th component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's say we wantto mimimize\n",
    "\n",
    "$$ f(x) = x_1 + x_2 $$\n",
    "\n",
    "Suc hthat both the conditions are true:\n",
    "\n",
    "$$ (x_1-1)^2 + x_2^2 -1 = 0 \\text{   }(\\lambda_1) $$\n",
    "$$ (x_1-2)^2 + x_2^2 -4 = 0 \\text{   }(\\lambda_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Inequality constraints.\n",
    "\n",
    "Equality problems. How to write down the legrange multiplier, how to write down the stationary solutions, how to talk about the cases where you are guaranteed to have multipliers. The constraints are linearly independent to each other. STill possible to have multipliers. This case, we are guranteed to have a unique solution. guarantee. Sensitivity.\n",
    "\n",
    "Legrangian function:\n",
    "\n",
    "$$ L(x,\\lambda) = f(x) = \\sum^m_{i=1} \\lambda_i h_i(x) $$\n",
    "\n",
    "Are we able to solve htis problem such that the local minimizer for the sum is also the local min for the original problem. The legrangian theorem states this is possible, given a couple of condition. Original problem: minimize $f(x)$ such that $h_i(x) = 0$ for all $i = 1...m$. If $x^*$ is a local min, it implies that: 1. must be feasible and 2. $\\nabla f(x^*) + \\sum_{i=1}^m \\lambda_i^* \\nabla h_i(x^*) = 0 $ if $\\nabla h)i(x^*)$ is independent. The equation is simply the derivative w.r.t x of the Lagrangian function ($\\nabla_x L(x^*,\\lambda^*) = 0$. Treating this as a surrogate (unconstrained problem). We are applying the knowledge of unconstrained problems. Remember, thisis no enough. We must make sure $x^*$ is feasible. How? Look at $L$. Can we take derivative w.r.t. $\\lambda$, then set to 0. Works? Yep! :) $\\nabla _\\lambda L(x^*,\\lambda^*) = 0$.\n",
    "\n",
    "Now we only have one object. Take derivative w.r.t. x and $\\lambda$, set to zero. Can use these conditions to test if these points are optimal. Consider this problem. The Legrangian can be written as\n",
    "\n",
    "$$ \\dfrac{1}{2}\\left( x_1^2 + x_2^2 + x_3^2 \\right) + \\lambda \\left( x_1 + x_2 + x_3 - 3 \\right) $$.\n",
    "\n",
    "$$ \\nabla_x L = \\left[\\begin{matrix} x_1 + \\lambda \\\\ x_2 + \\lambda \\\\ x_3 + \\lambda \\end{matrix}\\right] = 0 $$\n",
    "$$ \\nabla_\\lambda = x_1 + x_2 + x_3 = 3 $$\n",
    "\n",
    "Take derivative w.r.t. x. Use ofthe first order condition. Also need to check the second order sufficient condition. Must take the Hessian at the solution, then multiple left and right y.:\n",
    "\n",
    "$$ y^\\prime \\nabla ^2_{xx} L(x^*,\\lambda^*) y > 0, \\forall y\\neq - $$\n",
    "\n",
    "Let's double check. minimize the following problem:\n",
    "\n",
    "$$ -\\left( x_1x_2 + x_2x_3 + x_1x_3 \\right) $$\n",
    "\n",
    "subject to $x_1 + x_2 + x_3 = 3$ (this is the $h(x)$). The gradient is\n",
    "\n",
    "$$ \\nabla h = \\left[\\begin{matrix} 1 \\\\ 1\\\\ 1 \\end{matrix}\\right] $$\n",
    "\n",
    "We have that $x_1^* = x_2^* = x_3^* = 1$ and $\\lambda^* = 2$ to satisfy the 1st order condition. Second order: We can compute the Hessian. \n",
    "\n",
    "$$ \\nabla^2_{xx} L(x^*,\\lambda^*) = \\left[\\begin{matrix} 0 & -1 & -1 \\\\ -1 & 0 & -1 \\\\ -1 & -1 & 0 \\end{matrix}\\right] $$\n",
    "\n",
    "\n",
    "We have that for all $y\\neq 0$, with $\\nabla \\langle  \\rangleh(x^*) ^\\prime y = 0$ or $y_1 + y_2 + y_3 = 0$.\n",
    "\n",
    "The inner produce with the hessian is stirctly larger than 0. The gradient constraint. \n",
    "\n",
    "For all the ys which satisfy this, let's check our condition.\n",
    "\n",
    "$$ y^\\prime \\nabla ^2_{xx} L(x^*,\\lambda^*)y = -y_1(y_2 + y_3) - y_2(y_1 + y_3) - y_3(y_1+y_2) = y_1^2 + y_2^2 + y_3^2 > 0 $$\n",
    "\n",
    "We only look at directions which we can move to. \n",
    "\n",
    "Compare with what we did forthe previous case. Because we have explicit quation for constraint, we can write down Lagrangian, constraingt. Optimality condition we have developed, we are pretty general. min $f(x), x \\in X$. We must test for every poin in the feasible region to satisfy the condition: $\\langle \\nabla f(x), x - x^* \\rangle \\geq 0, \\forall x \\in X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sensitivity\n",
    "\n",
    "Ther eis something called Sensitivity. Briefly mention results. Single linear constraint:\n",
    "\n",
    "$$ a^Tx = 0 $$\n",
    "\n",
    "If we perturb the 0 to $\\epsilon$, the question is, can we say somehting about objective function without resolving problem Given problem, spend one day to solve it. The problem is a little bit, change one number slightly. Can we give a rough estimate for the new solution. The result here is that\n",
    "\n",
    "$$  $$\n",
    "\n",
    "The idea is thee following: change that we made ($\\Delta \\epsilon$), the change inthe objective will be related to the peturbation.\n",
    "\n",
    "$$ \\delta f \\propto \\Delta \\epsilon \\lambda $$\n",
    "\n",
    "If the $\\lambda$ is large, the peturbations will be large as well. The first order \n",
    "\n",
    "Give an example:\n",
    "\n",
    "Famly of problems parameterized by the $u$. We will change later.  Local min $x^*$. Lagrange multiplier $\\lambda^*$ satisfies the sufficiency condition. $x(u)$ meets the optimal solution and $\\lambda(u)$. Around the sphere, these solution also exist. the gradient ofthe optimal solution. $p(u)$ is defined as the optimal objective value for the minimization problem: $\\min_{h(x) = 0} f(x)$. If we change the $u$ a little bit, how is $p(u)$ changing?. Let'slook at a problem:\n",
    "\n",
    "$$ p(u) = f(x(u)) $$\n",
    "\n",
    "The goal is to minimize $f(x)$:\n",
    "\n",
    "$$ f(x) = \\dfrac{1}{2}\\left( x_1^2 - x_2^2 \\right) - x_2 $$\n",
    "\n",
    "subject to $h(x) = x_2 = 0$. The problem is given by\n",
    "\n",
    "$$ p(u) = \\min_{h(x) = u} f(x) = -\\dfrac{1}{2}u^2 - u $$\n",
    "\n",
    "and $\\lambda^* = -\\nabla p(0) = 1$. Ths is consistent with the sensitivity eheorem. \n",
    "\n",
    "Original constraint is $h(x) = x_2 = 0$. Let's peturb this a little bit and set it t o$h(x) = x_2 = u$. Thus, the perturped problem, \n",
    "\n",
    "$$ \\min \\dfrac{1}{2} \\left( x_1^2 - u^2 \\right) - u $$\n",
    "\n",
    "Solutions will be:\n",
    "\n",
    "$$ x(u) = \\begin{cases} x_1^* = \\pm u \\\\ x_2^* = u \\end{cases} $$\n",
    "\n",
    "Let's see what happens when we change it a little bit. The theorem says that if we change the r.h.s. a little bit, the objective function will change the following way. If we take derivative\n",
    "\n",
    "Take the legrangian of the objecting function\n",
    "\n",
    "$$ L = \\dfrac{1}{2}\\left( x_1 ^ 2 - x_2^2 \\right) - x_2 + \\lambda x_2 $$\n",
    "$$ \\dfrac{\\partial  L }{\\partial  x_1} = x_1 = 0  $$\n",
    "$$ \\dfrac{\\partial L}{\\partial x_2} = -x_2 + \\lambda - 1 = 0 $$\n",
    "\n",
    "This implies that:\n",
    "\n",
    "$$ \\left[\\begin{matrix} x_1 =^* = 0 \\\\ x_2^* = 0 \\\\ \\lambda^* = 1 \\end{matrix}\\right] $$\n",
    "\n",
    "Let's now perturb it with u. The Legrangian now becomes:\n",
    "\n",
    "$$  L = \\dfrac{1}{2}\\left( x_1 ^ 2 - x_2^2 \\right) - x_2 + \\lambda( x_2  -u) $$\n",
    "\n",
    "The soltions will now be:\n",
    "\n",
    "$$ \\left[\\begin{matrix} x_1^* = 0 \\\\ x_2^* = u\\\\ \\lambda = u+1 \\end{matrix}\\right] $$\n",
    "\n",
    "We can now see how it will change when peturbing the solution. The derivative is u. We now see that when $\\lambda^* = -\\nabla p(0) = 1$. When we change the optimal objective value, the change is proportional tothe Legrangian multiplier. $u$ is parameterizing the point. At this point, we've already solved the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Inequality Constrained Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now have anothe set of parameters; one for equality, another for inequality.\n",
    "\n",
    "$$ L(x, \\lambda, u) = f(x) + \\lambda h(x) + u g(x) $$\n",
    "\n",
    "Here for inequality, we are assume an equality less than 1. If an equality which was larger than 1, in this case, we can stick a negative sign in front of $g(x)$.\n",
    "\n",
    "Let's make a problem. min $f(x)$ such that $h(x) = 0$. Let's rearrange this problem in terms of inequalities. The problem is the same as:\n",
    "\n",
    "$$ \\min f(x) $$\n",
    "\n",
    "such that \n",
    "\n",
    "$$ \\begin{cases} h(x) \\leq 0, \\lambda_1 \\\\ -h(x) \\leq 0, \\lambda_2 \\end{cases}$\n",
    "\n",
    "$$ L = f(x) + \\lambda_1 h(x) + (-h(x) \\lambda_2) $$\n",
    "\n",
    "$$ = f(x) + (\\lambda_1 - \\lambda_2) h(x) $$\n",
    "\n",
    "This will be exactly the multiplier for this thing. The correspondence. Why doe we not hae a sign? difference between $\\lambda_1$ and $\\lambda_2$ could be positive or negative. Equality constraint is always equivelant to two inequality constraints. \n",
    "\n",
    "Let's look at which contstraints are active. If we aregiven a point $x^*$, the inequalities will be satsified. The inequality constraints could be equl. For those which are equl, keep them. Consider between active and inactive constraints:\n",
    "\n",
    "$$ A(x) = \\{ j | g_i (x) = 0\\} $$\n",
    "\n",
    "If they are not in this set, they are not acie. If $x^*$ is a local min, the active inequality constraints at $x^*$ can be treated as equations. The inactive constratints at $x^*$ do no matter. Why the $u$ is positive? We have discussed the sensitivity analysis. For this problem. If we have an inequality constraint, if we perturb the constraint a bit, $g(x) \\geq 0$, the objective function will also be perturbed. More places to test, the objective could be further minimized. This is only true for inequalities. This is reflected in the sign on the multipliers. \n",
    "\n",
    "Suppose we have three materials, $x_1, x_2, x_3$. We want to minimize the production cost for these such that:\n",
    "\n",
    "$$ c_1 x_1 + c_2 x_2 + c_3x_3 $$\n",
    "\n",
    "Gets maximized. For this type of problem, the \n",
    "\n",
    "$$ \\begin{cases} a_{11} x_1 + a_{21} x_2 + a_{31} x_3  \\leq b_1 \\\\ a_{21}x_1 + a_{22}x_2 + a_{32}x_3 \\leq b_2 \\\\ x_1, x_2, x_3 \\geq 0 \\end{cases} $$\n",
    "\n",
    "Here, if we want to produce x_1, we want raw material. First line says tha the toal raw materials is limited. The second is limited in a different way as well. We are maximizing the profit. Each of the constraints. The inequality constraints. The corresponding $\\lambda$ wil lbe different as well, and will characterize how precioius each material is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the inequality constrained probe, we have a condition: Karash Kuhn Tucker Condition. The forst order condition is similar. $x^*$ is a local min and a regular point. There exist Lagrange multiplier vecotrs such that:\n",
    "\n",
    "$$ \\nabla _x L(x^*, \\lambda^*, u^*) = 0 $$\n",
    "\n",
    "An optimal solution $x^*$, the corresponding  variable is exactly 0. Two \n",
    "\n",
    "$$ x^* \\begin{cases} h_i (x^*) = 0, \\lambda_i^8 \\\\ g_i(x^*) = 0, i \\in A \\\\ g_j(x) < 0,  j \\notin A \\end{cases} $$\n",
    "\n",
    "The way to say this, we make the corresponding $u_j=0$ for constraints which are not applicable and do not enter into the equation. \n",
    "\n",
    "$$ L = f(x^*) + \\sum \\lambda_i h_i(x^*) + \\sum _{j \\in A}u_ig_i(x^*) $$\n",
    "\n",
    "For the constraints which are not active, it will cancel out the constraint. Take derivative:\n",
    "\n",
    "$$ \\nabla_x L(x^*, u^*, \\lambda^*) $$\n",
    "\n",
    "Set equal to 0:\n",
    "\n",
    "$$ 0 = \\nabla f(x) + \\sum \\lambda_i^* \\nabla h_i (x^*) + \\sum_{i \\in A} u_i^* \\nabla g_i(x^*) $$\n",
    "\n",
    "For sufficient conditions, to find a pair of $x^*,\\lambda^*$ that satisfies all this. Lter will have examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 11-6-2017 - Constrained Optimization: Lagrangian Multipliers, Optimality Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Why the Legrangian multiplier must be positive in this case, and the KKT condition must make sense.\n",
    "\n",
    "We want to minimize :\n",
    "\n",
    "$$ \\min x_1 + x_2 \\text{ such that } x_1^2 + x_2^2 - 2 \\leq 0 $$\n",
    "\n",
    "We have an optimal solution, which looks somehting like a ball. The feasible region lies within the circle. The objective function is linear, so the gradient. Let's look at the Lagrangian Multiplier. First of all, inequality contstraints are always less than or equal to. Lets look at the point $(-1,-1)$. What's the gradient of the constraint? The gradient of the constraint, $g(x)$ is:\n",
    "\n",
    "$$ \\nabla g(x) = \\left[\\begin{matrix} 2x_1 \\\\ 2x_2 \\end{matrix}\\right] $$\n",
    "\n",
    "Evaluating at $(-1,-1)$:\n",
    "$$ \\nabla g\\left( \\left[\\begin{matrix} -1\\\\-1 \\end{matrix}\\right] \\right) = \\left[\\begin{matrix} -2 \\\\ -2 \\end{matrix}\\right] $$\n",
    "\n",
    "The gradient points towards the center. Let's look at the gradient of the objective function:\n",
    "\n",
    "$$ \\nabla f(x) = \\left[\\begin{matrix} 1 \\\\ 1 \\end{matrix}\\right] $$\n",
    "\n",
    "They are eaual and opposite. We can write this:\n",
    "\n",
    "$$ f(x^*) = -\\dfrac{1}{2}\\nabla g(x^*) $$\n",
    "$$ \\nabla f(x^*) + \\dfrac{1}{2}g(x^*) = 0 $$\n",
    "\n",
    "Here, $\\dfrac{1}{2} = \\lambda^*$. We may wonder what is the inequality constraint. For equality constraint, (chaning the constraint to be a circle rather than the area within the circle), we can write a constraint such that the gradient points in either direction. For example, the equation could be: $x_1^2 + x_2^2 - 2 = 0$ or $-x_1^2 - x_2^2 + 2 = 0$. Both describe the same constraint, but with the gradient pointing in opposite directions. Therefore, we must use inequalities to define constraints.\n",
    "\n",
    "Suppose $x$  is not locally optimal. We should be able to move somewhere to reduce the objective.  There should be some move $s$ such that $x+s$ reduces the objective. I have to have: $ 0 \\geq g(x+2)$ in order for the point to be feasible. HOw about reducing the objectives.\n",
    "\n",
    "1. Case 1: if $x$ lies strictly inside feasible region, then $s = -2\\nabla f(x)$.\n",
    "2. Case 2: What if $x$ lies on the border? We still have a descent direction: $\\nabla f^T(x) s < 0$ is true. Let's approximate $g(x+s)$ by first order.\n",
    "\n",
    "$$ 0 \\geq g(x+s) \\approx g(x) + \\nabla g(x) s  $$\n",
    "\n",
    "Becauese $g(x)$ is on the border, then:\n",
    "\n",
    "$$ 0 \\geq \\nabla g^T(x) s $$\n",
    "\n",
    "This is condition. Suppose x is a nonoptimal solution, then when choosing to move. We must use the intersection between $\\nabla f(x) s <0$ and $0 \\geq \\nabla g^T(x) s$. There will no longer exist a solution when these two gradients are opposite of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img  src=\"IMG_0594.JPG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img  src=\"IMG_0595.JPG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ L = f(x) = \\lambda g(x) $$\n",
    "\n",
    "$$ \\nabla L (x,\\lambda) = \\nabla f(x) + \\lambda f(x) $$\n",
    "\n",
    "$$ (x^*, \\lambda^*) \\nabla _x L(x^*, \\lambda^*) = 0 $$\n",
    "\n",
    "Add the constraints:\n",
    "* $g(x^*) \\leq 0$\n",
    "* $\\lambda \\geq 0$\n",
    "\n",
    "Say we have the two constraints, and decide to use two constraints:\n",
    "\n",
    "$$ L = f(x) + \\lambda_1 g_1(x) + \\lambda_2 g_2(x) $$\n",
    "$$ \\nabla L(x,\\lambda) = \\nabla f(x) + \\nabla g_1(x) \\lambda_1 + \\nabla g_2(x) \\lambda_2 $$\n",
    "\n",
    "For constraints which do not matter or not intersect, the associated $\\lambda = 0$.\n",
    "\n",
    "If we have a constraint $g_i(x^*) < 0 \\implies \\lambda_i ^* = 0$. If on the other hand the inequality constraint is $g_j(x^*) = 0 \\implies \\lambda_j^* \\geq 0$. Let's combine this into an easier equation. Because one or the other is 0, the multiplier must satisfy the condition:\n",
    "\n",
    "$$ g_i(x^*) \\lambda_i^* = 0, \\text{  } \\lambda_i^* \\geq 0 $$\n",
    "\n",
    "This summarizes the optmiality condition for constraints. At a local optimum $x^*$, we should satisfy the Lagangian condition:\n",
    "\n",
    "$$ f(x^*) + \\nabla g_1(x^*) \\lambda_1^* + \\nabla g_2(x^*) \\lambda_2^* = 0 $$\n",
    "\n",
    "\n",
    "\n",
    "Now, let's come back to the KKT condition. \n",
    "\n",
    "$x^*$ is a local min and a regular point. In the inequality constraing:\n",
    "\n",
    "$$ \\nabla_x L(x^*, \\lambda^*, u^*) = 0, u_j^* \\geq 0, j = 1... r, u_j^* = -, j\\notin A(x^*) $$\n",
    "\n",
    "We also have the second order condition, but we choose not to focus on that.\n",
    "\n",
    "Let's take an example. Consider the problem:\n",
    "\n",
    "$$ \\min_{x \\in R^2} f)x_ = x_1 + x_2 $$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "* $g_1(x) = x_2 - x_1^3 \\leq 0$\n",
    "* $g_2(x) = -x_2 \\leq 0$\n",
    "\n",
    "Where is $g_1$ pointing to?  we need to combine these two gradients uc htaht it is opposite the true gradient. Impossible for this problem:\n",
    "\n",
    "$$ \\nabla g_1 = \\left[\\begin{matrix} -3x_1^2 \\\\ 1 \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right] $$\n",
    "$$ \\nabla g_2 = \\left[\\begin{matrix} 0 \\\\ -1 \\end{matrix}\\right] $$\n",
    "\n",
    "Combining the gradients:\n",
    "\n",
    "$$ \\nabla f(x^*) = -\\lambda_1^*\\nabla g_1(x^*) - \\lambda_2\\nabla g_2(x^*) $$\n",
    "\n",
    "Not possible\n",
    "\n",
    "Let's see how to apply the KKT conditions. If there is no constraint, there is no problem. Now with constrinats, how to deal with the problem? Deal with two examples with complicated constraints.\n",
    "\n",
    "$$ \\min_x \\left( x - 5 \\right)^2 $$\n",
    "\n",
    "Subject to $0 \\leq x \\leq 3$. In order to write KKT, we want to change the constraints to fit into the problem. As is, they must be split:\n",
    "\n",
    "* $x \\leq 3$ for $g_1$\n",
    "* $-x \\leq 0$ for $g_2$\n",
    "\n",
    "$$ L = (x-5)^2 + \\lambda_1 (x-3) + \\lambda_2(-x) $$\n",
    "\n",
    "1. $ \\nabla L = 2(x-5) + \\lambda_1 - \\lambda_2 = 0 $\n",
    "2. $\\lambda_{1,2} \\geq 0$,\n",
    "3. $x \\leq 3$, $- x \\leq 0$,\n",
    "4. and finally $\\lambda_1(x-3) = 0$ and $\\lambda_2(-x)$. The last two are the conditions we derived in the previous section.\n",
    "\n",
    "How should we start? Let's try $x=0$, and $\\lambda_1=0$. These satisfy condition 4. plug into 1.:\n",
    "\n",
    "$$ -10 + 0 -\\lambda_2 = 0 \\implies \\lambda_2 = -10 $$\n",
    "\n",
    "Doesn't work! This implies that $x \\neq 0 \\implies \\lambda_2 = 0$. Take this info and plut into 1.\n",
    "\n",
    "$$ 2(x-5) + \\lambda_1 = 0 $$\n",
    "\n",
    "Because we know that $x \\leq 3$, we can imply that $\\lambda_1 > 0$. This must be true for the above equation to hold. This implies that $x = 3$. From 4, this implies that \n",
    "\n",
    "$$ \\left[\\begin{matrix} x \\\\ \\lambda_1 \\\\ \\lambda_2 \\end{matrix}\\right] = \\left[\\begin{matrix} 3 \\\\ 4 \\\\ 0 \\end{matrix}\\right] $$\n",
    "\n",
    "\n",
    "Let's look at another problem:\n",
    "\n",
    "$$ \\min_x ||x - z||^2 $$\n",
    "\n",
    "Such that $||x||^2 \\leq 1$. \n",
    "\n",
    "$$ L = ||x-z||^2 + \\lambda(||x||^2-1) $$\n",
    "$$ \\nabla L = (x-z) + \\lambda 2x $$\n",
    "Using the KKT:\n",
    "\n",
    "$$ \\nabla L = (x-z) + \\lambda 2x = 0 $$\n",
    "\n",
    "$$ (2\\lambda+1) x = z $$\n",
    "\n",
    "$$ x = \\dfrac{z}{2\\lambda + 1} $$\n",
    "\n",
    "The first KKT condition is that the gradient must equal 0. The second condition is: $\\lambda(||x||^2 - 1) = 0$, and the last ones are: $\\lambda \\geq 0$, $||x||^2 \\leq 1$. The optimization based on the cases:\n",
    "\n",
    "1. Case 1: $\\lambda = 0$, $\\impies x = z$, $\\implies ||z||^2 \\leq 1$\n",
    "2. Case 2: $\\lambda \\neq 0$, $\\implies ||x||^2 = 1$, $\\implies 2\\lambda+1 = ||z|| \\implies \\lambda = \\lambda = \\dfrac{||z|| - 1}{2} > ||z|| \\geq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Proof of the KKT condition\n",
    "\n",
    "What we showed is that we penalize $h$ and add a $k/2$. When $x$ approaches an optmial solution, the solution will be unconstrained local min. If the problem is regular,\n",
    "\n",
    "$$ f(x) + \\dfrac{k}{2}||h(x)||^2 + \\dfrac{k}{2}\\sum_{j=1}^r(g_j^+(x))^2 + \\dfrac{1}{2}||x-x^*||^2 $$\n",
    "\n",
    "1. $\\lambda_i^* = \\lim_{k \\to \\infty} kh_i(x^k), i=1,...,m$\n",
    "2. $u_j^* = \\lim_{k\\to \\infty} k g_j^+(x^k), j = 1,...,r$\n",
    "\n",
    "If we follow the sama analysis. If we elook at the equality constraint, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 11-8-2017 - Constrained Optimization: Duality Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Hw4 Clarification. The "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 11-13-2017 - Constrained Optimization: Duality Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using Duality, we will derive an efficient algorithm. DIvided into 2 parts: theory and then examples. Let's look at Duality Theorem: We look at optimization problem which is convex. The Dual function: first write the Lagrantian function. \n",
    "\n",
    "$$ q(\\mu) = \\inf_{x\\inX}\\{ f(x) + \\sum^r_{j=1}\\mu_j(x^\\prime_j x - b_j) \\} $$\n",
    "\n",
    "This means that $x$ is now related to $\\mu$ somehow. We whten want to maximize $q(\\mu)$. Additional requirement: the dual function cannot go to $-\\infty$. The two problems are basically equivelant. If the primal problem has an optimal solution, then the dual problem also has one, and they share the solution. The fist result says that, if we have  primal problem, it is equivelant to solving the dual problem. The second part is, once you have $\\mu^*$, how to recover $x^*$. We simply plut it back into the Lagrangian.\n",
    "\n",
    "1. For any $\\mu$, we will have: $q(\\mu) \\leq f(x^*)$. This is true for any feasible $\\mu$.\n",
    "2. For a particular $\\mu$, $q(\\mu^*) = f(x^*)$.\n",
    "\n",
    "Because we want to have this inequality. For any feasible $x$,\n",
    "\n",
    "$$ q(\\mu) := \\inf_{x \\in X} f(x) + \\sum^r_{i=1} \\mu_i(a_i^T x - b_i) $$\n",
    "\n",
    "We want to show that the r.h.s. is smaller than the left.\n",
    "\n",
    "$$ q(\\mu) \\leq \\inf_{x \\in X} f(x) $$\n",
    "\n",
    "We know that $\\mu_i \\geq 0 \\forall i$.\n",
    "\\theta\n",
    "From the complimentary, $\\mu_j^*(a^\\prime_j x^* - b_j) = 0,...\\forall j$. There is anothe rprimal problem. Another aspect of the problem which can be solved. We have seen the example from the lineary programming case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example:\n",
    "\n",
    "$$ \\min \\dfrac{1}{2}x^\\prime Qx + c^\\prime x $$\n",
    "\n",
    "subject to $Ax \\leq b$, where $Q$ is a given $nxn$ positive definite matrix. Let's get the Legrangina:\n",
    "\n",
    "$$ L = \\dfrac{1}{2}x^T Q x + c^T x + \\mu^T(Ax - b) $$\n",
    "Do we have a solution for this? Take the gradient!\n",
    "\n",
    "$$ Q x  + c A^T \\mu = 0 $$\n",
    "$$  \\implies x = -Q^{-1} (c + A^T \\mu) $$\n",
    "\n",
    "If we now plug in the original Legrangian:\n",
    "\n",
    "$$ q(\\mu) = \\dfrac{1}{2}(c + A^T \\mu) Q^{-1} Q Q^{-1}(c + A^T \\mu) $$\n",
    "\n",
    "$$ -c^T Q^{-1}(c + A^T \\mu) - \\mu^T A Q^{-1} (c + A^T \\mu) $$\n",
    "$$ =- \\mu^T b $$\n",
    "\n",
    "The optimization variable is $\\mu$. Combing this all, we get:\n",
    "\n",
    "$$ q(\\mu) = -\\dfrac{1}{2}\\mu^T A Q^{-1} A^T \\mu - \\mu^T(b + AQ^{-1}c) - \\dfrac{1}{2}c^T Q^{-1} c $$\n",
    "\n",
    "The dual problem, after a sign change, is:\n",
    "\n",
    "$$ \\min \\dfrac{1}{2}\\mu^\\prime P \\mu + t^\\prime \\mu $$\n",
    "\n",
    "subject to $\\mu \\geq 0$, where $P = A Q^{-1} A^\\prime$ and $t = b + AQ^{-1} c$.\n",
    "\n",
    "Wel, now we can do projection! Super easy! just make sure $\\mu \\geq 0$! These two are exactly the same problem. ONce we find $\\mu^*$, we can plug back into $q(\\mu)$. Then we plug into the problem for $x = -Q^{-1}(c + A^\\prime \\mu)$, and then find $x^*$. This is useful because, for example, the SVM problem. We can derive step by step for the algorithm.\n",
    "\n",
    "Example 2: we want to find the best line to separate two groups of data.\n",
    "\n",
    "$$ \\min_x x^T x $$\n",
    "\n",
    "subject to $b_i(x^T a_i + x_0) \\geq 1, \\forall i$. $b_i$ is a classification label: $b_i = 1$ is a spam, $b_i = -1$ is regular. We want to minimize the size of $x$. Let's look at the dual problem; Note that we have absored $x_0$ by letting $[a_i,1]^T$ be our new $a_i$.\n",
    "\n",
    "Step 1, construct the Lagrangian. Careful, the subject is written as greater or equal to; we always work with less than or equal to.\n",
    "\n",
    "$$ L(x,\\lambda) = \\dfrac{1}{2}||x||^2 - \\sum_i \\lambda_i b_i(a_i^T x) + \\sum_i \\lambda_i $$\n",
    "\n",
    "Setting the gradieent to 0,\n",
    "\n",
    "$$ x = \\sum_i \\lambda_ib_ia_i $$\n",
    "\n",
    "Once we know what $x$ looks like, we can plug it in and obtain the function. \n",
    "\n",
    "$$ \\dfrac{1}{2}|| \\sum_i \\lambda_i b_i a_i ||^2 - \\left( \\sum_i \\lambda_i b_i a_i^T\\right) \\left( \\sum_i \\lambda_i b_i a_i^T \\right) + \\sum_i \\lambda_i $$\n",
    "\n",
    "$$ = \\dfrac{1}{2}||\\sum_i \\lambda_i b_i a_i||^2 + \\sum_i \\lambda_i $$\n",
    "\n",
    "We can merge the two and get the result as:\n",
    "\n",
    "$$ L^*(\\lambda) = \\sum \\lambda_i - \\dfrac{1}{2}\\sum_{i,j} \\lambda_i\\lambda_j b_i b_j a_i^T a_j $$\n",
    "\n",
    "Constraints of the dual optimization:\n",
    "\n",
    "$$ \\lambda_i \\geq 0, \\forall i $$\n",
    "\n",
    "We can obtain the optimal dual variables $\\lambda_i$ by solving\n",
    "\n",
    "$$ \\max_{\\lambda} L^*(\\lambda) = \\sum_i \\lambda_i - \\dfrac{1}{2}\\sum \\lambda_i\\lambda_j b_i b_j a_i^T a_j  $$\n",
    "\n",
    "subject to $\\lambda_i \\geq 0$. We can solve tis problem! When the inerprit the results, suppose we solve the $\\lambda$ problem and want to construct the model $x$. Recmember that we have complementary slakness: $\\lambda_ig_i(x) = 0$ for $g_i(x) \\leq 0$. This means:\n",
    "\n",
    "* $\\lambda_i > 0 \\to b_i (a_ix) = 1$, $a_i$ is a support vector. \n",
    "\n",
    "The optimal solution. We must apply the second portion of the theroem. When we plug this in, we will get $x^*$. This makes the model very interesting:\n",
    "\n",
    "$$ x = \\sum_{i:\\lambda_i > 0}\\lambda_i b_i a_i $$\n",
    "\n",
    "The data is not separable. The two classes are not going to completely separate them. The more useful model is to use some slackness. We allow some error:\n",
    "\n",
    "$$ \\min \\dfrac{1}{2} x^T x + c \\sum_i \\epsilon_i $$\n",
    "\n",
    "subject to $b_i(a^T_i x) \\geq 1 - \\epsilon_i$\n",
    "\n",
    "If we have two sets of data points, it is not possible to divide by one line. Now, we allow some errors, which is what the $\\epsilon$ means. to ensure we still have a good model, we also minimize the sum of the errors, as shown in the above minimization problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "We have two variables now! Lagrangian:\n",
    "\n",
    "\n",
    "$$ \\dfrac{1}{2}||x||^2 + c \\sum_i \\epsilon_i - \\sum_i \\lambda_i(b_i(a_i^T x) - 1 + \\epsilon_i) - \\sum_i \\mu_i\\epsilon_i $$\n",
    "\n",
    "Setting the gratind w.r.t (x,$\\epsilon$) to 0:\n",
    "\n",
    "$$ x = \\sum_i \\lambda_i b_i a_i $$\n",
    "\n",
    "$$ \\dfrac{\\partial L}{\\partial \\epsilon_i} = c - \\lambda_i - \\mu_i = 0 $$\n",
    "\n",
    "For this particular problem, let's get rid of the $\\mu_i$ by making it $\\mu_i = c - \\lambda_i$. Now, applying the same constraing:\n",
    "\n",
    "$$ c - \\lambda_i \\geq 0 \\implies \\lambda_i \\leq C $$\n",
    "\n",
    "Now we have a bound. What is $c$? Basically infinity now! Boooo, this doesn't work. We must have some error. Let's plug it in!\n",
    "\n",
    "$$ q(\\lambda) = \\sum \\lambda_i - \\dfrac{1}{2}||\\sum_{i=1}^r \\lambda_i b_i a_i ||^2 $$\n",
    "$$ \\nabla_j a(\\lambda) = 1- b_j(a_j^T)\\left( \\sum_{i=1}^r \\lambda_i b_i a_i \\right) $$\n",
    "\n",
    "Basically, we can start with some initial number, build a orugh model, then compute the gradient:\n",
    "\n",
    "$$ \\nabla_i L^*(\\lambda^r) = 1 - b_ia_i^T \\sum_j \\lambda_j^r b_ja_j = 1 - b_ia_i^T x^t $$\n",
    "\n",
    "We then perform the gradient projection on $\\lambda_i$. Why this specific step size?\n",
    "\n",
    "$$ \\lambda_i^{r+1} = \\text{proj} \\left[ \\lambda_i^r + \\dfrac{1}{b_i^2 ||z_i^T||^2} \\nabla_i L^*(\\lambda^r) \\right] $$\n",
    "\n",
    "Note, proj$[x]$ basically means projecting to $0 \\leq x \\leq c$. Key: pick one data point at a time, improve the classifier gradually.\n",
    "\n",
    "After we pick the lambda, we go back and update only one thing. \n",
    "\n",
    "$$ x^0 = \\sum_{i=1} \\lambda_k b_i a_j $$\n",
    "$$ \\lambda_1^1, x^1 = \\sum_{i\\neq 1} \\lambda_i^0 b_i a_i + \\lambda_i^1 b_i a_i $$\n",
    "$$ x^1 = x^0 - (\\lambda^0_i -\\lambda_i^1) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 11-27-2017 - Subgradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Both model and algorithm are important. The reason that this is a largely open-ended problem.\n",
    "\n",
    "The subgradient method is simple algorithm to minimize nondifferentiable convex function $f$. \n",
    "\n",
    "Step size rules were discussed. Fixed and diminishing. The important thing is the following inequality:\n",
    "\n",
    "$$ f^{(k)}_{\\text{best}} - p^* \\leq \\dfrac{||x^{(1)} - x^*||_2^2}{???} $$\n",
    "\n",
    "First term is a constant( $x^1$). Sum of step size squared. Denominator is sum of step size. Right hand side guranteed to go to zero, aka, guranteed to find the global min.\n",
    "\n",
    "If we pick a constant step size, the error no longer goes to zero after a certain point. When a small step size is used, there is some oscillaiton, but trend is to go to zero.\n",
    "\n",
    "A related method is the projected subgradient method. Some contraint. We do the exact same thing as the pojected subgradient method. We no longer havea gradient, we have a subgradient. The analysis of the method is the same as the previous one. The only thingwhich will be used in the proof. If we project $P$, it will not increase the distance to $x^*$.\n",
    "\n",
    "$$ || P(u) - P(z)|| \\leq || u - z|| $$\n",
    "\n",
    "here, $u$ and $z$ are two points. The distance between projections will always be less than the distance between the two original points. If we look at a subgradient method, all the functions $f(x)$ is a common problem. Let's rewrite this. \n",
    "\n",
    "$$ L = f_o(x) + \\sum \\lambda_i f(x) $$\n",
    "\n",
    "$$ q(\\lambda) = \\inf L(x,\\lambda) $$\n",
    "\n",
    "The dual problem is to maximize $g(\\lambda)$ such that $\\lambda \\geq 0$. Recall, maximizing this will help minimize the original problem. \n",
    "\n",
    "The algorithms says the following. First, find $x^*$ such that it minimizes $L(x^r, \\lambda^r)$. We want to minimize w.r.t. x. For a fixed $\\lambda$, we minimize $x$. Next iteration, we will update the variable:\n",
    "\n",
    "$$ \\lambda^{r+1} = \\lambda_i^r + \\alpha f_i(x^{r+1}), \\forall i $$\n",
    "\n",
    "This is the method of multipliers. Before we dealt with a constrained problem. Now, with the Legrangian, we optimize $x$ without a constraint. Once done, we update the multiplier with the constraint (shown in previous equation). Because $\\lambda\\geq 0$, remember to project the equation!\n",
    "\n",
    "$$ \\lambda^{r+1} = \\text{Proj}_{\\lambda_i \\geq 0}\\left[\\lambda_i^r + \\alpha f_i(x^{r+1})\\right], \\forall i $$\n",
    "\n",
    "Let's aply this method to this problem. First of alll, we have the problem in primal form. Write down the Lagrangian. Now the constraints are much simpler. Potentially, we can go back, now knowing how to do projection, can we apply to the subgradient? Let's write it down as a minimiztion problem (recall, $g$ is the subgradient):\n",
    "\n",
    "$$ \\lambda^{r+1} = \\text{Proj}_{\\lambda \\geq 0}\\left[ \\lambda^r - \\alpha_r g^r \\right] $$\n",
    "\n",
    "Can we compute a subradient for this objective? This objective is very abstract. This objective is defined as the minimization w.r.t. x. For the constraint violations. If we can show that. Let's simplify things a little bit. To make things as simple as possible, let's assume we have one constraint, $\\lambda$. We will show that:\n",
    "\n",
    "$$ -f(x)^{r+1} \\in a\\lambda(-q(\\lambda^r)) $$\n",
    "\n",
    "Our current constraint evaluated at point $x^{r+1}$ exists within some constraint. What msut we do then? We plug it into our Projection equation! the $-g^r$ becomes $+f(x^{r+1})$. For constrained optimization problem, ignore the constraint, optimize the Lagrantian, update the $\\lambda$. This method aligns percisely with the subgradient projection problem. Be sure to choose step size to be diminishing. The derivation for this is in Secion 6.1, Dual Method. We are optimizing the dual method problem. \n",
    "\n",
    "We want to show that $-f(x) \\in \\alpha (-q(\\mu))$\n",
    "\n",
    "by definition, what do we need to show? What is the definition of the subgradient? \n",
    "\n",
    "$$ -q(\\lambda) \\geq -q(\\mu) + \\alpha(-q(\\mu))(\\lambda - \\mu) $$\n",
    "\n",
    "We need to show that this inequality holds true $\\forall \\lambda$. the subgradient, evaluated at point $\\mu$, always lies below the difference.\n",
    "\n",
    "First, we are given a Legrangian:\n",
    "\n",
    "$$ L(x,\\lambda) = f_o(x) + \\lambda f(x) $$\n",
    "$$ x(\\mu) = \\min_x f_o(x) + \\mu f(x) $$\n",
    "\n",
    "We want tosshow that the current constraint evalueated at the point $\\mu$ is in the subgradint set :\n",
    "\n",
    "$$ -f(x(\\mu)) \\in \\alpha(-q(\\mu)) $$\n",
    "\n",
    "Basically, we want to compute a subgradient set of $q(\\mu)$, recalling that this is defined as:\n",
    "\n",
    "$$ q(\\mu) = \\text{inf} L(x,\\mu) $$\n",
    "\n",
    "For any given $\\lambda$, we want to prove the following:\n",
    "\n",
    "$$ q(\\lambda) \\geq -q(\\mu)+ \\alpha(-f(x(\\mu))(\\lambda-\\mu) $$\n",
    "\n",
    "Notice that $x$ is the minimizer for this problem. By definition, we will hve thw following (here, $x(\\mu)$ is the optimal $x$, and $x(\\lambda)$ is any other x):\n",
    "\n",
    "$$ f_o(x(\\mu)) + \\mu f(x(\\mu)) \\leq f_o(x(\\lambda)) + \\mu f(x(\\lambda)) $$\n",
    "\n",
    "This is the first inequality. Notice that the q function is defined as the objective function. We will add another \n",
    "\n",
    "$$ f_o(x(\\mu)) + \\mu f(x(\\mu)) \\leq f_o(x(\\lambda)) + \\mu f(x(\\lambda)) $$\n",
    "$$ = f_o(x(\\lambda)) + \\lambdaf(x(\\lambda)) + (\\mu - \\lambda)(f(x(\\lambda))) $$\n",
    "\n",
    "Now we have an additional term $(\\mu - \\lambda)$. Now, this is already close. The only two things are the $q$ functions. Recall $q$ is the objective of $L(x,\\mu)$ when we optimize w.r.t. $x$. This is precisely: $q(\\mu) := f_o(x(\\mu)) + \\mu f(x(\\mu))$ and $q(\\lambda) := f_o(x(\\lambda)) + \\lambdaf(x(\\lambda)) + (\\mu - \\lambda)(f(x(\\lambda)))$.\n",
    "\n",
    "Stopping here. The full proof will be in Section 6.1. Let me rewrite the method, an application of why this is important. \n",
    "\n",
    "* Step 1: $L(x,\\mu) = f_x(x) + \\mu f(x)$\n",
    "* Step 2: $(x^r, \\mu^r)$. Minimize the following: $x^{r+1} = \\min L(x, \\mu^r)$\n",
    "    * Then update the following: $\\mu^{r+1} = \\text{Proj}_{\\mu \\geq 0} \\left[ \\mu^r + \\alpha^{r+1} f(x^{r+1}) \\right]$\n",
    "    \n",
    "To keep the x feasible, we will reduce the $\\mu$ further and further until it is negative. Because of the projection, $\\mu$ will be 0. Because $\\mu=0$, the equation in step 1 becomes $L(x,\\mu) = f_o(x)$.\n",
    "\n",
    "This is one way to rationalize the problem. Look at resource constrained problems. We can utalize this structure well. It is related to what we wrote on the slides. Can write on the board to give a step-by-step derivation as well. We minimize:\n",
    "\n",
    "$$ \\min \\sum_{i=1}^m f_i(x_i) $$\n",
    "\n",
    "such that:\n",
    "\n",
    "$$ \\sum_{i=1}^m g_i_j (x_i) \\leq 0, j = 1,...,n $$\n",
    "\n",
    "Suppose we have several things linked together, each with a cost function $f_1, f_2, ...$. We have some resource constraint to spread among them. each $g_{ij}$ represents how system $i$ utalizes resource $j$. Resource $j$ must be satisfied. This means that the nodes with connections\n",
    "\n",
    "$$ L = \\sum_{i=1}^m f_i(x_i) + \\sum_{j=1}^n \\mu_i \\sum_{i=1}^m g_{ij}(x_j) $$\n",
    "$$ x^{r+1} = \\min L(x,\\mu) $$\n",
    "$$ \\mu_j^{r+1} \\text{proj}\\left( \\mu_j^r + \\alpha^r \\sum_{i=1}^m g_{ij} (x_i^{r+1} \\right), \\forall j = 1...n $$\n",
    "\n",
    "We want to minimize $L$ w.r.t. x. We have a lot of sums! This is useful. It is acutally $m$ independent problems. Let's rewrite the objective.\n",
    "\n",
    "$$ L = \\sum_{i=1}^m \\left( f_i(x_i) + \\sum_{j=1}^n \\mu_j g_{ij}(x_i) \\right) $$\n",
    "\n",
    "Now, each thing within the outer sum is independent of $i$. These are now independent problems. We can now minimize each term inside:\n",
    "\n",
    "$$ x^{r+1}_i = \\min f_i(x_i) + \\sum_{j=1}^n \\mu_j g_{ij}(x_i), \\forall i = 1,...,m $$\n",
    "\n",
    "This is doable! :) Each subsystem $i$ is solving it's individual problem. Only related to $x_i$. recall $f_i(x_i)$ is the local cost. No only are we optimizing the local cost, but we care about other systems. All the $\\mu_i g_{ij}$ is the current utalization of the current resource. $\\mu_j$ is the price for the particular resource. Take the current iteration for all the $x$s. If the thing is strictly $\\leq 0$, we will reduce my price, and compute the price for each and every resource. Once we have hte price $\\mu$, we tell everyone this is the current price of the resource, utalization of the resource $g$, then each will decide the $x$. \n",
    "\n",
    "If resource is overutalized, $g > 0$. If underutalized, $g < 0$.  Once we know the $\\mu$s, the problem becomes highly parallelizable. \n",
    "\n",
    "Example:\n",
    "\n",
    "Say we have some sensors, and we can measure the distance between the notes. We want to decide their true locations, defined as $x_j, x_{j+1}, ...$. We want to minimize the distance between the measurements and the true distance $d$.\n",
    "\n",
    "$$ \\min f(x) = \\dfrac{1}{2}\\sum_{ij \\in E} ||| x_i - x_j|| - d_{ij}|^2 $$\n",
    "\n",
    "What we do is pick one component, compute the subgradient, then go on. We can combine the subgradient method with the incremental method. It is a very general method, more general than gradient descent, especially if the problem doesn't have a gradient. Here, the thing to pay atteniton to is the step size. Cannot use a constant step size, must use diminishing. Go to the motivating problem. Remind that the SVM problem can be written as follows:\n",
    "\n",
    "$$ \\min \\dfrac{1}{2} x^T x + c\\sum_i \\epsilon_i $$\n",
    "\n",
    "Wh yd o I need to use subgradient? Let's use an altrnative view:\n",
    "\n",
    "$$ \\min_x \\dfrac{1}{N}\\sum_{i=1}^N \\max \\{ 0, 1-b_i(a_i^T x) \\} + \\gamma x^T x $$\n",
    "\n",
    "We pik one point of data, $i$:\n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha \\partial g_i $$\n",
    "\n",
    "How to take the subgradient of $|x|,$ max${0,1-x}$? We have:\n",
    "\n",
    "$$ \\partial |x| = \\sgn (x) \\text{ if } x \\neq 0 $$\n",
    "$$ \\partial |x| = [-1,1], \\text{ if } x = 0 $$\n",
    "\n",
    "Aplying this then:\n",
    "\n",
    "$$ \\partial g_i = \\begin{cases} -b_i a & b_i^T(a_r^Tx) \\leq 1 \\\\ 0 & b_i^T(a_i^Tx) > 1 \\end{cases} $$\n",
    "\n",
    "Applying this to our iringinal equation, $ x^{r+1} = x^r - \\alpha \\partial g_i $, we get:\n",
    "\n",
    "$$ x^{r+1} = x^r - \\alpha \\partial g_i $$\n",
    "\n",
    "$$ = \\begin{cases} (x^r + \\alpha b_i a_i - 2\\gamma x)  & b_i^T(a_i^Tx) \\leq 1 \\\\ (x^r - 2\\gamma x^r) & b_i^T(a_i^Tx) > 1 \\end{cases} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 11-29-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Structrued optimization theories. We must follow slides. Start with linear regression.\n",
    "\n",
    "Have a training set, where $a_i$ is the feature and $b_i$ is the label.\n",
    "\n",
    "Objective: minimize the squared loss:\n",
    "\n",
    "$$ L(x) = \\dfrac{1}{2} ||Ax - b||^2$$\n",
    "\n",
    "In practice, performing linear regression is not enough. Linear regression gives us a linear model. We've combined the linear features. We want to only know a few key features. All other things are mostly noise. There could be problems where $x$ is very big. $x$ is size $n$ by 1 and $A$ is size $m$ by $n$. $n$ is the number of features and $m$ is the number of observations. For examples, genes. Very hard to get samples, but lots of data. Potentially lots of solutions to $Ax = b$. How to determine the \"best\" solution. \n",
    "\n",
    "Here, we add a regularization factor:\n",
    "\n",
    "$$ L(x) = \\dfrac{1}{2}\\sum_i \\left( a_i^T x - b_i \\right)^2 + R(x) $$\n",
    "\n",
    "This new term can arbitrarily impose sparsity. The second application is SVM. IN our homework, we only have two features. Those are basically the entire data pixels.\n",
    "\n",
    "Third application related to image denoising. We are given a noisy image. We want to find a clean one. We add in a regulazier:\n",
    "\n",
    "$$ \\min_x ||X - Y||^2 + \\gamma \\sum_i^M ||x_i - x_{x+1}||^2 + ||x_i - x_{i-1}||^2 $$\n",
    "\n",
    "This term is related to the regularizer. Basically, the term means that we want tit  to have a certain amount of structure. We want adjacent pixels to be similar.\n",
    "\n",
    "The next one is very popular, compressive sensing. We want to estimate. In MRI systems, $x$ is the image of interest. $A$ is something which transforms the image to the observation.\n",
    "\n",
    "We want toseparate the foreground and the background. Foreground is sparse. Background is very similar across all video frames. If they are very similar, matrices are of low rank. Thus, $S$ (foreground) must be sparse, $L$ (for background) must be of low rank, and $Z$ (for noise) must be small. \n",
    "\n",
    "$$ \\min ||L||_* + \\gamma_1 ||S||_1 + \\gamma ||Z||_F $$\n",
    "\n",
    "Subject to $L + S + Z = M$. \n",
    "\n",
    "Enforcing sparsity?\n",
    "\n",
    "Why $l_1$ norm? Mostly, from an optimality condition. Consider an optimization problem. Thikn about the simplist case, where $x$ is a scalar. We can write the optimality condition as such:\n",
    "\n",
    "$$ \\min f(x) := L(x) + \\lambda|x| $$\n",
    "\n",
    "Plug in the subdifferentiable here.\n",
    "\n",
    "We want to minimize:\n",
    "\n",
    "$$ \\min ||x-b||^2_2 + \\gamma ||x||_1 $$\n",
    "$$ = \\sum_{i=1}^N (x_i - b_i)^2 + \\gamma\\sum_{i=1}^N |x_i| $$\n",
    "\n",
    "$$ = \\sum_{i=1}^N \\left[ \\left( x_i - b_i \\right)^2 + \\gamma|x_i| \\right] $$\n",
    "\n",
    "For $i=1...N$ check if $b_i \\leq \\gamma$\n",
    "if yes: $x_i=0$\n",
    "if no: if sgn($b + \\gamma = -1$)\n",
    "    $x_i = b_i + \\gamma$\n",
    "    otherwise\n",
    "    $x_i = b_i - \\gamma$\n",
    "    \n",
    "This is still different than the gregression problem. How to minimize?\n",
    "\n",
    "Stradegy 1: coordinate descent.\n",
    "\n",
    "$$ \\dfrac{1}{2}||a_i x_i + \\sum_{j \\neq i} a_jx_j - b||^2 + \\gamma ||x||_1  $$\n",
    "\n",
    "Basically, we will ignore all of the $j$s and optimize the $i$s. \n",
    "\n",
    "$$ \\dfrac{1}{2}||a_i x_i + C||^2 + \\gamma|x_i| $$\n",
    "\n",
    "Now the optimization variable is only one component. We still need to put it in the form: $\\min \\dfrac{1}{2} |x-b|^2 + \\gamma |x|$\n",
    "\n",
    "$$ \\dfrac{1}{2}||a_i||^2 + c^T a_i x_i + ||c||^2\\dfrac{1}{2} + \\gamma |x_i| $$\n",
    "\n",
    "So we divide by $||a_i||^2$\n",
    "\n",
    "$$ = \\dfrac{1}{2}x_i^2 + \\dfrac{C^T a_i}{||z_i||^2}x_i + \\dfrac{||c||^2}{2||a_i||^2} + \\dfrac{\\gamma}{||z_i||^2}|x_i| $$\n",
    "\n",
    "$$ \\dfrac{1}{2}\\left( x_i + \\dfrac{c^T a_i}{||a_i||^2} \\right)^2 + \\dfrac{\\gamma}{||a_i||^2}|x_i| $$\n",
    "\n",
    "Now we have the desired form! :) \n",
    "\n",
    "Got distracted...\n",
    "\n",
    "$$ := \\dfrac{L}{2}||x - x^r + \\dfrac{1}{L} \\nabla f(x^r) ||^2 + \\gamma||x||_1 $$\n",
    "\n",
    "$$ := \\dfrac{L}{2}||x - \\left( x^r - \\dfrac{1}{L}\\nabla f(x^r) \\right)||^2 + \\dfrac{\\lambda}{L} ||x||_1 $$\n",
    "\n",
    "Here, $b = \\left( x^r - \\dfrac{1}{L}\\nabla f(x^r) \\right)$. Now we are in the form. First perform one gradient on the step assuming the $\\lambda ||x||$ portion doesn't exist. Then you go on. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture 12-4-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the following scalar optimization problem:\n",
    "\n",
    "$$ \\min \\dfrac{1}{2}||x - b||^2 + \\lambda ||x||_1 $$\n",
    "\n",
    "this problem is seperable, and the same as\n",
    "\n",
    "$$ \\dfrac{1}{2} \\sum_{i=1}^n (x_i - b_i)^2 + \\lambda |x_i| $$\n",
    "\n",
    "We can solve this through a lot of ways.\n",
    "\n",
    "Why having the $l_1$ norm induces a sparse solution. Consider the prlbem:\n",
    "\n",
    "$$ Ax = b $$\n",
    "\n",
    "This is the data, with $A$ as $mxn$, $x$ is $nx1$ and $b$ is $mx1$. Suppose now that we have $n$ as big and $m$ as small. This is a severely underdetermined problem. there are several solutions; we want to find one that also satisfies the norms:\n",
    "\n",
    "* min $||x||_0$ maximum number of nonzero components\n",
    "\n",
    "Can we draw a picture to visualize the solution. Start with min $||x||_2$. We have a line (representing $Ax = b$. We expand a circle (representing $||x||_2$ until it is tangent to the line.\n",
    "\n",
    "$l_1$ norm is a good onvex approximation of the $l_0$ norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between this and last problem:\n",
    "\n",
    "$$ \\min f(x) := g(x) + R(x0 $$\n",
    "\n",
    "Can we use gradient descent?\n",
    "\n",
    "Review what we did for gradient descent. in gradinet descent, we approxmated $g$ by its xecond order approximation:\n",
    "\n",
    "$$ g(x) \\leq g(x^r) + \\langle \\nabla g(x^r), x - x^r \\rangle + \\dfrac{L}{2}||x - x^r||^2 $$\n",
    "\n",
    "The second order approximation, the curvature of the funciton must be larger than $L$ (basically the largest curvature in the original problem). After we consruct this, we minimize the second order problem, then use that min to find the next min. \n",
    "\n",
    "1. compute: from $x^r - \\dfrac{1}{L} \\nabla g(x^r)$\n",
    "2. compute $x^{r+1} = \\min \\dfrac{1}{2}||x - (x^r - \\dfrac{1}{L}\\nabla g(x^r)||^2 \\dfrac{1}{L}R(x)$\n",
    "\n",
    "We need a sub problem. We solve using the proximity operator:\n",
    "\n",
    "$$ \\text{prox}_r^{\\lambda/L}(y) := \\min_x\\dfrac{1}{2}||x - y||^2 + \\dfrac{\\lambda}{L}R(x) $$\n",
    "\n",
    "The list of proximity operators can be found in a table on the slides. Take an example:\n",
    "\n",
    "$$ R(x) = \\dfrac{1}{2}d^2_x(x) $$\n",
    "\n",
    "Where $d_x(x)$ is the distance between $x$ and the convex set $X$. The operator can be written as $d_x^2(x) = \\min_{z \\in X} ||x-z||^2$ The prox operator is:\n",
    "\n",
    "$$ \\text{prox}_R^1(y) = \\min_x \\dfrac{1}{2}d_x^2(x) + \\dfrac{1}{2}||x-y||^2 $$\n",
    "\n",
    "plug the definition of $d_x(x)$ into the equation.\n",
    "\n",
    "$$ = \\min_{z \\in X,x} \\dfrac{1}{2}||z - x||^2 + \\dfrac{1}{2}||x - y||^2 $$\n",
    "\n",
    "We need to fix $z$, then solve for an optimal $x$. With a minimized $z$, we should only be left with $\\dfrac{1}{2}$. We now have two optimization problems. Our optimal point should lie somewhere along $x^* = \\dfrac{1}{2}(z-y)$. Plug this back into the objective. We then have:\n",
    "\n",
    "$$ \\dfrac{1}{2}||\\dfrac{1}{2}(y - z)||^2 + \\dfrac{1}{2}||(z-y)||^2 $$\n",
    "\n",
    "$$ = \\dfrac{1}{2}\\left( y + \\min_{z \\in X} \\dfrac{1}{4}||z - y||^2 \\right) $$\n",
    "$$ = \\dfrac{y + \\text{proj}_x(y)}{2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
