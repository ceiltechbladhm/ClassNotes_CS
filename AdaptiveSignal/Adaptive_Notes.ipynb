{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Note Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    var add_command_shortcuts = {\n",
       "            'Alt-w' : {\n",
       "                help    : 'Add Text Test',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                        var cell = IPython.notebook.get_selected_cell();\n",
       "        \t\tcell.code_mirror.replaceSelection('Testing');\n",
       "                }\n",
       "            }\n",
       "        };\n",
       "\n",
       "    var add_edit_shortcuts = {\n",
       "            'Alt-a' : {\n",
       "                help    : 'Insert alpha',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\alpha');\n",
       "                }\n",
       "            },\n",
       "        'Alt-b' : {\n",
       "                help    : 'Insert beta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\beta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-g' : {\n",
       "                help    : 'Insert gamma',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\gamma');\n",
       "                }\n",
       "            },\n",
       "        'Alt-shift-g' : {\n",
       "                help    : 'Insert Gamma',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\Gamma');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Ctrl-d' : { //Change\n",
       "                help    : 'Insert delta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\delta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Ctrl-e' : {\n",
       "                help    : 'Insert epsilon',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\epsilon');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Ctrl-Shift-e' : {\n",
       "                help    : 'Insert varepsilon',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\varepsilon');\n",
       "                }\n",
       "            },\n",
       "        'Alt-z' : {\n",
       "                help    : 'Insert zeta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\zeta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-t' : {\n",
       "                help    : 'Insert theta',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\theta');\n",
       "                }\n",
       "            },\n",
       "        'Alt-k' : {\n",
       "                help    : 'Insert kappa',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\kappa');\n",
       "                }\n",
       "            },\n",
       "        'Alt-l' : {\n",
       "                help    : 'Insert lambda',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\lambda');\n",
       "                }\n",
       "            },\n",
       "        'Alt-p' : {\n",
       "                help    : 'Insert prime',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\prime');\n",
       "                }\n",
       "            },\n",
       "        'Alt-r' : {\n",
       "                help    : 'Insert rho',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\rho');\n",
       "                }\n",
       "            },\n",
       "        'Alt-s' : {\n",
       "                help    : 'Insert sigma',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\sigma');\n",
       "                }\n",
       "            },\n",
       "        'Alt-w' : {\n",
       "                help    : 'Insert omega',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\omega');\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-1' : {\n",
       "                help    : 'Insert dfrac',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\dfrac{}{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-2' : {\n",
       "                help    : 'Insert equation',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('$$  $$');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-3' : {\n",
       "                help    : 'Insert bar',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\overrightarrow{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-4' : {\n",
       "                help    : 'Insert hat',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\hat{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-5' : {\n",
       "                help    : 'Insert hat',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\tilde{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-6' : {\n",
       "                help    : 'Insert parenthesis',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\left(  \\\\right)');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-7' : {\n",
       "                help    : 'Insert Brackets',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\left[  \\\\right]');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-8' : {\n",
       "                help    : 'Insert angle',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\langle  \\\\rangle');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-9' : {\n",
       "                help    : 'Insert Matrix',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\left[\\\\begin{matrix}  \\\\end{matrix}\\\\right]');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-20});\n",
       "                }\n",
       "            },\n",
       "        'Alt-0' : {\n",
       "                help    : 'Insert partial',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\partial ');\n",
       "                }\n",
       "            },\n",
       "        'Ctrl-Alt-n' : {\n",
       "                help    : 'Insert nabla',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\nabla');\n",
       "                }\n",
       "            },\n",
       "        'Alt-Shift-t' : {\n",
       "                help    : 'Insert text',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('\\\\text{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "/*        'Shift-6' : {\n",
       "                help    : 'Insert superscript',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('^{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "        'Shift-_' : {\n",
       "                help    : 'Insert subscript',\n",
       "                help_index : 'aa',\n",
       "                handler : function() {\n",
       "                    var cell = IPython.notebook.get_selected_cell();\n",
       "                    cell.code_mirror.replaceSelection('_{}');\n",
       "                    var cpos = cell.code_mirror.getCursor();\n",
       "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
       "                }\n",
       "            },\n",
       "*/\n",
       "        };\n",
       "\n",
       "   // var load_ipython_extension = function() {\n",
       "        IPython.keyboard_manager.edit_shortcuts.add_shortcuts(add_edit_shortcuts);\n",
       "        IPython.keyboard_manager.command_shortcuts.add_shortcuts(add_command_shortcuts);\n",
       "    //};"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "    var add_command_shortcuts = {\n",
    "            'Alt-w' : {\n",
    "                help    : 'Add Text Test',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                        var cell = IPython.notebook.get_selected_cell();\n",
    "        \t\tcell.code_mirror.replaceSelection('Testing');\n",
    "                }\n",
    "            }\n",
    "        };\n",
    "\n",
    "    var add_edit_shortcuts = {\n",
    "            'Alt-a' : {\n",
    "                help    : 'Insert alpha',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\alpha');\n",
    "                }\n",
    "            },\n",
    "        'Alt-b' : {\n",
    "                help    : 'Insert beta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\beta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-g' : {\n",
    "                help    : 'Insert gamma',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\gamma');\n",
    "                }\n",
    "            },\n",
    "        'Alt-shift-g' : {\n",
    "                help    : 'Insert Gamma',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\Gamma');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Ctrl-d' : { //Change\n",
    "                help    : 'Insert delta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\delta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Ctrl-e' : {\n",
    "                help    : 'Insert epsilon',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\epsilon');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Ctrl-Shift-e' : {\n",
    "                help    : 'Insert varepsilon',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\varepsilon');\n",
    "                }\n",
    "            },\n",
    "        'Alt-z' : {\n",
    "                help    : 'Insert zeta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\zeta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-t' : {\n",
    "                help    : 'Insert theta',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\theta');\n",
    "                }\n",
    "            },\n",
    "        'Alt-k' : {\n",
    "                help    : 'Insert kappa',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\kappa');\n",
    "                }\n",
    "            },\n",
    "        'Alt-l' : {\n",
    "                help    : 'Insert lambda',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\lambda');\n",
    "                }\n",
    "            },\n",
    "        'Alt-p' : {\n",
    "                help    : 'Insert prime',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\prime');\n",
    "                }\n",
    "            },\n",
    "        'Alt-r' : {\n",
    "                help    : 'Insert rho',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\rho');\n",
    "                }\n",
    "            },\n",
    "        'Alt-s' : {\n",
    "                help    : 'Insert sigma',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\sigma');\n",
    "                }\n",
    "            },\n",
    "        'Alt-w' : {\n",
    "                help    : 'Insert omega',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\omega');\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-1' : {\n",
    "                help    : 'Insert dfrac',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\dfrac{}{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-2' : {\n",
    "                help    : 'Insert equation',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('$$  $$');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-3});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-3' : {\n",
    "                help    : 'Insert bar',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\overrightarrow{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-4' : {\n",
    "                help    : 'Insert hat',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\hat{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-5' : {\n",
    "                help    : 'Insert hat',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\tilde{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-6' : {\n",
    "                help    : 'Insert parenthesis',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\left(  \\\\right)');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-7' : {\n",
    "                help    : 'Insert Brackets',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\left[  \\\\right]');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-8' : {\n",
    "                help    : 'Insert angle',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\langle  \\\\rangle');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-8});\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-9' : {\n",
    "                help    : 'Insert Matrix',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\left[\\\\begin{matrix}  \\\\end{matrix}\\\\right]');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-20});\n",
    "                }\n",
    "            },\n",
    "        'Alt-0' : {\n",
    "                help    : 'Insert partial',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\partial ');\n",
    "                }\n",
    "            },\n",
    "        'Ctrl-Alt-n' : {\n",
    "                help    : 'Insert nabla',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\nabla');\n",
    "                }\n",
    "            },\n",
    "        'Alt-Shift-t' : {\n",
    "                help    : 'Insert text',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('\\\\text{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "/*        'Shift-6' : {\n",
    "                help    : 'Insert superscript',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('^{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "        'Shift-_' : {\n",
    "                help    : 'Insert subscript',\n",
    "                help_index : 'aa',\n",
    "                handler : function() {\n",
    "                    var cell = IPython.notebook.get_selected_cell();\n",
    "                    cell.code_mirror.replaceSelection('_{}');\n",
    "                    var cpos = cell.code_mirror.getCursor();\n",
    "                    cell.code_mirror.setCursor({line: cpos.line, ch: cpos.ch-1});\n",
    "                }\n",
    "            },\n",
    "*/\n",
    "        };\n",
    "\n",
    "   // var load_ipython_extension = function() {\n",
    "        IPython.keyboard_manager.edit_shortcuts.add_shortcuts(add_edit_shortcuts);\n",
    "        IPython.keyboard_manager.command_shortcuts.add_shortcuts(add_command_shortcuts);\n",
    "    //};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For parts of the class, Spectral analysis will be used. The textbook is a bit difficult to read b/c of the differences between notation.\n",
    "\n",
    "There is some unknown plant. We take the input, where the plant is doing something, and it has an output. We want to understand the output of a plant. We wantto identify what is going on inside the plant. The output of the filter  is subtracted from the output of the plant, making the error signal. This is identifying a system.\n",
    "\n",
    "Inverse systems is trying to undo what a system has done. If a system has a transfunction $A$, the inverse will return the system to unity. The adaptive filter will represent the inverse of the plant. This is the basic assumption of many communication systems. The plant is a channel, transmission channel. Whatever signal which was being transmitted has been distorted; we want to restore it.\n",
    "\n",
    "A delay is required for comparing the original signal with the output of the plant and adaptive filter because these two former portions also have a delay.\n",
    "\n",
    "Prediction is the third application. We have some signal; there is a delay, then fed into an adaptive filer. \n",
    "\n",
    "The error might only have a 3-bit dynamic range. Instead of transmitting the speed singal, we can transmit the error signal (easier to do, requires much fewer bits). Sensors are not collecting ideal data; they collect noisy data. If we measure the heart or lung sound, or the EEG, those are all noisy data. We can cancel the noise. \n",
    "\n",
    "Tests are open notes. Homework are done by yourself. Free to discuss with friends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Random Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Beginning with chapter 1. We will try to use book notation. $U(n)$ is an observation, listed as $u(n), u(n-1),...,u(n-M)$.\n",
    "\n",
    "We are interested in wide-sense stationary processes. What's the difference bttween a random process or a random variable?  A random process is a time series ($u(n)$ is an example). Each sample within $u(n)$ is a random variable. If tehe system is not wide-sense stationary, all the tools we learn about doesn't hold. Same with LTI systems for FIR filters.\n",
    "\n",
    "What is wide-sense stationary?\n",
    "1. $E\\{u(n)\\} = \\mu_n = \\mu \\forall n$. The mean is independent of the time index.\n",
    "2. Autocorrelation. $r(n,n-k) = E\\{ u(n) u^*(n-k) \\} = r(k)$. Just as how the mean is independent of time, the autocorrelation does not depend on n. (P.S. $k$ is also called \"lag\". Lag 1 mean $E\\{\\u(n) u^*(n-1)\\}$). \n",
    "3. Autocovariance. $c(n,n-k) = E\\{ \\left( u(n) - \\mu \\right)\\left( u^*(n-k) - \\mu \\right) \\} = c(k)$.\n",
    "\n",
    "We will talk about the real time series. It is a comples time series. Take an example of time series of tempterature. The statistical properties of temperature, the properties of temperature between January 10-15 is the same as June 10-15. The difference is key; not the absolute value.\n",
    "\n",
    "If we substitute $k = 0$:\n",
    "$$ r(0) = E\\{ u(n) u^*(n) \\} $$\n",
    "$$ = E\\{ |u(n)\\}|^2 \\} \\geq 0 $$\n",
    "\n",
    "This is the average power, assumed to be finite.\n",
    "\n",
    "The double sum formula.\n",
    "\n",
    "$$ \\sum_{n=0}^{N-1} \\sum_{m=0}^{N-1} f(n-m) = \\sum_{l=-(N-1)}^{N-1}\\left( N - |l| \\right) f(l) $$\n",
    "\n",
    "$$ \\sum_{l=-(N-1)}^{N-1}\\left( 1 - \\dfrac{|l|}{N} \\right)f(l) $$\n",
    "\n",
    "Let's prove it. \n",
    "\n",
    "<img src='IMG_0659.JPG'>\n",
    "\n",
    "All the values along the diagonal are the same; therefore, it can be represented by a scalar multiple of the number of each element in the table.\n",
    "\n",
    "$$ \\sum ]sum - f(N-1) + .. + (N-1) f(1) + N f(0) + (N-1) f(-1) + ... + f(-(N-1)) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Real harmonic process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ u(n) = A\\sin \\left( n \\omega_o + \\phi \\right) $$\n",
    "\n",
    "$\\omega_o$ is static, and $\\phi$ is a random process $f_\\phi(n)$.\n",
    "\n",
    "$$ E\\{ u(n) \\} = A E\\{ \\sin\\left( n \\omega_o + \\phi \\right) \\} $$\n",
    "$$  = \\dfrac{A}{2\\pi}\\int_0^2\\pi \\sin \\left( n\\omega_o + \\phi \\right)d\\phi = 0 $$\n",
    "\n",
    "if the $\\mu=0$, then the autocorrelation is the same as the covariance. \n",
    "\n",
    "$$ r(n,n-k) = E\\{ u(n) u^*(n-k) \\} $$\n",
    "$$ = A^2 E\\{ \\sin (n\\omega_o + \\phi)  \\sin ((n-k)\\omega_o + \\phi) \\} $$\n",
    "\n",
    "\n",
    "Recall that $ 2\\sinA \\sin B = \\dfrac{1}{2}\\left[ \\cos(A-B) - \\cos(A+B) \\right] $\n",
    "\n",
    "$$ \\dfrac{A^2}{2} E\\{ \\cos \\left( k \\omega_0 \\right) - \\cos \\left( (2n-k)\\omega_o + 2\\phi \\right) \\} $$\n",
    "\n",
    "The only random part is $\\phi$. The second part. Just like the average value of sin is 0.\n",
    "\n",
    "$$ r(n,n-k) = \\dfrac{A^2}{2}\\cos k\\omega_o $$\n",
    "\n",
    "What happens if we have white noise? How to characterize it?  $v(n)$ has characteristics of $(0,\\sigma_v^2)$. \n",
    "\n",
    "$$ r_v(n,n-k) = \\sigma_v^2 \\delta(k) $$\n",
    "\n",
    "$$ r_v (n,n-k) = E\\left[ v(n) v^*(n-k) \\right] $$\n",
    "$$ = E\\left[ v(n) \\right] E\\left[ v^*(n0k) \\right] $$\n",
    "\n",
    "As long as $k neq 0$. if $k=0$, $r_v(n,n) = E\\left[ |v|^2 \\right]$\n",
    "\n",
    "$$ u(n) = A \\sin(n\\omega_o + \\phi) + v(n) $$\n",
    "\n",
    "$$ r_u(k) = \\dfrac{A^2}{2}\\cos\\left( k \\omega_o + \\sigma_v^2 \\delta(k) \\right) $$\n",
    "\n",
    "This is the autocorrelation of the noisy sinusoid. We could also take a mixture of many ($N$) sinusoids.\n",
    "\n",
    "$$ u(n) = \\sum_{m=1}^N A_m \\sin(n \\omega_m + \\phi_m) + v(n) $$\n",
    "\n",
    "$$ r_u(k) = \\dfrac{1}{2}\\left[ \\sum_{m=1}^N A_m^2 \\cos k\\omega_m \\right] + \\sigma_v^2 \\delta(k) $$\n",
    "\n",
    "If the mean is not independent of $n$, then it's not wide-sense stationary. What is the diffrence between. We must understand two different averages. Ovservation at time end. $u(n)$ at a particular $n$ is a random variable. \n",
    "\n",
    "We can also take one instance. Sample average is one sample over many simulations. Time behavior, we look at one simulation, and see how it progresses over time.\n",
    "\n",
    "Ergodicity. Ensemble average is written as $\\hat{\\mu}(n) = \\dfrac{1}{N} \\sum_{i=0}^{N-1} \\mu_i(n)$.\n",
    "\n",
    "Time Average:\n",
    "\n",
    "$$ \\hat{\\mu}(N) = \\dfrac{1}{N}\\sum_{n=0}^{N-1} u(n) $$\n",
    "\n",
    "The key difference is that there is no index for $u$ in the time average.\n",
    "\n",
    "** Example **\n",
    "\n",
    "Let's say we have a random process $u(m)$\n",
    "\n",
    "$$ u(n) = A \\cos n\\omega_o $$\n",
    "\n",
    "$$ \\begin{cases} P(A=1) = \\dfrac{1}{2} \\\\ P(A=2) = \\dfrac{1}{2} \\end{cases} $$\n",
    "\n",
    "There is equal probability that $A=1,2$. The ensemble average is:\n",
    "\n",
    "$$ E\\left[ u(n) \\right] = E\\left[ A \\right] \\cos n\\omega_o = 1.5 \\cos n \\omega_o $$\n",
    "\n",
    "If we take the time average for a specific observation:\n",
    "\n",
    "$$ \\hat{\\mu}(N) = \\dfrac{1}{N} \\sum_{n=0}^{N-1} A \\cos n\\omega_o = 0 $$\n",
    "\n",
    "Summing up:\n",
    "1. Ensemble average: 1.5\n",
    "2. Time average: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Mean Squared Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We want to understand the mean squared convergence of ensemble mena and time convergence. Let's start with the ensemble mean.\n",
    "\n",
    "$$\\lim_{N \\to \\infty} E\\left[ |\\mu - \\hat{\\mu}(N)|^2 \\right] $$\n",
    "\n",
    "$$ E\\left[ |\\mu - \\hat{\\mu}(N)|^2  \\right] = E\\left[ \\left(\\mu - \\dfrac{1}{N}\\sum_{n=1}^{N-1} u(n)\\right)\\left( \\mu^* - \\dfrac{1}{N}\\sum_{n=1}^{N-1} u^*(n) \\right) \\right] $$\n",
    "$$ = \\dfrac{1}{N^2}E\\left[ \\sum_{n=0}^{N-1} \\sum_{m=0}^{N-1} \\left( u(n) - \\mu \\right)\\left( u^*(m) - \\mu^* \\right) \\right] $$\n",
    "$$ = \\dfrac{1}{N^2} \\sum_n \\sum_m c(n-m) $$\n",
    "\n",
    "This is the covariance. Recall the double sum property discussed earlier in the lecture. Therefore, we now have:\n",
    "\n",
    "$$ =\\dfrac{1}{N} \\sum_{l= -(N-1)}^{N-1} (N-1)\\left( 1 - \\dfrac{|l|}{N} \\right) c(l) $$\n",
    "\n",
    "$$ = \\dfrac{1}{N}\\sum_{l = -(N-1)}^{N-1} c(l) - \\dfrac{1}{N^2} \\sum_{l = -(N-1)}^{N-1} |l| c(l) $$\n",
    "\n",
    "Take a look at this with the limit. The last term boes to zero.as this limit decays:\n",
    "\n",
    "$$ lim_{N \\to \\infty} \\dfrac{1}{N}\\sum_{l = -(N-1)}^{N-1} c(l) - \\dfrac{1}{N^2} \\sum_{l = -(N-1)}^{N-1} |l| c(l) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Correlation matrix and density matrix. We defined the autocorrelation earier as $r$.\n",
    "\n",
    "$$ r(k) = E\\left[ x(n)x^*(n-k) \\right] $$\n",
    "\n",
    "Cross correlation\n",
    "\n",
    "$$ r_xy(k) = E\\left[ x(n)y^*(n-k) \\right] $$\n",
    "\n",
    "The correlation matrix:\n",
    "\n",
    "$$ U(n) = \\left[\\begin{matrix} u(n)\\\\ u(n-1) \\\\ ... u(n-(M-1)) \\end{matrix}\\right] $$\n",
    "\n",
    "The correlation matrix is:\n",
    "\n",
    "$$ R_U  = R = E\\left[ U(n) U^H(n) \\right] $$\n",
    "\n",
    "This is the hermitian transpose. It is the complex conjugate and the transpose. The matrix $A$.\n",
    "\n",
    "$$ a = \\left[ 2 \\\\ 3+j \\\\ 1-j \\right] $$\n",
    "$$ a^H = \\left[\\begin{matrix} 2 & 3-j & 1+j \\end{matrix}\\right] $$\n",
    "\n",
    "Let's take another example:\n",
    "\n",
    "$$ A = \\left[\\begin{matrix} 2 & 1-j \\\\ 3+ j & j \\\\ 1 - j & 2 \\end{matrix}\\right] $$\n",
    "$$ A^H = \\left[\\begin{matrix} 2 & 3-j & 1+ j \\\\ 1+j & -j & 2 \\end{matrix}\\right] $$\n",
    "\n",
    "Back to the correlation matrix. The matrix is going to be:\n",
    "\n",
    "$$ R = E\\left[ \\left[\\begin{matrix} u(n) \\\\ u(n-1) \\\\ ... \\\\ u(n-M+1) \\end{matrix}\\right] \\left[\\begin{matrix} u^*(n) & u^*(n-1) & ... \\end{matrix}\\right] \\right] $$\n",
    "\n",
    "Expanding this out:\n",
    "\n",
    "$$ R = \\left[\\begin{matrix} E\\left[ u(n) u^*(n) \\right] & E\\left[ u(n)u^*(n-1) \\right] & ... \\\\ E\\left[ u(n-1) u^*(n) \\right] & E\\left[ u(n-1)u^*(n-1) \\right] & ... \\\\ \\vdots & \\vdots & \\ddots& \\vdots   \\end{matrix}\\right] $$\n",
    "\n",
    "Take the difference of indices for autocorrelation:\n",
    "\n",
    "$$ = \\left[\\begin{matrix} r(0) & r(1) & ... & r(M-1) \\ r(-1) & r(0) & ... & \\\\ ... & & &\\\\r(-M+1) & ... & ...& r(0) \\end{matrix}\\right] $$\n",
    "\n",
    "This is a Toeplitz matrix, where all the diagonals are the same. It so happens that $r(1) = r^*(-1)$. Let's prove this:\n",
    "\n",
    "$$ r(-k) = r^*(k) $$\n",
    "$$ r(k) = E\\left[ x(n-k) x^*(n) \\right] $$\n",
    "$$ = \\left[E\\left[ x^*(n-k)x(n) \\right]\\right]^* $$\n",
    "$$ \\left[ E\\left[ x(n) x^*(n-k) \\right] \\right] $$\n",
    "\n",
    "We look at this and see this is exactly the conjugate of $r(k)$.\n",
    "\n",
    "$$ r^*(k) $$\n",
    "\n",
    "If we take the hermitian operator of $R$, we transpose and take the conjugate, we see it's the same (I think??? yes!). R is therefore Toeplitz and Hermitian. In the case for real signals, the Hermitian matrix becomes a symmetric matrix.\n",
    "\n",
    "There are some very nice properties about this. For any wide-send stationary process, the question we can ask, can any Hermitian matrix be a correlation matrix of a WSP? We hace shown that the WSE will be a hermitian process. We are now asking the inverse.\n",
    "\n",
    "Let's give a few examples.\n",
    "\n",
    "$$ R = \\left[\\begin{matrix} -2 & 3 \\\\ 3 -2 \\end{matrix}\\right] $$\n",
    "\n",
    "The diagonal is a negative. This cannot be possible to be a correlation matrix, since it is filled with $r(0) = e\\left[ |u(n)|^2 \\right]$.\n",
    "\n",
    "Another example:\n",
    "\n",
    "$$ R = \\left[\\begin{matrix} 2 & 3 \\\\ 3 & 2 \\end{matrix}\\right] $$\n",
    "\n",
    "This is also not possible. The fact that the off-diagonal elements are greater than the diagonals means that this is not possible.\n",
    "\n",
    "Let's first prove that R is non-negative definite. Any vector $a \\neq 0$, then the definition of non-negativity is that $a^H R a \\geq 0$.\n",
    "\n",
    "$$ \\overrightarrow{a}^H R \\overrightarrow{a} $$\n",
    "$$  = \\overrightarrow{a}^H E \\left[ \\overrightarrow{u}\\overrightarrow{U}^H \\right] \\overrightarrow{a} $$\n",
    "$$ = E\\left[ \\overrightarrow{a}^H \\overrightarrow{U}\\overrightarrow{U}^H \\overrightarrow{a} \\right] $$\n",
    "\n",
    "Let's define $y = \\overrightarrow{a}^H \\overrightarrow{U}$. This is a scalar since $a$ is 1xM and $\\overrightarrow{U}$ is $Mx1$.\n",
    "\n",
    "$$ = E\\left[ |y|^2 \\right] \\geq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we need to show that\n",
    "\n",
    "$$ |r(k)| < r(0) $$\n",
    "\n",
    "There are many ways to prove it. This is the first brute-force proof.\n",
    "\n",
    "$$ E\\left[ \\left( u(n+k) - au(n) \\right)\\left( u^*(n+k) - a^*u^*(n) \\right) \\right] \\geq 0 $$\n",
    "\n",
    "If we now multiply and take the expected value:\n",
    "\n",
    "$$ E\\left[ U(n+k)u^*(n+k) + |a|^2 u(n)u^*(n) - a^*u(n+k)u^*(n) - au(n) u^*(n+k) \\right] \\geq 0 $$\n",
    "\n",
    "$$ \\left( 1 + |a|^2 \\right)r(0) - a^*r(k) - ar(-k) \\geq 0 $$\n",
    "\n",
    "Let's assume that $r(k)$ is in general a complex number: $r(k) = |r_k| e^{j\\phi(k)}$. Because we have assumed that $a$ is any arbitrary number, we can now choose a specific number $a = e^{j\\phi(k)}$. What is absolute value of $a$? 1!\n",
    "\n",
    "$$ 2\\left[ r(0) - |r(k)| \\right] \\geq 0 $$\n",
    "$$ r(0) \\geq |r(k)| $$\n",
    "\n",
    "Eigenvalues of R are real and nonnegative (positive). It is impossible for a correlation matrix to be singular. Why is it possible? For any random WSS process, there is always some noise. This so-called noise makes sure that the $r(0) > 0$, because every physical process has noise. If there were some signal without noise, it could be singular. \n",
    "\n",
    "$$ A v_i = \\lambda_i v_i $$\n",
    "\n",
    "Here, $\\lambda_i$ is the eigenvalue. If we premultiply $v_i^H \\implies v_i^H Av_i = \\lambda_i v_i^H v_i$\n",
    "\n",
    "If we take the Hermitian operator of both sides:\n",
    "\n",
    "$$ v_i^H A^H = \\lambda_i^* v_i^H $$\n",
    "\n",
    "If we postmultiply\n",
    "\n",
    "$$ v_i \\implies v_i^H A^H v_i = \\lambda_i^* v_i $$\n",
    "\n",
    "If we look at these equalities, the two left hand sides are the same. Recall that we assumed that A is Hermitian.\n",
    "\n",
    "$$ \\implies \\lambda_i = \\lambda_i^* $$\n",
    "\n",
    "This implies that $\\lambda_i$ must be real. All eigenvalues $\\lambda_i$. How do we prove the Hermitian matrix A mus tbe nonnegative?\n",
    "\n",
    "$$ v_i^H A v_i = \\lambda_i v_i^H v_i $$\n",
    "\n",
    "We just proved the quantity here is nonnegative. Let's take a simple example:\n",
    "\n",
    "$$ v_i = \\left[\\begin{matrix} v_0 \\\\ v_i \\end{matrix}\\right], \\text{   } v_i^H = \\left[\\begin{matrix} v_0^* v_1^* \\end{matrix}\\right] $$\n",
    "\n",
    "$$ v_i^Hv_i = |v_o|^2 + |v_1|^2 $$\n",
    "\n",
    "$$ \\implies \\lambda_i \\geq 0 $$\n",
    "\n",
    "Let's start with a deterministic signal $x(n)$. We know:\n",
    "\n",
    "$$ \\sum_n |x(n)|^2 = \\dfrac{1}{2\\pi}\\int |X(\\omega)|^2 d\\omega $$\n",
    "\n",
    "This says that the toal energy in the time domain is the energy spectral density. This will be generally be written in terms of $\\Delta \\omega$, or over a certain frequency band. What exactly is this quantity then?\n",
    "\n",
    "$$|X(\\omega)|^2 = \\sum_n x(n) e^{-j \\omega n} \\sum_m x^*(m) e^{j \\omega m} $$\n",
    "$$  = \\sum_n \\sum_m x(n) x^*(m) e^{-j\\omega(n-m)} $$\n",
    "\n",
    "If we replace the index:\n",
    "\n",
    "$$ n-m = l $$\n",
    "$$ m = n - l $$\n",
    "\n",
    "Then we can rewrite the sum:\n",
    "\n",
    "$$ \\sum_l \\sum_n x(n) x^*(n-l) e^{-j\\omega l} $$\n",
    "\n",
    "This is now the discrete foruier transform. For a deterministic system. Now we look at a random process. Look at the power spectral density.\n",
    "\n",
    "$$ S_x(\\omega) = DTFT\\left[ r(k) \\right] $$\n",
    "$$ DTFT\\left[ E\\left[ x(n)x^*(n-k) \\right] \\right] $$\n",
    "\n",
    "if $r(k)$ is real:\n",
    "\n",
    "$$ r(-k) \\leftrightarrow S(\\dfrac{1}{z}) $$\n",
    "$$ z\\left[ r(-k) \\right] = \\sum_k r(-K)z^{-k} $$\n",
    "\n",
    "$$ r^*(-k) \\leftrightarrow S^*\\left( \\dfrac{1}{z^*} \\right) $$\n",
    "\n",
    "What does replacing z by 1/z conjugate, what does that mean? In general, if $z = re^{j\\omega}$, $z^* = r^* e^{-j\\omega}$ and $\\dfrac{1}{z^*} = \\dfrac{1}{r^*}e^{j\\omega}$. Therefore $S(z) = S^*(\\dfrac{1}{z^*})$ (I think???).\n",
    "\n",
    "Onto something different\n",
    "\n",
    "$$ r(k) = IDTFT\\left[ S(\\omega) \\right] $$\n",
    "$$ = Z^-1\\left[ S(z) \\right] $$\n",
    "\n",
    "By definition of IDTFT:\n",
    "\n",
    "$$ \\dfrac{1}{2\\pi}\\int_{-\\pi}^\\pi S(\\omega) e^{j\\omega k} d\\omega $$\n",
    "\n",
    "By another definition.\n",
    "\n",
    "$$ S_x(\\omega) = \\lim_{N \\to \\infty} E \\left[ \\dfrac{1}{N}|\\sum_{n=0}^{N-1} x(n)e^{-j\\omega n}|^2  \\right] $$\n",
    "\n",
    "Though not obvous, the expected value of this is the same as the Fourier transform of the correlation matrix under certain conditions. How do we prove this?\n",
    "\n",
    "$$ E \\left[ \\sum_{n=1}^{N-1} x(n) e^{-j \\omega n} \\sum_{m=0}^{N-1} x^*(m) e^{j\\omega m} \\right] $$\n",
    "\n",
    "$$ = \\sum_n \\sum_m E\\left[ x(n) x^*(m) e^{j\\omega(n-m)} \\right] $$\n",
    "\n",
    "We will recognize that this is a double sum. recall the proof we did last lecture. Also notice that we have the autocorrelation function $r(n-m)$:\n",
    "\n",
    "$$ f(n-m) = r(n-m)e^{0j\\omega(n-m)} $$\n",
    "$$ f(n) = r(n)e^{-j \\omega n} $$\n",
    "\n",
    "Therefore, we ca nrewrite the equations as:\n",
    "\n",
    "$$ N \\sum_{l=-(N-1)}^{N-1} \\left( 1 - \\dfrac{|l|}{N} \\right) f(l) r(l) e^{-j\\omega l} $$\n",
    "\n",
    "$$ S_x(\\omega) = \\lim_{N\\to \\infty} \\sum_{l=-(N-1)}^{N-1} \\left( 1 - \\dfrac{|l|}{N} \\right) r(l) e^{-j\\omega l}  $$\n",
    "\n",
    "The $\\dfrac{1}{N} \\to 0$ due to the limit:\n",
    "\n",
    "$$ = \\sum ???$$\n",
    "\n",
    "## Filtering of a random process\n",
    "\n",
    "We are intereested in computing the autocorrelation and power spectral density of the system with impulse response $h(n)$, input $x(n)$ and output $y(n)$. Start with cross correlation of:\n",
    "\n",
    "$$ r_{yx}(k) = E \\left[ y(n) x^*(n-k) \\right] $$\n",
    "$$ E\\left[ \\sum_l h(l) x(n-l) x^*(n-k) \\right] $$\n",
    "$$ = \\sum_l h(l)  E\\left[ x(n-l) x^*(n-k) \\right]  $$\n",
    "$$ = \\sum_l h(l) r_x(k-l)$$\n",
    "\n",
    "$$ = h(k) \\ast r_x(k) $$\n",
    "\n",
    "The cross power spectral density:\n",
    "\n",
    "$$S_{xy}(\\omega) H(\\omega)S_x(\\omega) $$\n",
    "\n",
    "What will be $r_{xy}(k)$\n",
    "\n",
    "$$ r_{xy}(k) = h^*(-k)\\ast r_x(k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 1-23-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The autocorrelation of y:\n",
    "\n",
    "$$ r_y(k) = h^*(-k) \\ast r_{yx}(k) $$\n",
    "\n",
    "Recall the definition of $y(n)$:\n",
    "$$ y(n) = \\sum_l h(l)x(n-l) $$\n",
    "\n",
    "Is it possible to use this to find the power spectral density of $y$ from $x$ and $h$?\n",
    "\n",
    "$$ r_{yx}(k) = E\\left[ y(n) x^*(n-k) \\right] $$\n",
    "\n",
    "We substitute the sum from the definition of $y$:\n",
    "\n",
    "$$ E\\left[ \\sum_l h^*(l) x(n-l) x^*(n-k) \\right] $$\n",
    "\n",
    "Recall, h is not a random process; we don't need to take the expected value.\n",
    "\n",
    "$$  = \\sum_l h(l) E\\left[ x(n-l)x^*(n-k) \\right] $$\n",
    "\n",
    "$$= \\sum_lh(l) r_x(k-l)$$ \n",
    "$$ = h^*(k) \\ast r_x(k) $$\n",
    "\n",
    "Take the Fourier transform of both sides:\n",
    "\n",
    "$$ S_{yx}(e^{j\\omega}) = H(e^{j\\omega}) S_x(e^{j\\omega}) $$\n",
    "\n",
    "Take the z transform and it will be:\n",
    "\n",
    "$$ S_{yx}(z) = H(z) S_x(z) $$\n",
    "\n",
    "Let's try to substitute. Recall our prevous derivation for the power wpectral density of $x$. Now we can say:\n",
    "\n",
    "$$ r_y(k) = h^*(-k)\\ast h(k) \\ast r_x(k) $$\n",
    "\n",
    "sometimes we can think of $h^*(=k) \\ast h(k) = r_h(k)$. This is the autocorrelation of the filter itself.\n",
    "\n",
    "$$ S_y(z) = H^*\\left(\\dfrac{1}{z^*}\\right) $$\n",
    "\n",
    "Take an instance of this: $z = 3+4j$, thus $z^* = 3-4j$, and $\\dfrac{1}{z^*} = \\dfrac{1}{3-4j} = \\dfrac{3+4j}{25}$. However, on the unit circle: $z = e^{j\\omega}$, and $\\dfrac{1}{z^*} = \\dfrac{1}{e^{j\\omega}} = e^{j\\omega} \\implies H(e^{j\\omega)} = H(e^{-j\\omega}) = H\\left(\\dfrac{1}{z}\\right) $. \n",
    "\n",
    "$$ H(e^{j\\omega}) = \\sum h(n) e^{-j\\omega n} $$\n",
    "\n",
    "We make an assumption that the signal is real. $h(n) = h^*(n)$.\n",
    "\n",
    "$$ H^*(e^{j\\omega}) = \\sum h^*(n) e^{j\\omega n} $$\n",
    "= $$ \\sum h(n) e^{j\\omega n} $$\n",
    "\n",
    "Let's go back:\n",
    "\n",
    "$$ S_y(z) = H^*(\\dfrac{1}{z^*}) H(z) S_x(z) $$\n",
    "\n",
    "We notive that the pair of $H$'s are complex conjugate pairs. Additionally, $S_y$ can be described in this light. \n",
    "\n",
    "$$ S_y(e^{j\\omega}) = |H(e^{j\\omega})|^2 S_x(e^{j\\omega}) $$\n",
    "\n",
    "The term $S_{yx}$ is known as the Weiner-hart equation. Now we spend some time on $r_{xy}$. What exactly is it?\n",
    "\n",
    "$$ r_{xy}(k) = E\\left[ x(n)x^*(n-k) \\right] $$\n",
    "\n",
    "The difference between the first and dsecond terms is the conjugate and the lag term.\n",
    "\n",
    "$$ = E\\left[ x(n) \\sum_l h^*(l) x^*(n-k-l) \\right] $$\n",
    "$$ = \\sum_l h^*(l) r_x(k-l) $$\n",
    "$$ = h^*(-k) \\ast r_x(K) $$\n",
    "\n",
    "How is $r_{xy}$ related to $r_{yx}$?\n",
    "\n",
    "$$ r_yx(k) = h^*(k)\\ast r_x^*(k) $$\n",
    "$$ =h^*(k) \\ast r_x(-k) $$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ r_{xy} = r_{yx}^*(-k) $$\n",
    "\n",
    "What does this mean in terms of the $z$ transform?\n",
    "\n",
    "$$ S_{xy} (z) = S_{yx}(\\dfrac{1}{z^*}) $$\n",
    "\n",
    "For real $x$, and $h$:\n",
    "\n",
    "S_{xy}(e^{j\\omega}) = S_{yx}(e^{-j\\omega})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Onto modeling\n",
    "\n",
    "We can measure something using a sensor. Why use autoregressive models? Let's look at the model prediction $h(n)$. What is the filter that takes a noisy input $u$ and is able to flatten the power spectrum (called a whitening filter)?\n",
    "\n",
    "$$ S_u(e^{j\\omega}) = \\sigma_v^2 |H(e^{j\\omega})|^2 $$\n",
    "$$ S_{\\hat{v}}(e^{j\\omega}) = S_u(e^{j\\omega})|H_w|^2 $$\n",
    "$$ H_w(z) = \\dfrac{1}{H(z)} $$\n",
    "\n",
    "We may not get back the same white noise, but pretty close due to real world noise. $H_w$ must be a FIR filter if H is all-pole.\n",
    "\n",
    "Let's look at an AR moel:\n",
    "\n",
    "$$ 1 + az^{-1} + bz^{-2} $$\n",
    "$$ \\implies \\hat{v}(n) = u(n) + au(n-1) + bu(n-2) $$\n",
    "\n",
    "\n",
    "What he have is:\n",
    "\n",
    "$$ \\hat{v}(n)= u(n) - \\left[ -au(n-1) -b(n-2) \\right] $$\n",
    "\n",
    "We want to look for a predictor. the error $\\hat{v}$ is equal to the difference between $u(n)$ and the estimate of $u$ given by $\\hat{u}(n) = \\left[ -au(n-1) -b(n-2) \\right]$. We must predict current samples from past samples:\n",
    "\n",
    "$$ H_w = 1-Pz^{-1} $$\n",
    "\n",
    "Solving for P:\n",
    "\n",
    "$$ P(z) = \\left( 1 - H_w(z) \\right)z $$\n",
    "\n",
    "We are talking about a number of predictions. 2nd order autoregressive model predictions:\n",
    "\n",
    "$$ v(n) --> \\dfrac{1}{1+ a_1z^{-1} + a_2z^{-2}} --> u(n) $$\n",
    "\n",
    "Given $u(n)$ find $a_{1,2}$. Need to find the yule-walker equation.\n",
    "\n",
    "$$ \\dfrac{v(z)}{1 + a_1 z^{-1} + a_2 z^{-2}} = U(z) $$\n",
    "$$ v(z) = U(z) + a_1 z^{-1}u(z) + a_2 z^{-2} u(z) $$\n",
    "$$ v(n) = u(n) a_1u(n-1) a_2u(n-2) $$\n",
    "\n",
    "Let's try to multiply:\n",
    "\n",
    "$$ E\\left[ v(n)u^*(n-k) \\right] = E\\left[ u(n)u^*(n-k) \\right] + a_1E\\left[ u(n-1)u(n-k) \\right] + a_2E\\left[ u(n-2)u(n-k) \\right] $$\n",
    "\n",
    "$$ =  r(u) + a_1 r(k-1) a_2r(k-2) $$\n",
    "\n",
    "What do you think the expected value is? We canthink of the IIR filter as an infinitely long FIR filer. We can rewrite $u^*(n-k)$ as:\n",
    "\n",
    "$$ v^*(n-k) - a_1v^*(n-k-1) + (a_1-a_2)v^*(n-z) $$\n",
    "\n",
    "If we take the expevted value of $v$, $v$ is a zero mean wide stationary process with a variance of $\\sigma$ (maye squared).\n",
    "\n",
    "$$ E\\left( |v(n)|^2 \\right) = \\sigma_v^2\\delta(k) $$\n",
    "\n",
    "When $k$ is zero, it is the magnitude squared of the variance. For any other k, they are independent, and thus 0. Thus, for $k=0,1,2$,\n",
    "\n",
    "* k=0, $ \\sigma_v^2 = r(0) + a_1 r(-1) a_2r(-2) $\n",
    "* k=1, $r(1) = -a_1r(0) - a_2r(-1)$\n",
    "* k=2, $r(2) = -a_1r(1) - a_2r(0)$\n",
    "\n",
    "Recall that the expected value is zero for $k \\neq 0$. We rewrite the last two equations in to matrices:\n",
    "\n",
    "$$ \\left[\\begin{matrix} r(0)& r(-1) \\\\ r(1) & r(0) \\end{matrix}\\right] \\left[\\begin{matrix} -a_1 \\\\ -a_2 \\end{matrix}\\right] = \\left[\\begin{matrix} r(1) \\\\ r(2) \\end{matrix}\\right] $$\n",
    "\n",
    "Rewriting this by representing the matrices in terms of variables:\n",
    "\n",
    "$$ \\overrightarrow{R} \\overrightarrow{w} = \\overrightarrow{r} $$\n",
    "\n",
    "This is the Yule-walker equation; it simply states that we can calculate the unknown coefficients as:\n",
    "\n",
    "$$ \\overrightarrow{w} = \\overrightarrow{R}^-1 \\overrightarrow{r} $$\n",
    "$$ \\overrightarrow{a} = -\\overrightarrow{w} $$\n",
    "\n",
    "Once we know these, we can go back and find $\\sigma_v^2$.\n",
    "\n",
    "Given $H(z)$ can we find r(k)?\n",
    "\n",
    "$$ S_y(z) = \\sigma_v^2 H(z) H^*(\\dfrac{1}{z^*}) $$\n",
    "$$ r(k) = Z^-1\\left[ S_y(z) \\right] $$\n",
    "\n",
    "For a general order AR model:\n",
    "\n",
    "$$ \\left[\\begin{matrix} r(0) & r(-1) & ... & r(-M+1)\\\\r(1) & r(0) & ... & \\\\ \\vdots & & & \\\\ r(M-1) & ... & ... & r(0) \\end{matrix}\\right] \\left[\\begin{matrix} -a_1 \\\\ \\vdots \\\\-a_m \\end{matrix}\\right] = \\left[\\begin{matrix} r(1) \\\\ \\vdots \\\\ v(m) \\end{matrix}\\right] $$\n",
    "\n",
    "Solve for $-\\overrightarrow{a}$ thenk use the $k=0$ equation to find $\\sigma_v^2$.\n",
    "\n",
    "$$ \\sigma_v^2 = r(0) + \\sum_{k=1}^M a_k r(-k) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 11-25-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's point out a few things. Last class, if you take a signal, and whiten the output to get $\\hat{v}$. We made a whitening filer, or a prediction error filter. Design a predictor filter, find the a and b coefficients which make up $P(z)$. To design an error prediction filter, find $H_w = 1-Pz^-1$.\n",
    "\n",
    "** Correlogram:**\n",
    "How to we design this? Compute power spectral density? How many ways? We could estimate the correlation coefficeints from the given signal, then take the FFT. One approach, the so called estimate the correlation sequence, the **correlogram**. Estimate $\\hat{r}(k), k=-M...M$.\n",
    "\n",
    "$$ S(e^{j\\omega}) = \\sum_{k=-M}^M \\hat{r}e^{-j\\omega k} $$\n",
    "\n",
    "**Periodogram**\n",
    "We compute the dtft and compute the power spectral density. Another way is simply the magnitude squared of the spectrum of the sequence divided by N. This is called a **periodogram**. Take the signal y(n):\n",
    "\n",
    "$$ \\lim_{N\\to \\infty} |\\sum_{n=0}^{N-1} y(n)e^{-j\\omega n}|^2 $$\n",
    "\n",
    "**Modelling**\n",
    "This is pretty good! What else can you do? We can also do something based on the AR model. If $H$ is an all-pole filter, then v(n) --> H --> y(n), Yule walker equations:\n",
    "\n",
    "$$ Rw = r $$\n",
    "\n",
    "solve for $w$ for the predictors, then solve for H:\n",
    "\n",
    "$$ H = \\dfrac{1}{1-w_1z^{-1} - w_2z^{-2}} $$\n",
    "\n",
    "$$ S_y\\left( e^{j\\omega} \\right) = \\sigma_v^2 |H|^2 $$\n",
    "\n",
    "Let's examine the welch power spectral density. The basic idea of welch spectral density is this. If we are interested in computing some form of spectrum magnitude squared. The idea of Welch PSD is to divide this into segments of length M. Just like the overlap of the windows, the next signal begins just behind the end of the first window. We are left with a series of windows with some overlap. We then take the periodogram of each segment. \n",
    "\n",
    "Let $s$ be the number of segments.  $N = (s-1)k + M$. M is the length of each window. Typically, we multiply each window by some windowing function to reduce spectral leackage. \n",
    "\n",
    "$$ S_i\\left( e^{j\\omega} \\right) = \\dfrac{1}{PM}\\sum_{i=0}^{M-1} v(n) y_i(n)e^{-j\\omega m} $$\n",
    "\n",
    "Take the signal subsection. Here, $v(n)$ is the windowing function and $P$ is the total power of the window given by:\n",
    "\n",
    "$$ P = \\sum_{n=0}^{M-1} |v(n)|^2 $$\n",
    "\n",
    "Solving for the power spectral density:\n",
    "\n",
    "$$ S_y\\left( e^{j\\omega} \\right) \\dfrac{1}{s}\\sum_{i=0}^{s-1} S_i\\left( e^{j\\omega} \\right) $$\n",
    "\n",
    "The typical values in matlab are to use a Hamming window and a 50\\% overlap. The welch is used rather frequently. It adds excellent spectral representation. Lots of distorions are removed by the overlap. Leakage is taken care of by the windows. The averaging gives a better estimation of the error. The spectrum will eventually be a convolved spectrum of the frequency representations of the window function and the function.\n",
    "\n",
    "there are a lot of ways of calculating the PSD.\n",
    "\n",
    "## What is cross power spectral density?\n",
    "\n",
    "What is the simplist way to compute cross power spectral density and cross power spectrum density. We can estimate the cross correlation. We can compute the cross spectral power density through the estimate $r_{yx}$. The cross correlation for a certain number of lags. Then we can compute:\n",
    "\n",
    "$$ \\hat{S}_{yx}\\left( e^{} \\right) = \\sum_{k=-M}^M w(k) \\hat{r}_{yx} (k)e^{-j \\omega k} $$\n",
    "\n",
    "(I'm pretty sure here that the window function changed to $w(k)$ in this equation...)\n",
    "\n",
    "We can estimate the filter in a number of ways. One way is by the Yule Walker equation. Another way is tu use the wiener Hoff equation assuming we can build a model.\n",
    "\n",
    "One other term which we can talk about: **Coherency Spectrum**. What does this mean? We describe their correlation by $r_{yx}$, which describes the correlation between $x$ and $y$. It may be that we care about the correlation between their spectral densities. Just because some signals are correlated in the time domain does not mean that their spectra are correlated in the frequency domain.\n",
    "\n",
    "$$ C_{yx}\\left( e^{j\\omega} \\right) = \\dfrac{S_{yx}\\left( e^{j\\omega} \\right)}{\\sqrt{S_y\\left( e^{j\\omega} \\right)S_x\\left( e^{j\\omega} \\right)}} $$\n",
    "\n",
    "$$ |C_{yx}| \\leq 1 $$\n",
    "\n",
    "It is exactly equal to 1 when the signal y is exactly predictable from signal x. Let's think about the system with an error calculation $e = \\hat{y} - y$. We already know that:\n",
    "\n",
    "$$ H\\left( e^{j\\omega} \\right) = \\dfrac{S_{yx}\\left( e^{j\\omega} \\right)}{S_x\\left( e^{j\\omega} \\right)} $$\n",
    "\n",
    "We can find the expected value of the squared error. Try in terms of the squared spectrum. The coherency spectrum can play a major role as a feature ranking role or a feature itself.\n",
    "\n",
    "$$ e = y-x\\ast h $$\n",
    "\n",
    "What is the expected value?\n",
    "\n",
    "$$ E\\left[ e^2(n) \\right] = r_e(0)  $$\n",
    "\n",
    "this is true in general for the magnitude squared of the error. Continuing:\n",
    "\n",
    "$$ E\\left[ e^2(n) \\right]  = \\dfrac{1}{2\\pi} \\int S_e(e^{j\\omega})e^{j\\omega k} d\\omega $$\n",
    "\n",
    "Here,$k=0$, therefore $e^{j\\omega k}= 1$ so we ignore it. $S_e$ then is the spectrum of the error which can be written as:\n",
    "\n",
    "$$ S_e = S_y - S_{\\hat{y}}$$\n",
    "$$ = S_y - S_x H \\tilde{H} $$\n",
    "$$ = S_y - S_{yx}\\tilde{H} $$\n",
    "$$ = S_y - \\dfrac{S_{yx}\\tilde{S}_{xy}}{\\tilde{S}_x} $$\n",
    "\n",
    "Going back to the derivation:\n",
    "$$ E\\left[ e^2(n) \\right]  = \\dfrac{1}{2\\pi} \\int_{-\\pi}^\\pi S_y(e^{j\\omega}) \\left( 1-\\dfrac{|S_{yx}|^2}{S_y \\tilde{S}_x} \\right) d\\omega $$\n",
    "\n",
    "Take the magnitude squared of the coherence spectrum, this can be written as:\n",
    "\n",
    "$$ E\\left[ e^2(n) \\right]  = \\dfrac{1}{2\\pi} \\int_{-\\pi}^\\pi S_y(e^{j\\omega}) \\left( 1- |C_{yx}|^2 \\right) d\\omega $$\n",
    "\n",
    "We know that the power spectral density is always $\\geq 0$. Since $E\\left[ |e(n)|^2 \\right] \\geq 0$, we therefore know that $1-C_{yx} \\geq 0$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Prediction vs Prediction Error Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The idea of predicting a signal $\\hat{y}$ from a signal $x$.\n",
    "\n",
    "In DPCM (differential pulse code modulation, transmitting the signal across a phone (for example), it is distorted and obtains noise. It is much less costly to transmit the error between a prediction and the signal than the actual signal itself; therefore, we will be transmitting the error and reconstructing the signal. Say we have a signal $y$, and subtract it from an estimate $\\hat{y}$, then transmit the error across the distorted channel.he transceiver receives a signal $\\tilde{e}$ and tries to reconstruct the original signal $\\tilde{y}$.\n",
    "\n",
    "Here's the transfer function of the system.\n",
    "\n",
    "<img src='IMG_0673.JPG'>\n",
    "\n",
    "$$ e = y - \\hat{y} $$\n",
    "$$ \\implies y = \\hat{y} + e $$\n",
    "\n",
    "We are transmitting $e$, the receiver will try to reverse the process. The difference in error:\n",
    "\n",
    "$$ e - \\tilde{e} = \\left( y - \\hat{y} \\right) - \\left( \\tilde{y} - \\hat{y} \\right) $$\n",
    "$$  = y - \\tilde{y} $$\n",
    "\n",
    "This should show the difference between $y$ and $\\tilde{y}$ is the exact same error as the difference in $e$ and $\\tilde{e}$. How can we estimate $\\hat{y}$? The receiver must undo what is done at the transmitter. If we estimate $\\hat{y}$ from $y$.\n",
    "\n",
    "Let's look at the transfer function of the receiver.\n",
    "\n",
    "$$ \\dfrac{1}{1-P} $$\n",
    "\n",
    "Where $P$ is the loop gain. The transmitter is:\n",
    "\n",
    "$$ \\dfrac{1}{1 + \\dfrac{P}{1-P}} = 1-P $$\n",
    "\n",
    "This is the exact inverse of the receiver, which is precisely what is needed to undo the prediction filter. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1-30-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 2-1-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "HW3 - proof. Prove that eigenvalues ofa ny Hermitian matri are nonnegative. \n",
    "\n",
    "$$ S_x(z) = \\sigma^2 H(z)H^*(\\dfrac{1}{z^*}) $$\n",
    "\n",
    "The poles should be in receiprocal pairs. \n",
    "\n",
    "Problem 6 - \n",
    "\n",
    "Express cosines by the Euler properties. Factorize the equation, tells us what z is, from the random process $v(n)$. Once we know G, we can desin the whitening filer $H$. \n",
    "\n",
    "Problem 7 -\n",
    "\n",
    "Do the same thing, only using matlab. \n",
    "\n",
    "Problem 3-\n",
    "\n",
    "Filter is unstable (look into this)\n",
    "\n",
    "Problem 6-\n",
    "This i pretty intense. Cery clearly, we design a machine learning system, whant to know the machine learning classifier. Look at PSD \n",
    "\n",
    "Example of signal not correlated in time but highly correlated in freq.\n",
    "\n",
    "$$ x_1(n) $$\n",
    "$$ x_2(n) = x_1(n - \\Delta t) \\leftrightarrow |X_2(\\omega)| = |X_1(\\omega)e^{j\\omega \\Delta t} $$\n",
    "\n",
    "Note , this is only for one parameter; if we do the cross-correlation across all lags, we will see one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Chapter 2 - Weiner Filtering\n",
    "\n",
    "Finding the optimal filter. We will assume a complex signal. $u(n)$ will be filtered by $w_i = a_i + jb_i$; the output $y(n)$.\n",
    "\n",
    "If we have a $D$ tap filter:\n",
    "\n",
    "$$ y(n) = \\langle \\overrightarrow{u}, \\overrightarrow{w} \\rangle = \\sum_{i=0}^{M-1} w^*_i u(n-i) $$\n",
    "\n",
    "This is simply defining the inner prodcut of the two signals $u$ and $w$. The predicted value $d(n)$. We want to find the optimal coefficients such that min $J = E\\left[ |e(n)|^2 \\right]$. We are minimizing the cost function $J$. IN this case, it is the mean squared error.\n",
    "\n",
    "$$ e(n) = d(n) - \\sum_{i=0}^{M-1} w_i^* u(n-i) $$\n",
    "\n",
    "We must take the derivative of J w.r.t. to the complex quantities and set ti to zero. How to define a gradient of $i$?\n",
    "\n",
    "$$ \\nabla_k =\\dfrac{\\partial }{\\partial a_k} + j\\dfrac{\\partial }{\\partial b_k} $$\n",
    "\n",
    "This is the formal definition of the gradient of a complex quantity.\n",
    "\n",
    "$$ \\nabla_k J = \\dfrac{\\partial J}{\\partial a_k} + j \\dfrac{\\partial J}{\\partial b_k} = 0 $$\n",
    "\n",
    "If we solve this equation, we find the solution to the weiner filter. The expected value of \n",
    "\n",
    "$$ J = E\\left[ |e|^2 \\right] = E\\left[ ee^* \\right] $$\n",
    "$$ \\implies \\nabla_k J = \\dfrac{\\partial }{\\partial a_k} E\\left[ ee^* \\right] + j \\dfrac{\\partial }{\\partial b_k} E\\left[ ee^* \\right] $$\n",
    "$$ = E \\left[ \\dfrac{\\partial }{\\partial a_k} (ee^*)  \\right] + jE\\left[ \\dfrac{\\partial }{\\partial b_k} (ee^*)  \\right] $$\n",
    "$$ = E\\left[ \\dfrac{\\partial e}{\\partial a_k} e^* + e\\dfrac{\\partial }{\\partial a_k}e^* \\right] + jE\\left[ \\dfrac{\\partial e}{\\partial b_k} e^*  + e\\dfrac{\\partial e^*}{\\partial b_k} \\right] $$\n",
    "\n",
    "What exactly is $e$?\n",
    "\n",
    "$$e = d-y = d- \\sum_{k=0}^{M-1} a_k - jb_k)u(n-k)$$\n",
    "\n",
    "Let's try to look at what the partial derivatives are. \n",
    "\n",
    "$$ \\dfrac{\\partial e}{\\partial a_k} = -u(n-k) $$\n",
    "\n",
    "Be sure to take the conjugate of $e$! $e^* = d^*- \\sum_{k=0}^{M-1} a_k + jb_k)u^*(n-k)$\n",
    "\n",
    "$$ \\dfrac{\\partial e^*}{\\partial a_k} = -u^*(n-k) $$\n",
    "\n",
    "$$ \\dfrac{\\partial e}{\\partial b_k} = ju(n-k) $$\n",
    "$$ \\dfrac{\\partial e^*}{\\partial b_k} = -ju^*(n-k) $$\n",
    "\n",
    "Great! now we use these to write the gradient of $J$.\n",
    "\n",
    "$$ \\nabla_k J = E\\left[ -u(n-k) e^*(n) - e(n) u^*(n-k) - u(n-k)e^*(n) + e(n)u^*(n-k) \\right] = 0 $$\n",
    "$$ = -2E\\left[ u(n-k) e^*(n) \\right] = 0 $$\n",
    "\n",
    "For optimal prediction:\n",
    "\n",
    "$$ E\\left[ u(n-k) e^*_o(n) \\right] = 0 $$\n",
    "\n",
    "if it so happens that we solve these equations and compute the minimum mean squared error, when these weight parameters. This implies that $ e_0(n) $ is orthogonal to $u(n-k) \\forall k$. The error is orthogonal to observations.\n",
    "\n",
    "$$ E\\left[ y(n) e^*(n) \\right] $$\n",
    "$$ = E\\left[ \\sum_{k=0}^{M-1} w_k^* u(n-k) e^*(n) \\right] $$\n",
    "$$ = \\sum_{k=0}^{M-1} w_k^* E\\left[ u(n-k) e^*(n) \\right] $$\n",
    "\n",
    "When the weights are optimal. Now the expected value of $y_o(n)$ which is the best estimate of $d$. Thus $E\\left[ y_o(n) e_oe^*(n) \\right] = 0$. The error is orthogonal to 0. Think about the span of the vectors. (???) $d = e_o + y_o = e_o + \\hat{d}$. This impies that $e_o = d - \\hat{d}$\n",
    "\n",
    "$$ J_{min} = E\\left[ |e_o(n)|^2 \\right] $$\n",
    "$$ = E\\left[ \\left( d(n) - \\hat{d}(n) \\right)e_o^*(n) \\right] $$\n",
    "$$ E\\left[ d(n) e_o^*(n)  \\right] - E\\left[ \\hat{d}(n) e_o^*(n) \\right] $$\n",
    "$$ = E\\left[ d(n) \\left( d^*(n) - \\hat{d}(n) \\right) \\right] $$\n",
    "$$ = E\\left[ |d(n)|^2 \\right] - E\\left[ \\left(e_o(n) + \\hat{d}(n)\\right) d^*(n) \\right] $$\n",
    "\n",
    "Recall that $d$ is orthogonal to \n",
    "\n",
    "$$ E\\left[ |d(n)|^2 \\right] - E\\left[ |\\hat{d}(n)|^2 \\right] $$\n",
    "$$ = \\sigma_d^2 - \\sigma_{\\hat{d}}^2 $$\n",
    "\n",
    "Normalized MSE:  $\\varepsilon$\n",
    "\n",
    "$$ \\varepsilon = \\dfrac{J_{min}}{\\sigma_d^2} = \\dfrac{\\sigma_d^2}{\\sigma_d^2}  - \\dfrac{\\sigma_\\hat{d}^2}{\\sigma_d^2} = 1-\\dfrac{\\sigma_\\hat{d}^2}{\\sigma_d^2} $$\n",
    "\n",
    "Recall taht $d = e_o \\hat{d}$. We can therefore know. We need to compute the autocorrelation $r$, which descitbes the statistical properties of te weights. How do we compute these optimal weights? To compute, we come back to our equation here. We can use the orthogonal property. The optimal mean squared error.\n",
    "\n",
    "$$ E\\left[ u(n-k) \\left( d(n) - \\sum_{l=0}^{M-1} w_{l0}^* u^*(n-l) \\right) \\right] = 0 $$\n",
    "\n",
    "This is the equation to compute the optimal filter.\n",
    "\n",
    "$$ E\\left[ u(n-k) d(n) \\right] = \\sum_{l=0}^{M-1} w_{l0}^* E\\left[ u(n-k) u^*(n-l) \\right] $$\n",
    "\n",
    "Recall $E\\left[ u(n-k) u^*(n-l) \\right] = r(l-k)$.\n",
    "\n",
    "$$ \\sum_{l=0}^{M-1} w_{l0}r(l-k) = p(-k) $$\n",
    "\n",
    "$P$ is the cross-correlation of $u,d$.\n",
    "\n",
    "$$ \\left[\\begin{matrix} r(0) & r(1) & ... & r(M-1)\\\\ r^*(-1) & r(0) & ... & \\vdots \\\\ \\vdots & vdots &  & vdots \\end{matrix}\\right]\\left[\\begin{matrix} w_{0,0} \\\\ w_{1,0} \\\\ \\vdots \\\\ w_{M-1,0} \\end{matrix}\\right] \\left[\\begin{matrix} P(0) \\\\ P(-1) \\\\ \\vdots \\\\ P(M-1) \\end{matrix}\\right] $$\n",
    "\n",
    "Each row corresponds to a different lag $k=0,1,...$. How do we compoute optimal filter, weiner filter? Wiener-Hopf equation:\n",
    "\n",
    "$$ Rw_o = P $$\n",
    "$$ \\implies w_o = R^{-1} P $$\n",
    "\n",
    "Here $R$ is $mxm$, $w_0$ is $mx1$ and $P$ is $mx1$. \n",
    "\n",
    "If we think about the process, we don't know the order of the process or of the filter $M$. How do we estimate the order of the filter? Start with a large order, compute eigenvalues of correlation matrix, take square of eigenvalues and add them up (think of the energy ofthe process). If we are interested in a low rank process, but still want to capture 95% of energy. How many eigenvalues should we keep? Say, for example, 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We go back to compute the minimum min squared error. This is, again $\\sigma_d^2 - \\sigma_\\hat{d}^2$. \n",
    "\n",
    "$$ y_o(n) = \\hat{d}_o(n) = \\sum_l w_{l0}^* u(n-l) $$\n",
    "$$ = w_o^H u(n) $$\n",
    "\n",
    "Now, we can say:\n",
    "\n",
    "$$ \\hat{d}_o(n) = w_o^H u $$\n",
    "\n",
    "$$ \\sigma_{\\hat{d}}^2 = E\\left[ \\hat{d}_o(n) \\hat{d}_o^*(n) \\right] $$\n",
    "$$ = E\\left[ w_o^H u h^H w_o \\right] $$\n",
    "$$ = w_o^H R w_o $$\n",
    "$$ J_{min} = \\sigma_d^2 - w_o^H R w_o $$\n",
    "\n",
    "We can write $J_{min}$ in many different forms.\n",
    "\n",
    "$$ R w_o = P $$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ J_{min} = \\sigma_d^2 - w_o^H P $$\n",
    "\n",
    "Take the hermintion of both sides: $w_o^H R = P^H$\n",
    "\n",
    "$$ = \\sigma_d^2 - P^H w_o $$\n",
    "\n",
    "These three expressions are the same:\n",
    "$$ J_{min} = \\sigma_d^2 - w_o^H R w_o $$\n",
    "$$ J_{min} = \\sigma_d^2 - w_o^H P $$\n",
    "$$ J_{min} = \\sigma_d^2 - P^H w_o $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 2-5-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We want to spend a little more time talking about the error. Let's start with an example:\n",
    "\n",
    "We want to see how we can use the Weiner filter to decrease the SNR of a signal. We have asignal $d(n)$ and we know $r_d(k) = \\alpha^{|k|}$. The desired signal has been corrupted by noise.\n",
    "\n",
    "$$ d(n) + v(n) = u(n) $$\n",
    "\n",
    "Here $v(n)$ is white, gaussian noise which is not correlated to $d(n)$. What wen're interested is to reduce the effect of noise. We'll design a 2 tap Wiener filter to make an estimate of the original signal $\\hat{d}_n$.\n",
    "\n",
    "$$ W(z) = w_o  + w_1z^{-1} $$\n",
    "$$ \\hat{d}(n) - d(n) = e(n)  $$\n",
    "\n",
    "Let's say we're given $\\sigma_v^2$. We want to find $w_0, w_1,$ and the SNR with and without filtering. With the design of optimal filtering. Recall the optimal filter can be found through:\n",
    "\n",
    "$$ Rw_o = P $$\n",
    "\n",
    "Where $R$ is the correlation matrix of $u(n)$. We don't know what this is. We know the autocorrelation of $d(n) = r_d(k) = \\alpha^{|k|}$\n",
    "\n",
    "$$ r_u(k) = E\\left[ u(n) u^*(n-k) \\right] $$\n",
    "$$ = E\\left[ \\left( d(n) + v(n) \\right)\\left( d^*(n-k) + u^*(n-k) \\right) \\right] $$\n",
    "$$ = r_d(k) + \\sigma_v^2 \\delta (k) $$\n",
    "\n",
    "What is the $R$ matrix then? **Please note this is a real signal!** We don't deal with the complex conjugate here.\n",
    "\n",
    "$$ R = \\left[\\begin{matrix} r_u(0) & r_u(1) \\\\ r_u(1) & r_u(0) \\end{matrix}\\right] = \\left[\\begin{matrix} 1+\\sigma_v^2 & \\alpha \\\\ \\alpha & 1 + \\sigma_v^2 \\end{matrix}\\right] $$\n",
    "\n",
    "What is $P$? $P$ is the cross-correlation matrix between $u$ and $d$. We defined $p(k)$ in the previous lecture.\n",
    "\n",
    "$$ p(k) = r_ud(k) = E\\left[ u(n) d^*(n-k) \\right] $$\n",
    "$$ = E\\left[ \\left( d(n) + v(n) \\right) d^*(n-k) \\right] $$\n",
    "\n",
    "Because $v(n)$ is uncorrelated, it's expected value is 0. Also recall $E\\left[ u(n-k) d^*(n) \\right] = r_{ud}(-k)$\n",
    "\n",
    "$$ = r_d(k) $$\n",
    "\n",
    "therefore:\n",
    "\n",
    "$$ p(-k) = \\left[\\begin{matrix} 1 \\\\ \\alpha \\end{matrix}\\right] $$\n",
    "\n",
    "Therefore the equation we need to solve is:\n",
    "\n",
    "$$ \\left[\\begin{matrix} 1 + \\sigma_v^2 & \\alpha \\\\ \\alpha & 1 + \\sigma_v^2 \\end{matrix}\\right] \\left[\\begin{matrix} w_o \\\\ w_1 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 \\\\ \\alpha \\end{matrix}\\right] $$\n",
    "\n",
    "Solving the system by taking the inverse of $R$:\n",
    "\n",
    "$$ \\left[\\begin{matrix} w_o \\\\ w_1 \\end{matrix}\\right] = \\dfrac{1}{\\left( 1 + \\sigma_v^2 \\right)^2 - \\sigma^2}\\left[\\begin{matrix} 1+\\sigma^2_v & -\\alpha \\\\ -\\alpha & 1 + \\sigma_v^2 \\end{matrix}\\right]\\left[\\begin{matrix} 1 \\\\ \\alpha \\end{matrix}\\right] $$\n",
    "\n",
    "Solving for the coefficients:\n",
    "\n",
    "$$ w_o = \\dfrac{1 + \\sigma_v^2 - \\alpha^2}{\\left( 1 + \\sigma_v^2 \\right)^2 - \\alpha^2} $$\n",
    "$$ w_1 = \\dfrac{\\alpha \\sigma_v^2}{\\left( 1 + \\sigma_v^2 \\right)^2 - \\alpha^2} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's look at a numerical example:\n",
    "\n",
    "$$ \\alpha=0.8 $$\n",
    "$$ \\sigma_v^2 = 1 $$\n",
    "$$ w_0 = 0.405 $$\n",
    "$$ w_1 = 0.238 $$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ W(z) = 0.405 + 0.238 z^{-1} $$\n",
    "\n",
    "If we take the PSD of $d(n)$:\n",
    "\n",
    "$$ S_d(\\omega) = DTFT\\left[ \\alpha^{|k|} \\right] = \\dfrac{1-\\alpha^2}{1 + \\alpha^2  2\\alpha \\cos(\\omega)} $$\n",
    "$$  = \\dfrac{0.36}{1.64 - 1.6 \\cos \\omega} $$\n",
    "\n",
    "If we plot the power spectral density of $u$:\n",
    "\n",
    "SNR = signal Power / Noise Power. Therefore:\n",
    "\n",
    "$$ SNR = \\dfrac{r_d(0)}{\\sigma_v^2} = \\dfrac{1}{1} = 0 dB $$\n",
    "\n",
    "Now onto finding $J_{min}$.\n",
    "\n",
    "$$ \\sigma_{\\hat{d}}^2 = w_o^H p = \\left[\\begin{matrix} w_o & w_1 \\end{matrix}\\right] \\left[\\begin{matrix} p(0) \\\\ p(-1) \\end{matrix}\\right] $$\n",
    "$$  = \\left[\\begin{matrix} 0.405 & 0.238 \\end{matrix}\\right]\\left[\\begin{matrix} 1 \\\\ \\alpha \\end{matrix}\\right] $$\n",
    " $$ = 0.405 + 0.238\\cdot 0.8 $$\n",
    " \n",
    "What is the noise power? We started with $u(n)$ filtered with $W(z)$ and got the estimaged signal $\\sigma_d$. We can thnk of $\\hat{d} =\\hat{d}_s + \\hat{d}_n $. It contains the power of the signal and the noise. \n",
    "\n",
    "$$ \\sigma_{\\hat{d}_s}^2 = w_0^T R_d w_o $$\n",
    "$$ \\sigma_{\\hat{d}_n}^2 = w_o^T R_u w_o $$\n",
    "\n",
    "Finding these:\n",
    "\n",
    "$$ \\sigma_{\\hat{d}_s}^2 = \\left[\\begin{matrix}  0.405 & 0.238 \\end{matrix}\\right] \\left[\\begin{matrix} \\sigma_v^2 & 0 \\\\ 0 & \\sigma_v^2 \\end{matrix}\\right] \\left[\\begin{matrix} 0.405 \\\\ 0.238 \\end{matrix}\\right] = 0.375$$\n",
    "$$ \\sigma_{\\hat{d}_n}^2 = \\left[\\begin{matrix}  0.405 & 0.238 \\end{matrix}\\right] \\left[\\begin{matrix} 1 & \\alpha \\\\ \\alpha & 1 \\end{matrix}\\right] \\left[\\begin{matrix} 0.405 \\\\ 0.238 \\end{matrix}\\right] = 0.22 $$\n",
    "\n",
    "We've improved the SNR! :) We improved it by: $$ \\hat{d} = 10log\\dfrac{0.375}{0.22} = 2.3 dB $$\n",
    "\n",
    "$$ W(z) = 0.405 + 0.238 z^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Error Performance\n",
    "\n",
    "We want to better describe $J_{min}$\n",
    "\n",
    "$$ e(n) = d(n) - \\sum_{k=0}^{M-1} = w^*_k u(n-k) $$\n",
    "\n",
    "Take an example for the min. What is the equation of the parabola? $y = m(x-a)^2 + b$. This tells us that $y_{min} = b$. Look again at $J$ then:\n",
    "\n",
    "$$ J = E\\left[ |e(n)|^2 \\right] $$\n",
    "\n",
    "Let's try to compute $J$ and try to descibe this in a quadratic form; we will then find the min of this.\n",
    "\n",
    "$$ J = E\\left[ e(n) e^*(n) \\right] $$\n",
    "$$ = E\\left[ \\left( d(n) - \\sum_{k=0}^{M-1} w_k^* u(n-k)\\right)\\left( d^*(n) - \\sum_{l=0}^{M-1} w_l u^*(n-l) \\right) \\right] $$\n",
    "\n",
    "We want to describe this in quadratic form.\n",
    "\n",
    "$$ J = E\\left[ |d(n)|^2 \\right] - \\sum_{k=0}^{M-1} w_k^* E\\left[ u(n-k) d^*(n) \\right] - \\sum_{l=0}^{M-1} w_l E\\left[ u^*(n-k) d(n) \\right] + \\sum_k \\sum_l w_k^* s_l E\\left[ u(n-k) u^*(n-l) \\right]  $$\n",
    "\n",
    "We should note that: \n",
    "* $E\\left[ u(n-k) d^*(n) \\right]  = p(-k) $\n",
    "* $E\\left[ u^(n-k) d^*(n) \\right] = p^*(-l) $\n",
    "* $E\\left[ u(n-k) u^*(n-l) \\right] = r(l-k)$\n",
    "\n",
    "Taking these then:\n",
    "\n",
    "$$J =  \\sigma_d^2 - w^H p - p^H + w^H R w $$\n",
    "\n",
    "We now compare the optimal filter (with the expected value of the magnitude squared of error the error  $J_{min}$) to another suboptimal filter $J$. Let's start with this quantity: $w_o$ is the optimal filter. For any $w$ which is not a weiner filter, we can find the optimal expression.\n",
    "\n",
    "$$ \\left( w - w_o \\right)^H R (w - w_o) $$\n",
    "$$ = \\left( w^H - w_o^H \\right) R \\left( w - w_o \\right) $$\n",
    "$$  = \\left( w^H - w_o^H \\right)\\left( Rw_0 Rw_o \\right) $$\n",
    "$$  = w^H R w - w^H R w_o - w_o^HR w + w_o^HRw_o $$\n",
    "\n",
    "Recall that $p = Rw_o$.\n",
    "\n",
    "$$ w^H R w - w^H P - P^H w + w_o^H R w_o $$\n",
    "\n",
    "This is the expanted quadratic term. \n",
    "\n",
    "$$ J_{min} + \\left( w - w_o \\right)^H R \\left( w - w_o \\right) $$\n",
    "$$  = \\sigma_d^2 - w_o^H R w_o + w^H R w - w^H P - P^H w + w_o^H R w_o $$\n",
    "\n",
    "Finally we have $J_{min}$ plus the quadratic form. We are able to show that this takes a complex form. The minimum must take place when $w = w_o$. Now instead of us computing the minimum error, we now understand.\n",
    "\n",
    "Let's do a quick review of spectral theorem.\n",
    "\n",
    "$$ A q_k = \\lambda_k q_k $$\n",
    "\n",
    "We can write:\n",
    "\n",
    "$$ A\\left[\\begin{matrix} q_o & ... & q_{M-1} \\end{matrix}\\right] = \\left[\\begin{matrix} v_o & ... & v_{M-1} \\end{matrix}\\right]\\left[\\begin{matrix} \\lambda & & \\\\ & ... & \\\\ & & \\lambda_{M-1} \\end{matrix}\\right] $$\n",
    "\n",
    "$$ A Q = Q \\Lambda $$\n",
    "$$ A = Q \\Lambda Q^{H} $$\n",
    "\n",
    "For the hermitian:\n",
    "\n",
    "$$ R = Q \\Lambda Q^H $$\n",
    "\n",
    "We can now say that the quantity:\n",
    "\n",
    "$$ J = J_{min} + \\left( w - w_o \\right)^H Q \\Lambda Q^H \\left( w - w_o \\right) $$\n",
    "\n",
    "Let's do a substitution:\n",
    "* $v_H = ( w - w_o) ^H Q$\n",
    "* $v = Q^H (w - w_o) $\n",
    "\n",
    "Subbing this back IN:\n",
    "\n",
    "$$ J = J_{min} v^H \\Lambda v $$\n",
    "$$ = J_{min} + \\sum_{k=0}^{M-1} \\lambda_k v_k^* v_k $$\n",
    "$$ = J_{min} + \\sum_{k=0}^{M-1} \\lambda_k |v_k|^2 $$\n",
    "\n",
    "IN terms of the spectral theorem, $\\lambda_k$ and $v_k$ is the $kth$ component of the eigenvector. \n",
    "\n",
    "Recap. We've described the weiner filter, optimal design, and what the error is when the filter is not optimal, and how we can compute J to find out how far away we are from being optimal.\n",
    "\n",
    "The book calls this next multiple linear regression. If you design a 10 tap filter to convolve with the signal, then design a weiner filer to follow it, the weiner filter will also be 10 taps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 2-8-2017 Beamforming and Beginning Prediction filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We were talking about the error surface, the convexity, and expressing it in a form. Clearly when $w=w_o$.\n",
    "\n",
    "Beamforming: temporal filtering.\n",
    "\n",
    "$$ y(n) = ax(n) + bx(n-1) + cx(n-2) + ... $$\n",
    "\n",
    "We compute a sum. If it so happens that, if we think about $y(n) = a_1 x_1(n) + a_2 x_2(n) + ...$ for instance. This equation treats each component as a different signal received from different sources. The goal is to accomplish two objectives. Reduce the effect of noise and interference. Second, improve the SNR. What are some of the applications of beamforming?\n",
    "\n",
    "(Picture of beamforming)\n",
    "<img src='Beamforming.png'>\n",
    "\n",
    "Think about the signal received at element 0. It is received at a different time than element 1, which is different from element 2, etc.\n",
    "\n",
    "The time gap between tow elements separated by distance $d$ is $\\Delta t$. The angle of incidence is $\\phi$.\n",
    "\n",
    "$$ \\Delta t = \\dfrac{d \\sin \\phi}{c} $$\n",
    "\n",
    "In some sense, given the signal at the first element $u_o(n)$, the exact same signal is received after a certain delay $u_o(n) e^{j(\\omega_c n + \\theta)}$. They are separated by $e^{j\\theta}$\n",
    "\n",
    "$$ \\theta = \\omega_c \\Delta t $$\n",
    "$$ = 2\\pi f_c \\Delta t $$\n",
    "$$ = 2\\pi \\dfrac{d \\sin \\phi}{c/f_c} = \\dfrac{2 \\pi d\\sin \\phi}{\\lambda} $$\n",
    "\n",
    "Let's say that the output $y$ can be described as some weighted sum.\n",
    "\n",
    "$$ y = u_0(n) \\sum_k w_k^* e^{-jk\\theta } $$\n",
    "\n",
    "Looking at the separation again (recall, they are separated by a phase of $\\theta$).\n",
    "\n",
    "$$ \\theta = \\dfrac{2 \\pi d \\sin \\phi}{\\lambda} $$\n",
    "\n",
    "We want the angles to be constrained by such: $-pi < \\theta < \\pi$ and $\\dfrac{-\\pi}{2} < \\phi < \\dfrac{\\pi }{2}$. These constraints imply that $d \\leq \\dfrac{\\lambda}{2}$. Therefore, the spacing of the elements must be less than half the wavelength. This is the Nyquist sampling theorem in the spatial domain.\n",
    "\n",
    "$$ y = w^H s $$\n",
    "\n",
    "Recall that $w^H = \\left[\\begin{matrix} w_o^* & ... & w_{M-1}^* \\end{matrix}\\right]$. $s$ is the steering vector, and controls phase delays. Therefore:\n",
    "\n",
    "$$ y = \\left[\\begin{matrix} w_o^* & ... & w_{M-1}^* \\end{matrix}\\right]\\left[\\begin{matrix} e^{-j\\theta} \\\\ e^{-j2\\theta} \\\\ ... \\end{matrix}\\right] $$\n",
    "\n",
    "In the presence of no noise, the reception of the EM wave is perfect. In the no noise case, there will be a lot of noise. For a given signal, we can design the weights such that $y = w^H s = 1$. This is the constraint we want to satisfy. \n",
    "\n",
    "$$ y = u_o (n) w^H s + \\left[\\begin{matrix} v_o(n) \\\\ ... \\\\ v_{M-1}(n) \\end{matrix}\\right] $$\n",
    "\n",
    "For each element, we are receiving a signal plus noise $u_o(n) + v_o(n)$ where the noise is $v_k(n)$.\n",
    "\n",
    "$$ e = u_o - w^H\\left( su_o + v \\right) $$\n",
    "$$ = u_o - w^H s u_o - w^H v $$\n",
    "\n",
    "Recaling that $w^Hs = 1$:\n",
    "\n",
    "$$ = u_o - u_o - w^H v = w^H v$$\n",
    "\n",
    "Taking the magnitude of the error as our cost function:\n",
    "\n",
    "$$ J = E\\left[ ee^* \\right] $$\n",
    "$$ = E \\left[ w^H v v^H w  \\right] $$\n",
    "$$ = w^H R_v w $$\n",
    "\n",
    "We now want to minimize the error, and therefore want to minimize $w^H R_v w$. We will use the method of Lagrange multipliers to do so. We define a cost function $J$:\n",
    "\n",
    "$$ J = w^H R_v w + \\text{Re}\\left[ \\lambda^* \\left( w^H s - 1 \\right)  \\right] $$\n",
    "\n",
    "We multiply by some lagrange coefficent $\\lambda$. The only difference here is that all the variables are not real, they are complex.\n",
    "\n",
    "$$ J = \\sum_{k=0}^{M-1} \\sum_{l=0}^{M-1} w_k^* w_l r(l-k) + \\text{Re}\\left[ \\lambda^* \\left( w^H s - 1 \\right) \\right] $$\n",
    "\n",
    "Then all we have to do is take the gradient of J and set that equal to 0 for each parameter $w_k = a_k - jb_k$. If we assume $jb_k$, the conjugate becomes $a_k + jb_k$. If we see that the coefficient is $\\lambda = \\lambda_r + j\\lambda_i$. How do we compute \n",
    "\n",
    "$$ \\nabla_k J = \\dfrac{\\partial J}{\\partial a_k} + j \\dfrac{\\partial J}{\\partial b_k} $$\n",
    "\n",
    "If we take the derivative of specific $k$ for the sum, the other $w$'s will be trated as constant. The partial derivative then with respect to $a_k$? $ \\sum_l w_l r(l-k) $ What is the partial derivative w.r.t. $b_k$?  $ + j(-j)\\sum_l w_l r(l-k) $\n",
    "\n",
    "If we take the second derivative w.r.t. $a_k$, this will be $e^{-jk \\theta}$. Take the product rule. Not shown here. We are left with:\n",
    "\n",
    "$$ \\nabla_k J = \\sum_l w_l r(l-k) + j(-j)\\sum_l w_l r(l-k) + \\lambda^* e^{-jk\\theta} $$\n",
    "\n",
    "This gradient set =0, we will have the optimal wieghts:\n",
    "\n",
    "$$ 2\\sum_l w_l r(l-k) + \\lambda^* e^{-kk\\theta} = 0 $$\n",
    "\n",
    "If we write this in vector form:\n",
    "\n",
    "$$ 2Rw_o = -\\dfrac{\\lambda^* s}{} $$\n",
    "\n",
    "Since the derivative is set =0, we make the claim that the weights $w_o$ are optimal. Also recall that $s$ is the steering vector comprising the phase delays. We need to compute the optimal weights. Before we can compute optimal weights, we must find the Lagrange coefficient $\\lambda$\n",
    "\n",
    "The optimal weights are given by:\n",
    "\n",
    "$$ w_o = - \\dfrac{\\lambda^*}{2} R^{-1}s $$\n",
    "\n",
    "How do we find $\\lambda$? We need to use the constraint $w^Hs = 1$. Take the conjugate of both sides (and recalling R = R^H)\n",
    "\n",
    "$$ w_o^* = \\dfrac{\\lambda}{2}s^H R^{-1} $$\n",
    "$$ w_o^* S = \\dfrac{\\lambda}{2}s^H R^{-1}S = 1 $$\n",
    "$$ \\lambda = -\\dfrac{2}{S^H R^{-1} S} $$\n",
    "\n",
    "The $\\lambda$ quantity is a real quantity. Cool! :) Placing this optimal coefficient back into the equation for optimal taps:\n",
    "\n",
    "$$ w_o = \\dfrac{R^{-1}s}{s^H R^{-1}S} $$\n",
    "\n",
    "The error of this, then, is:\n",
    "\n",
    "$$ E\\left[ |e(n)|^2 \\right] = w_o^H R w_o $$\n",
    "\n",
    "$$ = w_o^H R \\left( \\dfrac{R^{-1}s}{s^H R^{-1} s} \\right) $$\n",
    "$$ = \\dfrac{w_o^H s}{s^H R^{-1}s} $$\n",
    "\n",
    "We look at Linarly constrained minimum variance (LCMV) or Minimum variance distortion response (MVDR).\n",
    "\n",
    "This completes the section 2.8.\n",
    "\n",
    "The SCMP or the MVDR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Chapter 3 - Prediciton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Forward prediction and backward prediction. If we try to predict $u(n)$ using $M$ past samples. We are predicting the next samples before they are received. This is forward prediction. Predicting previous values is backwards prediction.\n",
    "\n",
    "In our homework, we talked about the idea of predicting $y(n)$ using a two tap filter. If we have $u(n)$ and delay it by 1, then have a predictor $P = W(z) = w_1 + w_2z^{-1} + w_3z^{-2} + ...$. The forward predictor error will be $f_m(n)$\n",
    "\n",
    "(Insert picture of forward predictor)\n",
    "<img src='ForwardPredictor.png'>\n",
    "\n",
    "We want to recognize and estimate of $\\hat{u}(n)$ using $u(n-1),u(n-2),...$. This whole thing is alled a prediction error filter. Different than predictor. We can say the forward prediciton error filter transfer function:\n",
    "\n",
    "$$ H_{PEF}(z) = 1 - z^{-1} W(z) = 1 - z^{-1}P(z) $$\n",
    "\n",
    "Both $W$ and $P$ are the same thing. Just different notation.\n",
    "\n",
    "If $u(n)$ is the output of an autoregressive model. Let's say we have some white noise feeding the AR model. \n",
    "\n",
    "(Inser picture of white noise model)\n",
    "\n",
    "$$ S_u(\\omega) = \\sigma_v^2 \\dfrac{1}{A(\\omega)}\\dfrac{1}{A^*(\\omega)} $$\n",
    "$$ S_u(z) = \\sigma_v^2 \\dfrac{1}{A(z)}\\dfrac{1}{A^*(\\dfrac{1}{z^*}} $$\n",
    "\n",
    "The power spectransdensity will be exactly $\\sigma_v^2$. This is because $S_f(z) = S_u(z) A(z) A^*(1/z^*) = \\sigma_v^2$. For any AR model, the predictor $v(n) --> 1-a_1 z^{-1} a_2z^{-2} --> f_m(n)$, then the predictor can be written as:\n",
    "\n",
    "(Insert models of u and v models picture)\n",
    "<img src='SandV_Models.png'>\n",
    "\n",
    "Let's say that $u(n)$ was generated using a MA filter. $c(n) --> 1 - az^{-1} --> u(n)$. The prediction error filter will be: $u(n) --> \\dfrac{1}{1-az^{-1}} --> f_m(n)$. This is nol onger a finite length.\n",
    "\n",
    "$$ \\dfrac{1}{1-az^{-1}} = 1 + az^{-1} + a^2 z^{-2} ... $$\n",
    "\n",
    "isert inifinte length filter picture\n",
    "<img src='InfiniteLength_InverseMAFilter.png'>\n",
    "\n",
    "<img src='InfiniteLength_InverseMAFilter2.png'>\n",
    "\n",
    "The predictor is a finite length filter. If the process is a ma filter (all zero filter), the predictor is an infinite length filter.\n",
    "\n",
    "$$ P(z) =  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lecture 2-13-2018 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Last class, we were designing preddictors and prediction error filters for a moving average process. If we have a filter, the moving average filter is an all zero filter. An AR model is all pole filter. We want to use a predictor.\n",
    "\n",
    "When $a$ is larger than 1, the filter is unstable.\n",
    "\n",
    "Let's start with the problem:\n",
    "\n",
    "v(n) --> $\\dfrac{1}{1 - az^{-1}} --> u(n)$. Here, $\\dfrac{1}{1 - az^{-1}}  = G(z)$\n",
    "\n",
    "Let's assume a is real and |a| > 1. The power spectral density:\n",
    "\n",
    "$$ S_u(z) = \\sigma_v^2 \\left( 1 - a z^{-1} \\right)\\left( 1-az \\right) $$\n",
    "\n",
    "We want this filter to be such that $u(n)$ --> PEF -- >$f_m$\n",
    "\n",
    "$$ S_u(z) H(z) H\\left( \\dfrac{1}{z} \\right) \\propto \\sigma_v^2 $$\n",
    "\n",
    "$$ S_{fm}(z) = \\sigma_v^2 \\left( 1-az^{-1} \\right)\\left( 1-az \\right)\\dfrac{1}{1-az}\\dfrac{1}{1-az^{-1}} $$\n",
    "\n",
    "How can we design the filter such that the filter is stable?\n",
    "\n",
    "$$ H(z) = \\dfrac{1}{1-az} $$\n",
    "$$ = \\dfrac{1}{-az}\\dfrac{1}{1-\\dfrac{1}{a}z^{-1}} $$\n",
    "$$ = \\dfrac{1}{-a}z^{-1}\\left[ 1 + 1\\dfrac{1}{a}z^{-1} + \\dfrac{1}{a^2}z^{-2} + ... \\right] $$\n",
    "$$ = \\dfrac{1}{-a}z^{-1}\\left[ 1-z^{-1} \\left(1 + 1\\dfrac{1}{a} + \\dfrac{1}{a^2}z^{-1} + ... \\right)\\right] $$\n",
    "\n",
    "The predictor is given by $ \\left(1 + 1\\dfrac{1}{a} + \\dfrac{1}{a^2}z^{-1} + ... \\right)$ and the prediction filter is given by $\\left[ 1-z^{-1} \\left(1 + 1\\dfrac{1}{a} + \\dfrac{1}{a^2}z^{-1} + ... \\right)\\right]$.\n",
    "\n",
    "(Insert picture of implemented filter)\n",
    "<img src='ForwardPredictionFilter.png'>\n",
    "\n",
    "$$ S_{fm}(z) = \\sigma_v^2\\left( 1-az^{-1} \\right)\\left( 1-az \\right)\\dfrac{1}{1-az}\\dfrac{1}{1-az^{-1}} $$\n",
    "$$ = \\sigma_v^2 \\left( 1-az^{-1} \\right)\\left( 1-az \\right)\\dfrac{az}{az-1}\\dfrac{az^{-1}}{az^{-1}-1} $$\n",
    "$$ = \\sigma_v^2 a^2 $$\n",
    "\n",
    "If $a$ is greater than 1, the pole then gets moved inside the unit circle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## With that as a brief introduction of forward prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For an $M-step$ forward prediction filter, we can estimate $\\hat{u}(n)$. Start with the tap vector:\n",
    "\n",
    "$$ \\overrightarrow{wf} = \\left[\\begin{matrix} wf_1 \\\\ ... \\\\wf_M \\end{matrix}\\right] $$\n",
    "\n",
    "A predictor is simply trying to predict the prediction filter. The forward filter of $f_m$ is the prediction filter. $f_m(n)$, the forward prediction error:\n",
    "\n",
    "$$ f_m(n) = u(n) - \\sum_{k=1}^{M} wf_k^* u(n-k) $$\n",
    "\n",
    "We can also write this prediction error filter transfer function. Another way of saying hte prediction error filter \n",
    "\n",
    "(Insert picture of modified prediction filter)\n",
    "\n",
    "Here, $u(n)$ is multiplied by 1. This simply places the minus sign in front of all the taps. We can therefore say that $\\overrightarrow{a}_m = \\left[\\begin{matrix} 1 \\\\ -wf_1 \\\\ \\vdots \\\\ -wf_m \\end{matrix}\\right]$\n",
    "\n",
    "$$ \\overrightarrow{a}_m = \\left[\\begin{matrix} 1 \\\\ -wf_1 \\\\ \\vdots \\\\ -wf_m \\end{matrix}\\right] = \\left[\\begin{matrix} 1 \\\\ -\\overrightarrow{wf} \\end{matrix}\\right] $$\n",
    "\n",
    "When we describe this :\n",
    "\n",
    "$$ f_m(n) = \\sum_{k=0}^M a^*_{m,k} u(n-k) $$\n",
    "\n",
    "Also, note that $a_{m,0} = 1$ always. Also note that $a_{m,k} = -wf_k $ for $k \\neq 0$.\n",
    "\n",
    "The error prediction filter  $H_f(z)$\n",
    "\n",
    "$$ H_f(z) = \\sum_{k=0}^M a_{m,k}^*z^{-k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Backwards prediction\n",
    "\n",
    "Onto the backwards prediction filter.  We want to predict $u(n-M)$ using the quantities $u(n-m+1), ..., u(n-1)$. \n",
    "\n",
    "$$ u(n-m) = \\sum wb_{k}^* u(n-k+1) $$\n",
    "$$ = wb^* u(n) + ... + wb_{M-1} $$\n",
    "\n",
    "The filter coefficients can be written as :\n",
    "\n",
    "$$ \\overrightarrow{wb} = \\left[\\begin{matrix} wb_1 \\\\ \\vdots \\\\ wb_M \\end{matrix}\\right] $$\n",
    "\n",
    "We want to define the backwards prediction error filter. \n",
    "\n",
    "(insert picture of backwards prediction filter 9.6)\n",
    "\n",
    "<img src='BackwardPredictionFilter.png'>\n",
    "\n",
    "We define a vector of coefficients: $a^B$\n",
    "\n",
    "$$ a^B = \\left[\\begin{matrix} -wb_1 \\\\ -wb_2 \\\\ \\vdots \\\\ 1 \\end{matrix}\\right] $$\n",
    "\n",
    "We can use this to define:\n",
    "\n",
    "$$ b_M(n) = u(n-M) - \\hat{u}(n-M) $$\n",
    "$$ = u(n-M) - \\sum_{k=1}^M w_b^* u(n-k+1) $$\n",
    "$$ = \\langle a^B, \\hat{u}(n) \\rangle $$\n",
    "\n",
    "We want to find the error for the forward and backwards prediction error. How do we find $Rw_f$. What is $R$ exactly? It is $R = E\\left[ \\overrightarrow{u}(n-1) \\overrightarrow{u}^H(n-1) \\right]$. As the correlation matrix of $a$??? Whether we take a delayed matrix $u$ or not, $R$ is the same.\n",
    "\n",
    "$$ R \\overrightarrow{wf}= E\\left[ \\overrightarrow{u}(n-1) \\overrightarrow{u}^*(n) \\right]  $$\n",
    "$$ = E\\left[\\begin{matrix} u(n-1) u^*(n) \\\\ u(n-2) u^*(n) \\\\ \\vdots \\\\ u(n-M)u^*(n) \\end{matrix}\\right] $$\n",
    "$$ = \\left[\\begin{matrix} r(-1) \\\\ \\vdots \\\\ r(-M) \\end{matrix}\\right] =\\overrightarrow{r}  $$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ R \\overrightarrow{wf} = r $$\n",
    "$$ \\implies \\overrightarrow{wf} = R^{-1} r $$\n",
    "\n",
    "For the magnitude of the error squared (don't think this was derived???):\n",
    "\n",
    "$$ P_m = E\\left[ |f_m(n)|^2 \\right] = r(0) - f^H  \\overrightarrow{wf} $$\n",
    "\n",
    "This chapter has a lot of math. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Augmented Wiener-Hoff for the forward PEF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ -R \\overrightarrow{wf} + r = 0 $$\n",
    "$$ -r^H \\overrightarrow{wf} + r(0) = PM $$\n",
    "\n",
    "Writing these two equations in matrix form:\n",
    "\n",
    "$$ \\left[\\begin{matrix} r(0) & r^H \\\\ r & R \\end{matrix}\\right] \\left[\\begin{matrix} 1 \\\\ -wf \\end{matrix}\\right] = \\left[\\begin{matrix} P_M \\\\ 0_{(M+1)x1} \\end{matrix}\\right] $$\n",
    "\n",
    "The row $r(0) r^H$ is of size $1x(M+1)$. $R$ is $MxM$, and $-wf$ is $(M+1)x1$. the answer is $(M+1)x1$. There is a zero vector $0_{M+1}$. I don't believe $P_M$ is a vector: only a scalar (yes it is a scalar! verified later in lecture), how could it not? This somehow leaves us with:\n",
    "\n",
    "$$ R_{m+1}a_m = \\left[\\begin{matrix} P_m \\\\ 0_{M+1} \\end{matrix}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Backward PEF Wiener-Hoff\n",
    "\n",
    "What will be the Wiener Hoff equation for backward predictor. PEF.\n",
    "\n",
    "$$ R \\overrightarrow{wb} = E \\left[ u(n) u^*(n-m) \\\\ u(n-1) u^*(n-M) \\\\ \\vdots \\\\ u(n-M+1) u^*(n-M) \\right] $$\n",
    "$$ = \\left[\\begin{matrix} r(m) \\\\ r(M-1) \\\\ \\vdots \\\\ r(1) \\end{matrix}\\right] = r^{B*}$$\n",
    "\n",
    "What is the relation to the $\\overrightarrow{r} = \\left[\\begin{matrix} r(-1) \\\\ r(-2) \\\\ \\vdots \\\\ r(-M) \\end{matrix}\\right]$? It is the conjugate of the values, but written in reverse order, thus the $B$ denoting \"backwards\". Writing the mean squared error:\n",
    "\n",
    "$$ P_M = E\\left[ |b_m(n)|^2 \\right] = r(0) - (r^{B*})^H \\overrightarrow{wb} $$\n",
    "$$ = r(0) - r^{BT}\\overrightarrow{wb} $$\n",
    "\n",
    "This product is a scalar quantity.\n",
    "\n",
    "The augmented wiener -hoff:\n",
    "\n",
    "$$ \\left[\\begin{matrix} R & r_B^* \\\\ r^{BT} & r(0) \\end{matrix}\\right] \\left[\\begin{matrix} -\\overrightarrow{wb} \\\\ 1 \\end{matrix}\\right] = \\left[\\begin{matrix} 0_{Mx1} \\\\ P_m \\end{matrix}\\right] $$\n",
    "\n",
    "We can rewrite this as:\n",
    "\n",
    "$$ R_{M+1} a^B = \\left[\\begin{matrix} 0_{Mx1} \\\\ P_M \\end{matrix}\\right] $$\n",
    "\n",
    "Let's see how this goes! We'll start with:\n",
    "\n",
    "$$ R \\overrightarrow{wb} = r $$\n",
    "$$ \\left[\\begin{matrix} r(0) & r(1) & ... & r(M-1) \\\\ r(1)^* & & & \\\\ \\vdots & & & \\vdots \\\\ r^*(M-1) & ... & & r(0) \\end{matrix}\\right] \\left[\\begin{matrix} wb_1 \\\\ \\vdots \\\\ wb_M \\end{matrix}\\right] = \\left[\\begin{matrix} r(M) \\\\ \\vdots \\\\ r(1) \\end{matrix}\\right] $$\n",
    "\n",
    "We want to relate $R \\overrightarrow{wf} = r$. WIf we could rewrite the vector $r^{B*}$ such that it could be in terms of $r$, much easier! We will reqrite the equations backwards then take the conjugate.\n",
    "\n",
    "Even though we cannot simply write down the row as it is, because we rewrote the vector $\\overrightarrow{wf}$. Must rewrite the vector $R_{M+1}$.\n",
    "\n",
    "$$ \\left[\\begin{matrix} r(0) & r^*(1) & ... & r^*(M-1) \\\\ r(1) & & & \\vdots \\\\ \\vdots & & & \\vdots \\\\ r(M-1) & ... & r(1) & r(0) \\end{matrix}\\right]\\left[\\begin{matrix} wb_m \\\\ \\vdots \\\\ wb_1 \\end{matrix}\\right] = \\left[\\begin{matrix} r(1) \\\\ \\vdots \\\\ r(m) \\end{matrix}\\right] $$\n",
    "\n",
    "Now we take the conjugate of both sides! we can see that the first matrix above is $R^*$. Therefore:\n",
    "$$ R \\left[\\begin{matrix} wb^*_M \\\\ \\vdots \\\\ wb^*_1 \\end{matrix}\\right] = \\left[\\begin{matrix} r(-1) \\\\ \\vdots \\\\ r(-M) \\end{matrix}\\right] $$\n",
    "\n",
    "We compare this with:\n",
    "\n",
    "$$ R \\left[\\begin{matrix} wf_1 \\\\ \\vdots \\\\ wf_m \\end{matrix}\\right] = \\left[\\begin{matrix} r(-1) \\\\ \\vdots \\\\ r(-M) \\end{matrix}\\right] $$\n",
    "\n",
    "Take note here that:\n",
    "* $ wb_1^* = wf_m \\implies wb_1 = wf_m^*$\n",
    "* $ wb_m^* = wf_1 \\implies wb_M = wf_1^* $\n",
    "\n",
    "Therefore the forward predictor in reverse order and then conjugating is the same as the backward predictor:\n",
    "\n",
    "$$ \\left[\\begin{matrix} wb_1 \\\\ \\cdots \\\\ wb_M \\end{matrix}\\right] = \\left[\\begin{matrix} wf_M \\\\ \\vdots \\\\ wf_1 \\end{matrix}\\right]^* $$\n",
    "\n",
    "This is pretty cool! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Levinson-Durbin Algorithm (9.13)\n",
    "\n",
    "Let's say we know the solution to $R_m wf_{Mx1} = r_{Mx1}$. Before the levinson-Durbin algorithm, there was no way to compute, for example, the 11th order optimal filter from the 10th order optimal filter. We will compute $wf_{M+1}$ from $R wf = r$.\n",
    "\n",
    "We start with the augmented weiner-hoff equation:\n",
    "\n",
    "$$ R_m a_{m-1} = \\left[\\begin{matrix} P_{M-1} \\\\ 0_{M-1x1} \\end{matrix}\\right] $$\n",
    "\n",
    "We know the solution to $R_{M-1} wf_{M-1x1} = r_{M-1x1}$. We will follow a few steps. We want to solve $R_{m}$  If we write down this whole equation:\n",
    "\n",
    "$$ \\left[\\begin{matrix} r(0) & r(1) & ... & r(m-1) \\\\ r^*(1) & & & \\\\ \\vdots & & & \\vdots \\\\ r^*(m-1) & & & r(0) \\end{matrix}\\right] \\left[\\begin{matrix} a_{m-1,0} \\\\ a_{m-1,1}\\\\ \\vdots \\\\ a_{m-a,m-1} \\end{matrix}\\right] = \\left[\\begin{matrix} P_{m-1} \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{matrix}\\right] $$\n",
    "\n",
    "This is the term for the system. We need to rewrite the matrix with the additional order terms. We need to solve the matrix:\n",
    "\n",
    "$$ \\left[\\begin{matrix} r(0) & r(1) & ... & r(m-1) & r(m) \\\\ r^*(1) & & & & \\\\ \\vdots & & & & \\vdots \\\\ r^*(m-1) & & & & r(0) \\\\ r^*(m) & &  & r^*(1) & r(0) \\end{matrix}\\right] \\left[\\begin{matrix} a_{m,0} \\\\ \\vdots \\\\ a_{m,m} \\end{matrix}\\right] = \\left[\\begin{matrix} P_M \\\\ 0 \\\\ vdots \\\\ 0 \\end{matrix}\\right] $$\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$ \\left[\\begin{matrix} R & r_m^* \\\\ r_m^T & r(0) \\end{matrix}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2-15-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2-20-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were discussing things. We can compute k, the partial correlation coefficients. We can compute them iteratively. INstead of inverting a 10x10 matrix.\n",
    "\n",
    "When we derived $b_M(n) = u(n-m) - \\hat{u}(n-m)$, this was the error between signal  and backward predictor. We are computing $u(n-1)$ using the indices through $u(n-M+1)$. Notice that there is a diffferece of one tap (one index) between these approaches. Therefore, we are now calculating $b_{m-1}(n-1)$.\n",
    "\n",
    "Before we derive the lattice predictor. This takes a direct form transfer function and computes the coefficients using ??? polynomials. The Levinson algorithm can be used here. Forward and backward prediction are used at the same time. The properties of forward prediction error and backward error.\n",
    "\n",
    "Beginning with the forward error:\n",
    "\n",
    "$$ E\\left[ f_m(n) u^*(n-k) \\right] = 0 $$\n",
    "\n",
    "This is true for $1 \\leq k \\leq m$.  This tells us that $f_m(n)$ is orthogonal to $u(n-1...m)$. Recall that the error is orthogonal  to observation. here, we are predicting the next signal using $\\hat{u}(n) = \\sum_{i=1}^m wf_{i} u(n-i)$.\n",
    "\n",
    "In a similar way, backward error:\n",
    "\n",
    "$$ E\\left[ b_m(n) u^*(n-k) \\right] = 0  $$\n",
    "\n",
    "This is true for $k = 0, 1 , ... m-1$. $b_m(n)$ is also ortogonal to observations $u(n), u(n-1), ..., u(n-M+1)$\n",
    "\n",
    "Moving on:\n",
    "\n",
    "$$ E\\left[ f_m(n) u^*(n) \\right] = P_m $$\n",
    "\n",
    "Here we use $f_m(n) = u(n) - \\hat{u}(n) \\implies u(n) = f_m(n) + \\hat{u}(n)$\n",
    "\n",
    "$$ E\\left[ f_m(n) \\left( f_m(n) + \\hat{u}(n) \\right) \\right] $$\n",
    "\n",
    "Important to note here that $f_m(n) \\hat{u}(n) = 0$. This is because the error $f_m$ is orthogonal to all $n-1$ pas observations. Because the current estimate is simply a sum of all past observations, $\\hat{u}(n) = \\sum_{i=1}^m wf_{i} u(n-i)$, then the current observation is also 0.\n",
    "\n",
    "This leaves us with:\n",
    "\n",
    "$$ E\\left[ f_m(n) f_m^*(n) \\right] = P_m $$\n",
    "\n",
    "Now for the backward error:\n",
    "\n",
    "$$ E\\left[ b_m(n) u^*(n-m) \\right] = P_M $$\n",
    "\n",
    "Through the same arguments for the forward proof (not shown here), this is true.\n",
    "\n",
    "For the orthogonality property of backward prediction errors.\n",
    "\n",
    "$$ E\\left[ b_m(n) b_i^*(n) \\right] = 0 \\text{ for } i\\neq m $$\n",
    "\n",
    "If $i=m$, then $E\\left[ |b_m(n)|^2 \\right] = Pm$ by definition. Therefore:\n",
    "\n",
    "$$ b_i(n) = \\sum_{k=0}^i a_{i,i-k} u(n-k) $$\n",
    "\n",
    "Backward predition of order $i$. Recall that the forward and backward predictor taps are related by reversing the order and taking the conjugate. \n",
    "\n",
    "$$ E\\left[ b_m(n) \\sum_{k=0}^i a^*_{i,i-k}u^*(n-k) \\right] $$\n",
    "\n",
    "$$ = \\sum_{k=0}^i a^*_{i,i-k}E\\left[ b_m(n) u^*(n-k) \\right] $$\n",
    "\n",
    "$$ =0 $$\n",
    "\n",
    "There is something technical that we should worry about. Recall that all the backward errors are orthogonal to each other. The forward prediction errors are NOT orthogonal to each other. \n",
    "\n",
    "$$ E\\left[ f_m(n) f_i^*(n) \\right] $$\n",
    "\n",
    "Recall that: \n",
    "\n",
    "$$ f_i(n) = \\sum_{k=0}^i a_{i,k}^* u(n-k) $$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ E\\left[ f_m(n) f_i^*(n) \\right] = E\\left[ f_m(n) \\sum_k a_{i,k} u(n-k) \\right] $$\n",
    "$$ = \\sum_k a_{i,k} E\\left[ f_m(n) u^*(n-k) \\right] $$\n",
    "\n",
    "What we at the beginning of the lecture was  showed was that $f_m$ was orthogonal to $u^*(n-k)$ for $1 \\leq k \\leq m$. Therefore:\n",
    "\n",
    "$$ = a_{i,0} E\\left[ f_m(n) u^*(n) \\right] = P_m $$\n",
    "\n",
    "This is the only non-zero term for $k=0$. Therefore if we want toe xpand, we must use the backward prediction errors, and not the forward prediction errors. We've now established the backround for lattice  filters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lattice Predictor\n",
    "\n",
    "If we are computing some prediction error filter\n",
    "\n",
    "(insert picture of forward prediction filter)\n",
    "\n",
    "Joint process estimation. The basic assumption in prediction $d$ from $u$ are both jointly stationary processes. This is a formal definition that we won't get into now. \n",
    "\n",
    "Derivation of the lattice predictor:\n",
    "\n",
    "$$ a_m = \\left[\\begin{matrix} \\overrightarrow{a}_{m-1} \\\\ 0 \\end{matrix}\\right] + k_m \\left[\\begin{matrix} 0 \\ \\overrightarrow{a}_{m-1}^{B*} \\end{matrix}\\right] $$\n",
    "\n",
    "We will stop using the vector symbol for simplicity, but understand the yare still column vectors. Onto the backward prediction coefficients:\n",
    "\n",
    "$$ a_m^{B*} = \\left[\\begin{matrix} 0 \\\\ a_{m-1}^{B*} \\end{matrix}\\right] + k_m^* \\left[\\begin{matrix} a_{m-1} \\\\ 0 \\end{matrix}\\right] $$\n",
    "\n",
    "These coefficients can be described inan autogregressive model. Here $k_m$ is from the levinson algorithm. ??? We will use these properties to derive the lattice filter.\n",
    "\n",
    "Let us begin with a definition:\n",
    "\n",
    "$$ \\overrightarrow{u}_m(n) = \\left[\\begin{matrix} u(n) \\\\ \\vdots \\\\ u(n-M+1) \\end{matrix}\\right] $$\n",
    "\n",
    "We want to compute in an autoregressive manner. We also want to write these vectors in a way which makes the future math easier.\n",
    "\n",
    "$$ \\overrightarrow{u}_{m+1}(n) = \\left[\\begin{matrix} u(n) \\\\ \\vdots \\\\ U(n-M) \\end{matrix}\\right] = \\left[\\begin{matrix} \\overrightarrow{u}_M(n) \\\\ u(n-M) \\end{matrix}\\right] = \\left[\\begin{matrix} u(n) \\\\ \\overrightarrow{u}_M(n-1) \\end{matrix}\\right] $$\n",
    "\n",
    "We use this notation to make the math more conventient. Wit this, we can formulate the autoregressive models:\n",
    "\n",
    "$$ f_m(n) = a_m^H u_{m+1}(n) $$\n",
    "\n",
    "What exactly is $a_m^H$? We must conjugate and transpose $a_m$!\n",
    "\n",
    "$$ = \\left[\\begin{matrix} a_{m-1}^H 0 \\end{matrix}\\right] \\left[\\begin{matrix} u_m(n) \\\\ u(n-M) \\end{matrix}\\right] + k_m^*\\left[\\begin{matrix} 0 a_{m-1}^{BT} \\end{matrix}\\right]\\left[\\begin{matrix} u(n) \\\\ u_M(n-1) \\end{matrix}\\right] $$\n",
    "\n",
    "Take note of teh sizes. $a_{m-1}^H$ is $1 x m$ and the $0$ is 1x1, and $a_{m-1}^{BT}$ is $mx1$.\n",
    "\n",
    "$$ = a_{m-1}^H u_M(n) + k_m^* a_{m-1}^{BT} u_m(n-1) $$\n",
    "\n",
    "We see here that $f_{m-1}(n) = a_{m-1}^H u_M(n)$ and $b_{m-1}(n-1) = a_{m-1}^{BT} u_m(n-1) $. Therefore:\n",
    "\n",
    "$$ = f_{M-1}(n) + k_M^*b_{M-1}(n-1) $$\n",
    "\n",
    "This is order recursive. We will come back and finish this derivation, but now we want the backwards pecition.\n",
    "\n",
    "### Backwards prediction\n",
    "\n",
    "$$ b_M(n) = a_M^{BT}u_{M+1} $$\n",
    "\n",
    "How do we get $a_M^{BT}$? We must conjugate the equation and transpose. If we look at this quantity and from here we write\n",
    "\n",
    "??? I dozed off ???\n",
    "\n",
    "$$ = b_{M-1}(n-1) + K_mf_{m-1}??? $$\n",
    "\n",
    "(insert picture of lattice filter)\n",
    "\n",
    "Thus we get the first stage of the latice filter.  This is pretty cool! :) The eloquency comes from the fact that the forward and backward predictions are being computed at the exact same time. If the AR model ws a 3rd order process. \n",
    "\n",
    "(insert picture of recursive calculation)\n",
    "\n",
    "The first input to the recursive equation is $f_o(n) = b_o(n) = u(n)$. We can now compute in a similar way an AR lattice. This AR lattice does the reverse of this process. We just took $u(n) --> PEF -- > Errors$. With an AR filter, we take some noise and compute the process $u(n)$.\n",
    "\n",
    "$$ f_{m-1}(n) = f_m(n) - K_m^* b_{m-1}(n-1) $$\n",
    "$$ b_m(n) = b_{m-1} + K_m f_{m-1}(n) $$\n",
    "\n",
    "(insert picture of first stage of AR lattice model)\n",
    "\n",
    "We just gave $m$ as a parameter.\n",
    "\n",
    "(insert recursive AR lattice filter)\n",
    "\n",
    "As we see, $f_0(n) = b_0(n) = u(n)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2-22-2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were talking about how to derive lattice predictors. We looked at the lattice predicotr for a wide stationary process. The AR model generates a random process from a white noise. The prediction error filter. The backward prediction error.\n",
    "\n",
    "We use hte backwqrd errors as a basis set. We cannot do this in the forward prexiction set.\n",
    "\n",
    "Lattice joint process estimator. \n",
    "\n",
    "(insert picture of lattice joint process estimator)\n",
    "\n",
    "Expanding the b terms to make $h_0^*, h_1^*, ..., h_m^*$. The error signal $\\hat{d}(n)$:\n",
    "\n",
    "$$ \\hat{d}(n) = \\sum_{i=0}^M h_i^* b_i(n) $$\n",
    "\n",
    "How do we obtain the $h_n^*$ coefficients? In a typical predictor, we start with the observation $u$. Think of the following:\n",
    "\n",
    "$$ R_b = E\\left[ \\overrightarrow{b}\\overrightarrow{b}^H \\right] $$\n",
    "\n",
    "$$ \\overrightarrow{b} = \\left[\\begin{matrix} b_o \\\\ \\vdots \\\\ b_M \\end{matrix}\\right] $$\n",
    "\n",
    "From the Weinerhoff equation:\n",
    "\n",
    "$$ R_b h = E\\left[ \\overrightarrow{b} d^*(n) \\right] = r_{bd} $$\n",
    "$$ \\implies h = R^{-1}_b r_{bd} $$\n",
    "\n",
    "The $R$ matrix will be a diabonal matrix as $b_i$'s are orthogonal. The inverse $R$ is simply the reverse matrix.\n",
    "\n",
    "We work out problem 5 from the homework now. We are transmitting binary symbols. Symbols are either +/- 1. We are assuming it is a 2-tap channel. The equalizer is called a DFE. We start with a simple linear equalizer, sometimes called a feed forward equalizer. When we talk about equilizers, they might have logs of taps.\n",
    "\n",
    "$S(n) --> 1 + \\dfrac{1}{2}z^{-1} --> + --> \\alpha_o + \\alpha_1 z^{-1} + \\alpha_2 z^{-2} --> \\hat{S}(n)$\n",
    "\n",
    "Here, $\\sigma_s^2 = E\\left[ s^2(n) \\right] = 1$. \n",
    "\n",
    "$$ \\hat{s}(n) = w^T y = \\alpha_o y(n) + \\alpha_1 y(n-1) + \\alpha_2 y(n-2) $$\n",
    "\n",
    "Here:\n",
    "\n",
    "$$ w^T = \\left[\\begin{matrix} \\alpha_o & \\alpha_1 & \\alpha_2 \\end{matrix}\\right] $$\n",
    "\n",
    "$$ y = \\left[\\begin{matrix} y_0 \\\\ y(n-1) \\\\ y(n-2) \\end{matrix}\\right] $$\n",
    "\n",
    "IN reality we don't know the variance, we will get to predicting this at some point in the class. Let's begin:\n",
    "\n",
    "R is the correlation matrix of the recived signal $y$. $P$ is the corss correlation of the desired and observed signal. $r_{ud}$ from previous lectures. this is the expected value of $y$ and $\\hat{s}$.\n",
    "\n",
    "$$ R w = P $$\n",
    "\n",
    "$$ P = \\left[\\begin{matrix} E\\left[ y(n) s^*(n) \\right] \\\\ E\\left[ y(n-1) s^*(n) \\right] \\\\ E\\left[ y(n-2) s^*(n) \\right] \\end{matrix}\\right] = \\left[\\begin{matrix} r_{ys}(0) \\\\ r_{ys}(1) \\\\ r_{yx}(2) \\end{matrix}\\right] $$\n",
    "\n",
    "We want to solve for the coefficient metrix:\n",
    "\n",
    "$$ w = R^{-1} P $$\n",
    "\n",
    "This may not be the best linear equalizer for this problem. We will typically have a delayed version of the signal. What is being predicted here is not the best transmitted signal of time $n$. If we recall from basic filter class.\n",
    "\n",
    "$$ y = s(n) + \\dfrac{1}{2}s(n-1) + v(n) $$\n",
    "\n",
    "What is the coorelation matrix?\n",
    "\n",
    "$$ r(0) = E\\left[ y(n) y^*(n) \\right] = E\\left[ s^2(n) \\right]  + \\dfrac{1}{4} E\\left[ s^2(n-1) \\right] + \\sigma_v^2 = \\dfrac{9}{4}$$\n",
    "\n",
    "Some terms cancel out. here. Now for $r(1)$.\n",
    "\n",
    "$$ r(1) = E\\left[ y(n) y^*(n-1) \\right] $$\n",
    "$$ = E\\left[ \\left( s(n) + \\dfrac{1}{2}s(n-1) + v(n) \\right)\\left( s(n-1) + \\dfrac{1}{2}s(n-2) + v(n-1) \\right) \\right] $$\n",
    "$$ = \\dfrac{1}{2} $$\n",
    "\n",
    "Now the next term:\n",
    "\n",
    "$$ r(2) = 0 $$\n",
    "\n",
    "The correlation matrix can be written as:\n",
    "\n",
    "$$ R_y = \\left[\\begin{matrix} \\dfrac{9}{4} & \\dfrac{1}{2} & 0 \\\\ \\dfrac{1}{2} & \\dfrac{9}{4} & \\dfrac{1}{2} \\\\ 0 & \\dfrac{1}{2} & \\dfrac{9}{4} \\end{matrix}\\right] $$\n",
    "\n",
    "The next portion:\n",
    "\n",
    "$$ r_{ys}(0) = E\\left[ y(n) s(n) \\right]  = 1 $$\n",
    "$$ r_{ys}(1) = E\\left[ y(n-1) s(n) \\right] $$\n",
    "$$ E\\left[ \\left( s(n-1) + \\dfrac{1}{2}s(n-2) + v(n) \\right) s(n) = 0 \\right] $$\n",
    "$$ r_{ys}(2) = E\\left[ y(n-2) s(n) \\right] = 0 $$\n",
    "\n",
    "Therefore the solution:\n",
    "\n",
    "$$ \\left[\\begin{matrix} \\alpha_0 \\\\ \\alpha_1 \\\\ \\alpha_2 \\end{matrix}\\right] = \\left[\\begin{matrix} \\dfrac{9}{4} & \\dfrac{1}{2} & 0 \\\\ \\dfrac{1}{2} & \\dfrac{9}{4} & \\dfrac{1}{2} \\\\ 0 & \\dfrac{1}{2} & \\dfrac{9}{4} \\end{matrix}\\right]^{-1} \\left[\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right] = \\left[\\begin{matrix} \\dfrac{324}{693} \\\\ \\dfrac{-72}{693} \\\\ \\dfrac{16}{693} \\end{matrix}\\right] $$\n",
    "\n",
    "What is the mean squared error:\n",
    "\n",
    "$$ MMSE = \\sigma_s^2 - \\omega^H P $$\n",
    "$$ = 1 - \\left[\\begin{matrix} \\alpha_0 \\alpha_1 \\alpha_2 \\end{matrix}\\right]\\left[\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right] $$\n",
    "$$ = 1 - \\alpha_0 = 1-\\dfrac{324}{693} $$\n",
    "$$ = \\dfrac{369}{693} $$\n",
    "\n",
    "This is the solution to problem 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent\n",
    "\n",
    "We need to iteratively update weights. This is based on the gradient of the error. We move in the direction opposite of that to the gradient.\n",
    "\n",
    "$$ J(w_o) < J(w) \\text{ for } w \\neq w_o $$\n",
    "\n",
    "This is saying that the minimum error (or cost function) occurs at some $w_o$. We want to compute this gradient (signified here as g). We want to take $g = \\dfrac{\\partial J}{\\partial w}$.\n",
    "\n",
    "$$ w(n+1) = w(n) - \\dfrac{\\mu}{2}g(n) $$\n",
    "\n",
    "Here, $\\mu$ is the step size, or the learning rate. the $\\dfrac{1}{2}$ comes in later, we will see why we simply don't absorb it into the constant.\n",
    "\n",
    "$$ J(w(n+1)) = J(w(n)) + \\dfrac{\\partial J}{\\partial w} \\delta w $$\n",
    "\n",
    "Here, $\\delta w$ is $w(n+1) - w(n)$ ans is the difference between subsequent $w$s.\n",
    "\n",
    "$$ = J(w(n)) - \\dfrac{\\mu}{2}||g(n)||^2 $$\n",
    "\n",
    "What will happen if $\\mu$ is large? A quick definition:\n",
    "\n",
    "$$ \\langle a,b \\rangle  = \\left[\\begin{matrix} a_1^* a_2^* \\end{matrix}\\right] \\left[\\begin{matrix} b_1 \\\\ b_2 \\end{matrix}\\right] = a^H b $$\n",
    "\n",
    "So then we can compute the gradient. Start the error:\n",
    "\n",
    "$$ e(n) = d(n) - w^H u(n) $$\n",
    "\n",
    "remember that $w$ is a $1xM$ matrix and $u(n)$ is $Mx1$.\n",
    "\n",
    "$$ J(n) = \\sigma_d^2 - w^H P - P^H w + w^H R w $$\n",
    "$$ \\nabla J = -2P + 2Rw $$\n",
    "\n",
    "We won't spend the time showing this proof. We went through taking the gradient.\n",
    "\n",
    "$$ E\\left[ ue \\right] $$\n",
    "$$ = E \\left[ u\\left( d(n) - w^H u \\right) \\right] $$\n",
    "\n",
    "Here $(w^H u) = (u^H w)^*$ I think.\n",
    "\n",
    "$$ = E\\left[ u(n) d^*(n)  \\right] - E\\left[ uu^H \\right]w $$\n",
    "$$ = P - RW $$\n",
    "\n",
    "We can then way that $w(n+1) = w(n) + \\mu E\\left[ \\overrightarrow{u}e^* \\right]$. This then says that we can use the gradient by taking the error $e$ and taking the conjugate.\n",
    "\n",
    "(insert picture of weighted gradints)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "121px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
